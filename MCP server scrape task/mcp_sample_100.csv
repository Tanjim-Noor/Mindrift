mcp_detail_url,github_url,name,description,language,category,tags,overview,readme,github_stars
https://mcp.so/server/image-tools-mcp/kshern,https://github.com/kshern/image-tools-mcp,Image Tools Mcp,"A Model Context Protocol (MCP) service for retrieving image dimensions and compressing images, supporting both URL and local file sources.",English,,images compress; figma; tinyPNG,no content,"# Image Tools MCP

[![smithery badge](https://smithery.ai/badge/@kshern/image-tools-mcp)](https://smithery.ai/server/@kshern/image-tools-mcp)

A Model Context Protocol (MCP) service for retrieving image dimensions and compressing images, supporting both URL and local file sources.

_[ä¸­æ–‡æ–‡æ¡£](./README_zh.md)_

## Features

- Retrieve image dimensions from URLs
- Get image dimensions from local files
- Compress images from URLs using TinyPNG API
- Compress local images using TinyPNG API
- Convert images to different formats (webp, jpeg/jpg, png)
- Returns width, height, type, MIME type, and compression information

### Example Results

![Example Result 1](./public/image_gemini_1.jpg)
![Example Result 2](./public/image_gemini_2.jpg)

![Example Result 1](./public/image_1.png)
![Example Result 2](./public/image_2.png)

download from figma url and compress
![Example Result 3](./public/image_figma_url.png)

## Usage

### Using as an MCP Service

This service provides five tool functions:

1. `get_image_size` - Get dimensions of remote images
2. `get_local_image_size` - Get dimensions of local images
3. `compress_image_from_url` - Compress remote images using TinyPNG API
4. `compress_local_image` - Compress local images using TinyPNG API
5. `figma` - Fetch image links from Figma API and compress them using TinyPNG API

### Client Integration

To use this MCP service, you need to connect to it from an MCP client. Here are examples of how to integrate with different clients:

#### Usage

```json
{
  ""mcpServers"": {
    ""image-tools"": {
      ""command"": ""npx"",
      ""args"": [""image-tools-mcp""],
      ""env"": {
        ""TINIFY_API_KEY"": ""<YOUR_TINIFY_API_KEY>"",
        ""FIGMA_API_TOKEN"": ""<YOUR_FIGMA_API_TOKEN>""
      }
    }
  }
}
```

#### Using with MCP Client Library

````typescript
import { McpClient } from ""@modelcontextprotocol/client"";

// Initialize the client
const client = new McpClient({
  transport: ""stdio"" // or other transport options
});

// Connect to the server
await client.connect();

// Get image dimensions from URL
const urlResult = await client.callTool(""get_image_size"", {
  options: {
    imageUrl: ""https://example.com/image.jpg""
  }
});
console.log(JSON.parse(urlResult.content[0].text));
// Output: { width: 800, height: 600, type: ""jpg"", mime: ""image/jpeg"" }

// Get image dimensions from local file
const localResult = await client.callTool(""get_local_image_size"", {
  options: {
    imagePath: ""D:/path/to/image.png""
  }
});
console.log(JSON.parse(localResult.content[0].text));
// Output: { width: 1024, height: 768, type: ""png"", mime: ""image/png"", path: ""D:/path/to/image.png"" }

// Compress image from URL
const compressUrlResult = await client.callTool(""compress_image_from_url"", {
  options: {
    imageUrl: ""https://example.com/image.jpg"",
    outputFormat: ""webp"" // Optional: convert to webp, jpeg/jpg, or png
  }
});
console.log(JSON.parse(compressUrlResult.content[0].text));
// Output: { originalSize: 102400, compressedSize: 51200, compressionRatio: ""50.00%"", tempFilePath: ""/tmp/compressed_1615456789.webp"", format: ""webp"" }

// Compress local image
const compressLocalResult = await client.callTool(""compress_local_image"", {
  options: {
    imagePath: ""D:/path/to/image.png"",
    outputPath: ""D:/path/to/compressed.webp"", // Optional
    outputFormat: ""image/webp"" // Optional: convert to image/webp, image/jpeg, or image/png
  }
});
console.log(JSON.parse(compressLocalResult.content[0].text));
// Output: { originalSize: 102400, compressedSize: 51200, compressionRatio: ""50.00%"", outputPath: ""D:/path/to/compressed.webp"", format: ""webp"" }

// Fetch image links from Figma API

const figmaResult = await client.callTool(""figma"", {
  options: {
    figmaUrl: ""https://www.figma.com/file/XXXXXXX""
  }
});
console.log(JSON.parse(figmaResult.content[0].text));
// Output: { imageLinks: [""https://example.com/image1.jpg"", ""https://example.com/image2.jpg""] }

### Tool Schemas

#### get_image_size

```typescript
{
  options: {
    imageUrl: string // URL of the image to retrieve dimensions for
  }
}
````

#### get_local_image_size

```typescript
{
  options: {
    imagePath: string; // Absolute path to the local image file
  }
}
```

#### compress_image_from_url

```typescript
{
  options: {
    imageUrl: string // URL of the image to compress
    outputFormat?: ""image/webp"" | ""image/jpeg"" | ""image/jpg"" | ""image/png"" // Optional output format
  }
}
```

#### compress_local_image

```typescript
{
  options: {
    imagePath: string // Absolute path to the local image file
    outputPath?: string // Optional absolute path for the compressed output image
    outputFormat?: ""image/webp"" | ""image/jpeg"" | ""image/jpg"" | ""image/png"" // Optional output format
  }
}
```

#### figma

```typescript
{
  options: {
    figmaUrl: string; // URL of the Figma file to fetch image links from
  }
}
```

## Changelog

- **2025-05-12:** Updated Figma API to support additional parameters, including 2x image scaling.

## Technical Implementation

This project is built on the following libraries:

- [probe-image-size](https://github.com/nodeca/probe-image-size) - For image dimension detection
- [tinify](https://github.com/tinify/tinify-nodejs) - For image compression via the TinyPNG API
- [figma-api](https://github.com/figma/api) - For fetching image links from Figma API

## Environment Variables

- `TINIFY_API_KEY` - Required for image compression functionality. Get your API key from [TinyPNG](https://tinypng.com/developers)
  - When not provided, the compression tools (`compress_image_from_url` and `compress_local_image`) will not be registered
- `FIGMA_API_TOKEN` - Required for fetching image links from Figma API. Get your API token from [Figma](https://www.figma.com/developers)
  - When not provided, the Figma tool (`figma`) will not be registered

Note: The basic image dimension tools (`get_image_size` and `get_local_image_size`) are always available regardless of API keys.

## License

MIT
",6
https://mcp.so/server/-Creating-a-MCP-Server-for-Claude-AI-Assistant-to-interact-with-GitHub-and-demonstrate-its-Use-/Preeti75,https://github.com/Preeti75/-Creating-a-MCP-Server-for-Claude-AI-Assistant-to-interact-with-GitHub-and-demonstrate-its-Use-,""" Creating-a-MCP-Server-for-Claude-AI-Assistant-to-interact-with-GitHub-and-demonstrate-its-Use """,""" Creating a MCP Server for Claude AI Assistant to interact with GitHub and demonstrate its Use """,English,research-and-data,mathgpt; math-solver; math-assistant,"What is the MCP Server for Claude AI? The MCP Server for Claude AI is a tool that allows the Claude AI Assistant to interact with GitHub, enabling users to manage their repositories and enhance their development workflow. How to use the MCP Server for Claude AI? To use the MCP Server, set up the server by following the installation steps, configure it with your GitHub account, and then use Claude AI to query and manage your GitHub repositories. Key features of the MCP Server for Claude AI? Direct interaction with GitHub repositories through Claude AI. Ability to read/write files and generate insights from your code. Enhanced development workflow with AI assistance. Use cases of the MCP Server for Claude AI? Managing GitHub repositories directly through AI commands. Generating insights and explanations for projects. Creating and managing new repositories with AI assistance. FAQ from the MCP Server for Claude AI? Can the MCP Server work with other APIs? Yes, it can be configured to work with any application that has an API, such as Spotify or Twitter. What are the prerequisites for setting up the MCP Server? You need Node.js, npm, Claude Desktop, and a GitHub account with appropriate permissions. Is there any cost associated with using the MCP Server? The MCP Server is free to use, but you may need to check the API usage policies of the services you integrate.","# "" Creating-a-MCP-Server-for-Claude-AI-Assistant-to-interact-with-GitHub-and-demonstrate-its-Use ""


## Requirements:
  Choose any application with an API (Spotify, GitHub, Twitter or any of your favorite one etc.).I have chosen GitHub.
  Create an MCP server and demonstrate it's use with Claude AI Assistant.

## Overview:
  The Model Context Protocol (MCP) server, a powerful tool introduced by Anthropic, bridges the gap between your AI assistant and your GitHub repository. By setting up an MCP server, you can directly query your code, read/write files, generate insights, and step-up your development workflow.

## Prerequisites:
  Before you begin, ensure you have the following installed on your system:
 1. Node.js and npm (Node Package Manager)
 2. The latest version of Claude Desktop
 3. A GitHub account with appropriate permissions

## Steps Taken:
Step 1: Generate a GitHub Personal Access Token ,

Step 2: Install the GitHub MCP Server in Claude AI Assistant,

Step 3: Configure the MCP Server,

Step 4: Update the Claude Desktop Coniguration by editing the config.json file,

Step 5: Restart Claude Desktop,

Step 6: Verify the GitHub Integration by checking how Claude AI is able to access my GitHub Repositories and work on them like displaying all my Repos,Explaining the Project , Displaying Project Structure,
        Creating a New Repository and working on it and much more work can be done.
        
## Technologies Used:
1.Claude AI Assistant : 3.7 Sonnet

2.Node js : 22.14.0

3.npm : 10.9.2

4.Visual Studio Code

5.Model Context Protocol (MCP) Server

6.GitHub Server

## Attachments:
Attaching the Screenshots folder of the Assignment done .
![Claude_Accessing_Preeti_GitHub_Repositories](https://github.com/user-attachments/assets/bc0924bd-0e31-4252-a5bf-2fda56e278a0)
![Claude_Creating_A_New_Repository_in_my_GitHub_Account](https://github.com/user-attachments/assets/7237deb9-3aca-4ad6-b53d-e7e6bdb8c822)




## Contact / Made By:
Preeti Shrivastava

GitHub Username : Preeti75

GitHub Account:github.com/Preeti75
",0
https://mcp.so/server/-toast-mcp-server/naru-sensei,https://github.com/naru-sensei/-toast-mcp-server,Windows 10 ãŠã‚ˆã³ macOS é€šçŸ¥æ©Ÿèƒ½ä»˜ã MCP ã‚µãƒ¼ãƒãƒ¼,A Model Context Protocol (MCP) server with Windows 10 desktop notifications support. It processes notification requests from MCP clients like VSCode Cline and displays customizable desktop notifications using win10toast.,English,developer-tools,mcp-server; notification; desktop-notifications,"What is the MCP Server? The MCP Server is a Model Context Protocol (MCP) server designed to display desktop notifications on Windows 10 and macOS. It processes notification requests from MCP clients like VSCode Cline and utilizes win10toast for Windows and osascript for macOS to show customizable notifications. How to use the MCP Server? To use the MCP Server, run the server with the command python mcp_server.py --port 8000 --host 0.0.0.0 , and then trigger notifications from an MCP client such as VSCode Cline. Key features of the MCP Server? Supports Model Context Protocol for communication with clients. Displays customizable desktop notifications on Windows and macOS. Handles both synchronous and asynchronous requests. Provides a simple API for triggering notifications. Use cases of the MCP Server? Sending notifications from development tools like VSCode. Displaying alerts or messages from various applications. Customizing notification parameters for different use cases. FAQ from the MCP Server? Can the MCP Server run on both Windows and macOS? Yes! The MCP Server is designed to work on both operating systems. How do I customize notifications? You can customize the title, message, display time, and more through the server's API. Is there a limit to the number of clients that can connect? No, the server can handle multiple simultaneous client connections.","# Windows 10 ãŠã‚ˆã³ macOS é€šçŸ¥æ©Ÿèƒ½ä»˜ã MCP ã‚µãƒ¼ãƒãƒ¼

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦
ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—é€šçŸ¥ã‚’è¡¨ç¤ºã§ãã‚‹ Model Context Protocol (MCP) ã‚µãƒ¼ãƒãƒ¼ã‚’ä½œæˆã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚Windows 10 ã§ã¯ win10toast ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã€macOS ã§ã¯ osascript ã‚’ä½¿ç”¨ã—ã¦é€šçŸ¥ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚ã“ã®ã‚µãƒ¼ãƒãƒ¼ã¯ MCP ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆVSCode Cline ãªã©ï¼‰ã‹ã‚‰ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å—ã‘å–ã‚Šã€ãã‚Œã«åŸºã¥ã„ã¦ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—é€šçŸ¥ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚

## è¦ä»¶

### æ©Ÿèƒ½è¦ä»¶

1. **MCP ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚µãƒãƒ¼ãƒˆ**
   - MCP ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¨ã®é€šä¿¡ã‚’ç¢ºç«‹ã™ã‚‹ãŸã‚ã® Model Context Protocol ã®å®Ÿè£…
   - æœ€æ–°ã® MCP ä»•æ§˜ã®ã‚µãƒãƒ¼ãƒˆï¼ˆVSCode Cline ã¨ã®å®Œå…¨ãªäº’æ›æ€§ã‚’æä¾›ï¼‰
   - åŒæœŸãŠã‚ˆã³éåŒæœŸãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å‡¦ç†

2. **é€šçŸ¥ã‚·ã‚¹ãƒ†ãƒ **
   - Windows 10 ã§ã¯ win10toast ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—é€šçŸ¥ã‚’è¡¨ç¤º
   - macOS ã§ã¯ osascript ã‚’ä½¿ç”¨ã—ã¦é€šçŸ¥ã‚»ãƒ³ã‚¿ãƒ¼ã«é€šçŸ¥ã‚’è¡¨ç¤º
   - ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ãªé€šçŸ¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚µãƒãƒ¼ãƒˆï¼š
     - ã‚¿ã‚¤ãƒˆãƒ«
     - ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å†…å®¹
     - è¡¨ç¤ºæ™‚é–“
     - ã‚¢ã‚¤ã‚³ãƒ³ï¼ˆWindows ã®ã¿ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
     - ã‚µãƒ–ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆmacOS ã®ã¿ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
     - é€šçŸ¥éŸ³ï¼ˆmacOS ã®ã¿ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
     - é€šçŸ¥ã‚¿ã‚¤ãƒ—ï¼ˆæƒ…å ±ã€è­¦å‘Šã€ã‚¨ãƒ©ãƒ¼ã€æˆåŠŸï¼‰

3. **ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ¥ç¶š**
   - è¨­å®šå¯èƒ½ãªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§ã®ãƒªã‚¹ãƒ‹ãƒ³ã‚°ï¼ˆlocalhost ã® 127.0.0.1 ã ã‘ã§ãªãã€ã™ã¹ã¦ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã«å¯¾å¿œã™ã‚‹ 0.0.0.0 ã§ã‚‚ï¼‰
   - è¨­å®šå¯èƒ½ãªãƒãƒ¼ãƒˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 8000ï¼‰
   - è¤‡æ•°ã®åŒæ™‚ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ¥ç¶šã®å‡¦ç†
   - æ¥ç¶šå•é¡Œã«å¯¾ã™ã‚‹é©åˆ‡ãªã‚¨ãƒ©ãƒ¼å‡¦ç†

4. **ã‚³ãƒãƒ³ãƒ‰å‡¦ç†**
   - MCP ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‹ã‚‰ã®é€šçŸ¥ã‚³ãƒãƒ³ãƒ‰ã®å‡¦ç†
   - é€šçŸ¥ã‚’ãƒˆãƒªã‚¬ãƒ¼ã™ã‚‹ãŸã‚ã®ã‚·ãƒ³ãƒ—ãƒ«ãª API ã®ã‚µãƒãƒ¼ãƒˆ
   - ã‚³ãƒãƒ³ãƒ‰æ¤œè¨¼ã¨é©åˆ‡ãªã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®æä¾›

### æŠ€è¡“è¦ä»¶

1. **ã‚µãƒ¼ãƒãƒ¼å®Ÿè£…**
   - å®Ÿè£…ã«ã¯ Python 3.8 ä»¥ä¸Šã‚’ä½¿ç”¨
   - asyncio ã¾ãŸã¯åŒæ§˜ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ãŸéåŒæœŸã‚µãƒ¼ãƒãƒ¼ã¨ã—ã¦å®Ÿè£…
   - MCP ã‚µãƒ¼ãƒãƒ¼å®Ÿè£…ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã«å¾“ã†

2. **ä¾å­˜é–¢ä¿‚**
   - Windows 10 ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—é€šçŸ¥ç”¨ã® win10toast
   - macOS é€šçŸ¥ç”¨ã® osascriptï¼ˆã‚·ã‚¹ãƒ†ãƒ ã«æ¨™æº–æ­è¼‰ï¼‰
   - MCP ãƒ—ãƒ­ãƒˆã‚³ãƒ«å®Ÿè£…ã«å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒª
   - å¤–éƒ¨ä¾å­˜é–¢ä¿‚ã‚’æœ€å°é™ã«æŠ‘ãˆã‚‹

3. **è¨­å®š**
   - ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã«ã‚ˆã‚‹è¨­å®šã®ã‚µãƒãƒ¼ãƒˆ
   - ç’°å¢ƒå¤‰æ•°ã«ã‚ˆã‚‹è¨­å®šã®ã‚µãƒãƒ¼ãƒˆ
   - ã™ã¹ã¦ã®è¨­å®šã«å¯¾ã™ã‚‹åˆç†çš„ãªãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã®æä¾›

4. **ãƒ­ã‚°è¨˜éŒ²ã¨ã‚¨ãƒ©ãƒ¼å‡¦ç†**
   - åŒ…æ‹¬çš„ãªãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…
   - ã™ã¹ã¦ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ¥ç¶šã€ã‚³ãƒãƒ³ãƒ‰ã€ã‚¨ãƒ©ãƒ¼ã®ãƒ­ã‚°è¨˜éŒ²
   - æ„å‘³ã®ã‚ã‚‹ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å«ã‚€é©åˆ‡ãªä¾‹å¤–å‡¦ç†

### ãƒ†ã‚¹ãƒˆè¦ä»¶

1. **ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ**
   - ã‚µãƒ¼ãƒãƒ¼ã®æ©Ÿèƒ½ã‚’å®Ÿè¨¼ã™ã‚‹ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å«æœ‰
   - æ§˜ã€…ãªé€šçŸ¥ã‚¿ã‚¤ãƒ—ã®ä¾‹ç¤º

2. **ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆäº’æ›æ€§**
   - VSCode Cline ã¨ã®äº’æ›æ€§ã®ç¢ºä¿
   - ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå›ºæœ‰ã®è¨­å®šè¦ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŒ–

## æˆæœç‰©

1. MCP ã‚µãƒ¼ãƒãƒ¼ã® Python å®Ÿè£…
2. æ©Ÿèƒ½ã‚’å®Ÿè¨¼ã™ã‚‹ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
3. ã™ã¹ã¦ã®ä¾å­˜é–¢ä¿‚ã‚’ãƒªã‚¹ãƒˆã—ãŸ requirements.txt
4. ä½¿ç”¨æ–¹æ³•ã¨è¨­å®šã«é–¢ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
5. ä¸€èˆ¬çš„ãªå•é¡Œã®ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰

## å®Ÿè£…ä¸Šã®æ³¨æ„

- ã‚µãƒ¼ãƒãƒ¼ãŒ localhost ã ã‘ã§ãªãã€ã™ã¹ã¦ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼ˆ0.0.0.0ï¼‰ã«ãƒã‚¤ãƒ³ãƒ‰ã™ã‚‹ã‚ˆã†ã«ã™ã‚‹
- åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‡¦ç†ã™ã‚‹ãŸã‚ã®é©åˆ‡ãªã‚¹ãƒ¬ãƒƒãƒ‡ã‚£ãƒ³ã‚°ã¾ãŸã¯éåŒæœŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹
- é©åˆ‡ãªã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–ï¼ˆå…¥åŠ›æ¤œè¨¼ãªã©ï¼‰ã‚’å«ã‚ã‚‹
- æ¥ç¶šå•é¡Œã®ãƒ‡ãƒãƒƒã‚°ã®ãŸã‚ã®è©³ç´°ãªãƒ­ã‚°ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æä¾›ã™ã‚‹
- ã‚µãƒ¼ãƒãƒ¼ã®é©åˆ‡ãªã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ã‚’å®Ÿè£…ã™ã‚‹

## ä½¿ç”¨ä¾‹

å®Œæˆã—ãŸã‚µãƒ¼ãƒãƒ¼ã¯æ¬¡ã®ã‚ˆã†ã«å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ï¼š

```bash
python mcp_server.py --port 8000 --host 0.0.0.0
```

ãã—ã¦ã€VSCode Cline ã¾ãŸã¯ä»–ã® MCP ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‹ã‚‰ MCP ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚’ä»‹ã—ã¦é€šçŸ¥ã‚’ãƒˆãƒªã‚¬ãƒ¼ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚
",3
https://mcp.so/server/.net-types-explorer-mcp-server/V0v1kkk,,404,,,,,,,
https://mcp.so/server/05-make-your-mcp-server/ollama-tlms-golang,https://github.com/ollama-tlms-golang/05-make-your-mcp-server,Creating an MCP Server in Go and Serving it with Docker,,,developer-tools,mcp; docker; go; server,"What is the MCP Server? The MCP Server is a tool created in Go that allows users to fetch webpage content using the Model Context Protocol (MCP) and the curl utility. How to use the MCP Server? To use the MCP Server, you need to build it using Docker and then run it with a configuration file for mcphost. You can request webpage content by providing a URL. Key features of the MCP Server? Fetches webpage content using curl Simple implementation in Go Dockerized for easy deployment Use cases of the MCP Server? Retrieving content from web pages for analysis. Integrating with LLMs to enhance their capabilities. Serving as a backend tool for applications needing web data. FAQ from the MCP Server? What is required to run the MCP Server? You need Docker installed and a basic understanding of Go. Can I customize the tools provided by the MCP Server? Yes! You can add more tools by modifying the server code. Is the MCP Server open-source? Yes! The MCP Server is available on GitHub.","# Creating an MCP Server in Go and Serving it with Docker

## Introduction

Today we'll look at how to create an MCP server in Go and serve it with Docker.

Prerequisites: having read [Understanding the Model Context Protocol (MCP)](https://k33g.hashnode.dev/understanding-the-model-context-protocol-mcp)

To write an MCP server, there are several official SDKs:

- [TypeScript SDK](https://github.com/modelcontextprotocol/typescript-sdk)
- [Python SDK](https://github.com/modelcontextprotocol/python-sdk)
- [Kotlin SDK](https://github.com/modelcontextprotocol/kotlin-sdk)

In recent years, I've developed an appetite for Go, so I looked around to see if there was a Go implementation. On this page [https://github.com/punkpeye/awesome-mcp-servers?tab=readme-ov-file#frameworks](https://github.com/punkpeye/awesome-mcp-servers?tab=readme-ov-file#frameworks), I found several, notably [https://github.com/mark3labs/mcp-go](https://github.com/mark3labs/mcp-go), by the creator of `mcphost` (which I discuss and use in the [previous blog post](https://k33g.hashnode.dev/understanding-the-model-context-protocol-mcp)).

The advantage of the `mcp-go` project is that it's simple and also provides tools to develop an MCP client, which will be very useful for a future blog post.

## Creating an MCP Server

I won't detail or implement all the possibilities of an MCP server here. I'll just implement the essentials:

- Provide a list of tools for the LLM
- Execute these tools when the LLM invokes them

When I use an LLM, there's one thing I'd like to be able to do: give it a website link so it can find information there, make a summary for me, etc.

So I created an MCP server that calls the `curl` utility with a URL parameter and returns its content. But let's look at the source code:

**`go.mod`**:
```go
module mcp-curl

go 1.23.4

require github.com/mark3labs/mcp-go v0.8.2
require github.com/google/uuid v1.6.0
```

**`main.go`**:
```go
package main

import (
	""context""
	""fmt""
	""os/exec""

	""github.com/mark3labs/mcp-go/mcp""
	""github.com/mark3labs/mcp-go/server""
)

func main() {
	// Create MCP server
	s := server.NewMCPServer(
		""mcp-curl"",
		""1.0.0"",
	)

	// Add a tool
	tool := mcp.NewTool(""use_curl"",
		mcp.WithDescription(""fetch this webpage""),
		mcp.WithString(""url"",
			mcp.Required(),
			mcp.Description(""url of the webpage to fetch""),
		),
	)

	// Add a tool handler
	s.AddTool(tool, curlHandler)

	fmt.Println(""ğŸš€ Server started"")
	// Start the stdio server
	if err := server.ServeStdio(s); err != nil {
		fmt.Printf(""ğŸ˜¡ Server error: %v\n"", err)
	}
	fmt.Println(""ğŸ‘‹ Server stopped"")
}

func curlHandler(ctx context.Context, request mcp.CallToolRequest) (*mcp.CallToolResult, error) {

	url, ok := request.Params.Arguments[""url""].(string)
	if !ok {
		return mcp.NewToolResultError(""url must be a string""), nil
	}
	cmd := exec.Command(""curl"", ""-s"", url)
	output, err := cmd.Output()
	if err != nil {
		return mcp.NewToolResultError(err.Error()), nil
	}
	content := string(output)

	return mcp.NewToolResultText(content), nil
}
```

### Explanations

It's really very simple. My server will provide only one tool. The code implements a server that exposes a web ""fetching"" tool based on `curl` using the Model Control Protocol.

- First, I create an MCP server named `mcp-curl` version `1.0.0` that works with standard input/output (stdio)
- Then I define a tool named `use_curl` that takes a required `url` parameter
- Finally, I add the `handler` for this tool that executes the `curl` command with the `-s` option to retrieve the webpage content
- Of course, I handle errors and return the webpage content as text

## Packaging the MCP Server with Docker

To make my life easier, I'll use a `Dockerfile` to build a Docker image containing my MCP server. This way, I can more easily deploy it on different platforms or make it available to anyone who wants to use it.

**`Dockerfile`**:
```Dockerfile
FROM golang:1.23.4-alpine AS builder
WORKDIR /app
COPY go.mod .
COPY main.go .

RUN <<EOF
go mod tidy 
go build
EOF

FROM curlimages/curl:8.6.0
WORKDIR /app
COPY --from=builder /app/mcp-curl .
ENTRYPOINT [""./mcp-curl""]
```

My `Dockerfile` has two parts:

- The first part builds my MCP server in Go
- The second part uses the `curlimages/curl:8.6.0` image that contains `curl` and copies my `mcp-curl` server into the image (so `mcp-curl` is the executable that calls `curl`)

To build the Docker image, I run the following command:

```bash
docker build -t mcp-curl .
```

And now let's see how to use our new MCP server with `mcphost`:

## Using the MCP Server with `mcphost`

First, we need to create a configuration file `mcp.json` for `mcphost`:

```json
{
  ""mcpServers"": {
    ""mcp-curl-with-docker"" :{
      ""command"": ""docker"",
      ""args"": [
        ""run"",
        ""--rm"",
        ""-i"",
        ""mcp-curl""
      ]
    }
  }
}
```

Then we can use `mcphost` to use our MCP server and an LLM with Ollama, like this:

```bash
mcphost --config ./mcp.json --model ollama:qwen2.5-coder:14b
```

The MCP server is recognized by `mcphost`:
![mcp](imgs/01-mcp.png)

You can request the list of available tools with the `/tools` command:
![mcp](imgs/02-mcp.png)

Now you can request the content of a webpage and analyze its content (in my example I retrieve Go code from GitHub):
![mcp](imgs/03-mcp.png)

Wait a little bit:
![mcp](imgs/04-mcp.png)

And here's the webpage content:
![mcp](imgs/05-mcp.png)

## Conclusion

You can see that with just a few lines, it becomes really easy to give ""superpowers"" to your LLMs. In a future blog post, I'll show you how to create a generative AI application with Ollama and an MCP client in Go to interact with our MCP server.
",8
https://mcp.so/server/07-make-your-mcp-server-/ollama-tlms-golang,https://github.com/ollama-tlms-golang/07-make-your-mcp-server-,Creating an MCP Server in Go and Serving it with Docker (part 2),,,developer-tools,mcp; docker; go; server,"what is the MCP Server Project? The MCP Server Project is a tutorial series focused on creating a Minecraft server using the Go programming language and deploying it with Docker. how to use the MCP Server Project? To use the MCP Server Project, follow the instructions provided in the GitHub repository to set up your development environment, build the server, and run it using Docker. key features of the MCP Server Project? Step-by-step guide for building a Minecraft server in Go Instructions for containerizing the server with Docker Best practices for server management and deployment use cases of the MCP Server Project? Setting up a custom Minecraft server for personal use. Learning Go programming through practical application. Experimenting with Docker for application deployment. FAQ from the MCP Server Project? Can I run the server on my local machine? Yes! The tutorial provides instructions for running the server locally. Is Docker required to run the server? While Docker is recommended for deployment, you can also run the server without it by following the local setup instructions. What version of Go is needed? The project is compatible with the latest stable version of Go.","# Creating an MCP Server in Go and Serving it with Docker (part 2)
",0
https://mcp.so/server/0xtotaylor_mcp-server-skyfire/MCP-Mirror,https://github.com/MCP-Mirror/0xtotaylor_mcp-server-skyfire,mcp-server-skyfire,Mirror of,English,developer-tools,mcp-server; payment-system; skyfire,"what is mcp-server-skyfire? The mcp-server-skyfire is a Model Context Protocol (MCP) server implementation that interfaces with the Skyfire payment system, enabling AI models to make payments through Skyfire's infrastructure using a standardized protocol. how to use mcp-server-skyfire? To use the mcp-server-skyfire, clone the repository, install the dependencies, set up your Skyfire API key in a .env file, build the project, and run the server to initiate payments. key features of mcp-server-skyfire? Implements the Model Context Protocol for payment functionality. Exposes a make_payment tool for initiating payments. Comprehensive error handling for various payment scenarios. use cases of mcp-server-skyfire? Enabling AI models to perform transactions automatically. Facilitating payments between users in a standardized manner. Integrating payment functionalities into AI applications. FAQ from mcp-server-skyfire? What is the make_payment tool? It is a tool that allows authorized clients to make payments to specified Skyfire users. What are the prerequisites for running this server? You need Node.js, TypeScript, and a valid Skyfire API key. How does error handling work? The server returns specific error messages for invalid tool names, missing parameters, and payment processing failures.","# mcp-server-skyfire

A Model Context Protocol (MCP) server implementation that interfaces with the Skyfire payment system. This server enables AI models to make payments through Skyfire's infrastructure using a standardized protocol.

## Overview

This server implements the Model Context Protocol to provide payment functionality via Skyfire's API. It exposes a single tool, `make_payment`, which allows authorized clients to initiate payments to Skyfire users.

## Prerequisites

- Node.js (ES2022 compatible)
- TypeScript
- A valid Skyfire API key

## Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd mcp-server-skyfire
```

2. Install dependencies:
```bash
npm install
```

3. Create a `.env` file in the root directory and add your Skyfire API key:
```env
SKYFIRE_API_KEY=your_api_key_here
```

## Building

To build the project:

```bash
npm run build
```

This will:
- Compile TypeScript to JavaScript
- Make the output file executable
- Place the built files in the `./build` directory

## Features

The server exposes one tool through the MCP interface:

### make_payment

Makes a payment to a specified Skyfire user.

**Parameters:**
- `receiverUsername` (string): The username of the payment recipient
- `amount` (string): The payment amount

**Example Response:**
```json
{
  ""content"": [
    {
      ""type"": ""text"",
      ""text"": ""Payment of [amount] successfully sent to [username]""
    }
  ]
}
```

## Error Handling

The server implements comprehensive error handling for various scenarios:

- Invalid tool name: Returns `MethodNotFound` error
- Missing or invalid parameters: Returns `InvalidParams` error
- Payment processing failures: Returns `InternalError` error

## Dependencies

Main dependencies include:
- `@modelcontextprotocol/sdk`: ^1.0.3
- `@skyfire-xyz/skyfire-sdk`: ^0.8.5
- `dotenv`: ^16.4.7
- `zod`: ^3.24.1

## Development

The project is set up with TypeScript and includes:
- Strict type checking
- ES2022 target
- Node16 module resolution
- Consistent file casing enforcement

## Running the Server

The server runs on standard input/output (stdio). After building, you can run it using:

```bash
./build/index.js
```

Or through the npm binary:

```bash
mcp-server-skyfire
```",0
https://mcp.so/server/100-tool-mcp-server-json-example/angrysky56,https://github.com/angrysky56/100-tool-mcp-server-json-example,100-tool-mcp-server-json-example,My last working mcp-server Claude app JSON,English,developer-tools,,"what is 100-tool-mcp-server-json-example? 100-tool-mcp-server-json-example is a project that provides working configurations for setting up MCP servers, aimed at helping users avoid common pitfalls in server setup. how to use 100-tool-mcp-server-json-example? To use this project, refer to the provided configurations and examples in the GitHub repository. Ensure to check the original repositories for any additional installation steps required. key features of 100-tool-mcp-server-json-example? Provides working configurations for MCP servers Includes a single example setup for reference Aims to simplify the server setup process use cases of 100-tool-mcp-server-json-example? Setting up MCP servers for development purposes. Learning about server configurations and setups. Troubleshooting common server setup issues. FAQ from 100-tool-mcp-server-json-example? Can I use these configurations for any MCP server? These configurations are tailored for specific setups, so it's important to check compatibility with your server version. Is there support available if I encounter issues? Support is primarily through the GitHub repository's issue tracker, where you can ask questions and report problems. Are these configurations guaranteed to work? While these configurations have worked for the author, results may vary based on individual setups and environments.","# 100-tool-mcp-server-json-example
Since setting up the servers is not usually correct in the readme's these were some of the working configs I had going, obviously you should check the repos they are from. Many had to be installed and a few extra steps etc...

Here is one I made with a single example- it is a bit wonky but works.

https://github.com/angrysky56/mcp-windows-website-downloader
",2
https://mcp.so/server/100-training-of-mcp-servers/Ossamoon,https://github.com/Ossamoon/100-training-of-mcp-servers,100-training-mcp-servers,MCPã‚µãƒ¼ãƒãƒ¼ã‚’100å€‹ä½œã£ã¦ã¿ã‚‹ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°,Chinese,developer-tools,mcp-servers; training; server-creation,"what is 100-training of MCP Servers? 100-training of MCP Servers is a project aimed at training users to create and manage 100 MCP (Minecraft) servers. how to use 100-training of MCP Servers? To use this project, follow the instructions provided in the GitHub repository to set up and configure multiple MCP servers. key features of 100-training of MCP Servers? Step-by-step guidance on creating MCP servers Best practices for server management Community support through GitHub issues use cases of 100-training of MCP Servers? Learning how to set up multiple Minecraft servers for gaming. Experimenting with server configurations and plugins. Training for server administration roles in gaming communities. FAQ from 100-training of MCP Servers? Is this project suitable for beginners? Yes! The project provides detailed instructions for users of all skill levels. Can I customize the servers? Absolutely! You can modify server settings and install plugins as per your requirements. Where can I find support? You can ask questions and report issues on the GitHub repository.","# 100-training-mcp-servers
MCPã‚µãƒ¼ãƒãƒ¼ã‚’100å€‹ä½œã£ã¦ã¿ã‚‹ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
",0
https://mcp.so/server/123123231/LiMing1459276281,https://github.com/LiMing1459276281/test-demo.git,123123231,asd asdas ad,English,,,no content,,0
https://mcp.so/server/1Panel-mcp/ruibaby,https://github.com/ruibaby/1Panel-mcp,1Panel MCP Server,A MCP server for automated website deployment to 1Panel (Experimental),English,cloud-platforms,,"What is 1Panel MCP Server? 1Panel MCP Server is an experimental Model Context Protocol (MCP) server designed for automated website deployment to 1Panel. How to use 1Panel MCP Server? To use the server, configure MCP in Cursor IDE by creating a .cursor/mcp.json file with the necessary MCP configuration, including your 1Panel base URL and API key. You can then deploy websites using specific commands in the AI chat. Key features of 1Panel MCP Server? Automates website deployment to 1Panel servers. Creates websites if they do not already exist. Uploads static website files to 1Panel. Fully compatible with the MCP standard protocol. Use cases of 1Panel MCP Server? Automating the deployment of static websites. Simplifying the website creation process on 1Panel. Managing multiple website deployments efficiently. FAQ from 1Panel MCP Server? Is this project stable for production use? No, this project is currently experimental and not recommended for direct production use. What is the MCP standard protocol? The MCP standard protocol is a framework for managing context in applications, allowing for automated interactions with services like 1Panel. How do I troubleshoot deployment issues? Check your API key permissions, ensure the website directory exists, and review server logs for detailed error information.","# 1Panel MCP Server

A [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) server for automated website deployment to 1Panel.

> [!IMPORTANT]
> Currently, this project is an experimental project and does not mean that it can be used directly.

## Video demo

<https://www.bilibili.com/video/BV1SjQRY3EmM/>

## Features

- Automates website deployment to 1Panel servers
- Creates websites if they don't already exist
- Uploads static website files to 1Panel
- Fully compatible with the MCP standard protocol

## Usage

### Configure MCP in Cursor IDE

To use this server with Cursor IDE, add the following MCP configuration:

1. Open Cursor
2. Create `.cursor/mcp.json`

```json
{
  ""mcpServers"": {
    ""1panel-mcp"": {
      ""command"": ""npx"",
      ""args"": [
        ""-y"",
        ""1panel-mcp""
      ],
      ""env"": {
        ""ONEPANEL_BASE_URL"": ""<your 1Panel base URL>"",
        ""ONEPANEL_API_KEY"": ""<your 1Panel API key>"",
        ""ONEPANEL_API_VERSION"": ""v2"" // optional, default is v2
      }
    }
  }
}
```

### Use MCP to Deploy Websites

In Cursor, you can deploy websites using the following command in the AI chat:

```plaintext
Deploy to 1Panel with domain=yourdomain.com
```

Or you can use the following format:

```plaintext
Deploy website to 1Panel server, domain: yourdomain.com
```

## API Reference

### MCP Tool: deploy_website

Deploys a website to 1Panel.

**Parameters:**

- `domain` (required): Website domain
- `buildDir` (optional): Build directory path

**Response:**

```plaintext
Successfully deployed to 1Panel!
Domain: yourdomain.com
URL: http://yourdomain.com
Upload statistics:
- Total files: 25
- Successfully uploaded: 25
- Failed to upload: 0
```

## Implementation Details

### Deployment Process

1. **Check Build Directory**: Verifies if the specified build directory exists
2. **Website Creation**: Creates a new static website through 1Panel API if it doesn't exist
3. **File Upload**: Uploads all files from the build directory to the website
4. **Statistics**: Returns detailed statistics about the upload process

## Troubleshooting

If you encounter deployment issues, check the following:

1. Ensure your API Key is valid and has sufficient permissions
2. Verify that the website directory exists and has write permissions
3. Check the 1Panel server logs for more detailed error information
4. If file uploads fail, it may be due to file permission or format issues
",33
https://mcp.so/server/1Panel/1Panel-dev,https://github.com/1Panel-dev/1Panel,1Panel,"ğŸ”¥ 1Panel provides an intuitive web interface and MCP Server to manage websites, files, containers, databases, and LLMs on a Linux server.",English,cloud-platforms,linux; docker; lnmp; lamp; cockpit; webmin,"What is 1Panel? 1Panel is a web-based Linux server management tool that provides an intuitive interface for managing websites, files, containers, databases, and LLMs on a Linux server. How to use 1Panel? To use 1Panel, execute the installation script provided in the documentation and follow the prompts to set up the server management interface. Key features of 1Panel? User-friendly web interface for server management One-click website deployment with WordPress integration Application store for easy installation of open-source tools Security features including firewall management and log auditing One-click backup and restore functionality MCP Server for executing server operations via natural language Use cases of 1Panel? Managing multiple websites and databases on a single server. Rapid deployment of web applications with SSL configuration. Utilizing containerization for secure application management. Performing backups and restores of server data easily. FAQ from 1Panel? Is 1Panel free to use? Yes! 1Panel is open-source and free to use under the GPL-3.0 license. What platforms does 1Panel support? 1Panel is designed for Linux servers and supports various configurations including Docker. How can I get support for 1Panel? You can join the community on Discord for support and discussions.","<p align=""center""><a href=""https://1panel.pro""><img src=""https://resource.1panel.pro/img/1panel-logo.png"" alt=""1Panel"" width=""300"" /></a></p>
<p align=""center""><b>Top-Rated Web-based Linux Server Management Tool</b><br>Best VPS control panel<br>æ–°ä¸€ä»£çš„ Linux æœåŠ¡å™¨è¿ç»´ç®¡ç†é¢æ¿</p>
<p align=""center"">
  <a href=""https://trendshift.io/repositories/2462"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/2462"" alt=""1Panel-dev%2F1Panel | Trendshift"" style=""width: 240px; height: auto;"" /></a>
</p>
<p align=""center"">
  <a href=""https://www.gnu.org/licenses/gpl-3.0.html""><img src=""https://shields.io/github/license/1Panel-dev/1Panel?color=%231890FF"" alt=""License: GPL v3""></a>
  <a href=""https://app.codacy.com/gh/1Panel-dev/1Panel?utm_source=github.com&utm_medium=referral&utm_content=1Panel-dev/1Panel&utm_campaign=Badge_Grade_Dashboard""><img src=""https://app.codacy.com/project/badge/Grade/da67574fd82b473992781d1386b937ef"" alt=""Codacy""></a>
  <a href=""https://discord.gg/bUpUqWqdRr"" target=""_blank"">
        <img src=""https://img.shields.io/discord/1318846410149335080?logo=discord&labelColor=%20%235462eb&logoColor=%20%23f5f5f5&color=%20%235462eb""
            alt=""chat on Discord""></a>
  <a href=""https://github.com/1Panel-dev/1Panel/releases""><img src=""https://img.shields.io/github/v/release/1Panel-dev/1Panel"" alt=""GitHub release""></a>
  <a href=""https://github.com/1Panel-dev/1Panel""><img src=""https://img.shields.io/github/stars/1Panel-dev/1Panel?color=%231890FF&style=flat-square"" alt=""Stars""></a><br>
</p>
<p align=""center"">
  <a href=""/README.md""><img alt=""English"" src=""https://img.shields.io/badge/English-d9d9d9""></a>
  <a href=""/docs/README.zh-Hans.md""><img alt=""ä¸­æ–‡(ç®€ä½“)"" src=""https://img.shields.io/badge/ä¸­æ–‡(ç®€ä½“)-d9d9d9""></a>
  <a href=""/docs/README.ja.md""><img alt=""æ—¥æœ¬èª"" src=""https://img.shields.io/badge/æ—¥æœ¬èª-d9d9d9""></a>
  <a href=""/docs/README.pt-br.md""><img alt=""PortuguÃªs (Brasil)"" src=""https://img.shields.io/badge/PortuguÃªs (Brasil)-d9d9d9""></a>
  <a href=""/docs/README.ar.md""><img alt=""Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"" src=""https://img.shields.io/badge/Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©-d9d9d9""></a>
  <a href=""/docs/README.de.md""><img alt=""Deutsch"" src=""https://img.shields.io/badge/Deutsch-d9d9d9""></a>
  <a href=""/docs/README.es.md""><img alt=""EspaÃ±ol"" src=""https://img.shields.io/badge/EspaÃ±ol-d9d9d9""></a><br>
  <a href=""/docs/README.fr.md""><img alt=""franÃ§ais"" src=""https://img.shields.io/badge/franÃ§ais-d9d9d9""></a>
  <a href=""/docs/README.ko.md""><img alt=""í•œêµ­ì–´"" src=""https://img.shields.io/badge/í•œêµ­ì–´-d9d9d9""></a>
  <a href=""/docs/README.id.md""><img alt=""Bahasa Indonesia"" src=""https://img.shields.io/badge/Bahasa Indonesia-d9d9d9""></a>
  <a href=""/docs/README.zh-Hant.md""><img alt=""ä¸­æ–‡(ç¹é«”)"" src=""https://img.shields.io/badge/ä¸­æ–‡(ç¹é«”)-d9d9d9""></a>
  <a href=""/docs/README.tr.md""><img alt=""TÃ¼rkÃ§e"" src=""https://img.shields.io/badge/TÃ¼rkÃ§e-d9d9d9""></a>
  <a href=""/docs/README.ru.md""><img alt=""Ğ ÑƒÑÑĞºĞ¸Ğ¹"" src=""https://img.shields.io/badge/%D0%A0%D1%83%D1%81%D1%81%D0%BA%D0%B8%D0%B9-d9d9d9""></a>
  <a href=""/docs/README.ms.md""><img alt=""Bahasa Melayu"" src=""https://img.shields.io/badge/Bahasa Melayu-d9d9d9""></a>
</p>

------------------------------

1Panel is an open-source, modern web-based control panel for Linux server management.

- **Efficient Management**: Through a user-friendly web graphical interface, 1Panel enables users to effortlessly manage their Linux servers. Key features include host monitoring, file management, database administration, container management, LLMs management.
- **Rapid Website Deployment**: With deep integration of the popular open-source website building software WordPress, 1Panel streamlines the process of domain binding and SSL certificate configuration, all achievable with just one click.
- **Application Store**: 1Panel curates a wide range of high-quality open-source tools and applications, facilitating easy installation and updates for its users.
- **Security and Reliability**: By leveraging containerization and secure application deployment practices, 1Panel minimizes vulnerability exposure. It further enhances security through integrated firewall management and log auditing capabilities.
- **One-Click Backup & Restore**: Data protection is made simple with 1Panel's one-click backup and restore functionality, supporting various cloud storage solutions to ensure data integrity and availability.

## Quick Start

Execute the script below and follow the prompts to install 1Panel:

```bash
curl -sSL https://resource.1panel.pro/quick_start.sh -o quick_start.sh && bash quick_start.sh
```

Please refer to our [documentation](https://docs.1panel.pro/quick_start/) for more details.

ä¸­å›½ç”¨æˆ·è¯·ä½¿ç”¨è¿™ä¸ª [å®‰è£…è„šæœ¬](https://1panel.cn/docs/installation/online_installation/)ï¼Œå…¶åº”ç”¨æ•°é‡æ¯”å›½é™…ç‰ˆæœ¬æ›´ä¸°å¯Œã€‚

## Screenshot

![UI Display](https://resource.1panel.pro/img/1panel.png)

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=1Panel-dev/1Panel&type=Date)](https://star-history.com/#1Panel-dev/1Panel&Date)

## Pro Edition

Compared to the OSS Edition, 1Panel Pro Edition provides users with a wealth of enhanced features and technical support services. Enhanced features include WAF enhancement, website tamper protection, website monitoring, GPU monitoring, custom logo and theme color, etc. [Click to view the detailed introduction of the Pro Edition](https://1panel.pro/pricing).

## Security Information

If you discover any security issues, please refer to [SECURITY.md](/SECURITY.md).

## License

Licensed under The GNU General Public License version 3 (GPLv3)  (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License at

<https://www.gnu.org/licenses/gpl-3.0.html>

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.",32
https://mcp.so/server/1broseidon_mcp-jira-server/MCP-Mirror,https://github.com/MCP-Mirror/1broseidon_mcp-jira-server,Jira MCP Server,Mirror of,English,developer-tools,jira; mcp-server; api-integration,"what is Jira MCP Server? Jira MCP Server is a Model Context Protocol server that integrates with Jira's REST API, enabling AI assistants to manage Jira issues programmatically. how to use Jira MCP Server? To use the Jira MCP Server, set up your environment by installing dependencies, building the server, and configuring it with your Jira credentials. You can then use various tools to create, list, update, get, delete issues, and add comments. key features of Jira MCP Server? Create new issues (Tasks, Epics, Subtasks) List issues with optional status filtering Update existing issues (summary, description, status) Get detailed issue information Delete issues Add comments to issues use cases of Jira MCP Server? Automating issue management in software development projects. Integrating AI assistants to streamline project management tasks. Enhancing collaboration by programmatically managing Jira issues. FAQ from Jira MCP Server? What are the prerequisites for using Jira MCP Server? You need a Jira account with API access and a Jira API token. How do I install Jira MCP Server? Install dependencies using npm install and build the server with npm run build . Can I filter issues by status? Yes, you can filter issues by status when listing them.","# Jira MCP Server

A Model Context Protocol server that provides integration with Jira's REST API, allowing AI assistants to manage Jira issues programmatically.

## Features

This server provides tools for managing Jira issues:

- Create new issues (Tasks, Epics, Subtasks)
- List issues with optional status filtering
- Update existing issues (summary, description, status)
- Get detailed issue information
- Delete issues
- Add comments to issues

## Setup

### Prerequisites

1. A Jira account with API access
2. Jira API token (can be generated from [Atlassian Account Settings](https://id.atlassian.com/manage-profile/security/api-tokens))

### Installation

1. Install dependencies:

```bash
npm install
```

2. Build the server:

```bash
npm run build
```

### Configuration

1. Create a `.jira-config.json` file in your working directory:

```json
{
  ""projectKey"": ""YOUR_PROJECT_KEY""
}
```

2. Configure the MCP server with your Jira credentials:

On MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`
On Windows: `%APPDATA%/Claude/claude_desktop_config.json`

```json
{
  ""mcpServers"": {
    ""jira"": {
      ""command"": ""node"",
      ""args"": [""/path/to/jira-server/build/index.js""],
      ""env"": {
        ""JIRA_EMAIL"": ""your-email@example.com"",
        ""JIRA_API_TOKEN"": ""your-api-token"",
        ""JIRA_DOMAIN"": ""your-domain""
      }
    }
  }
}
```

## Available Tools

### create_issue

Creates a new Jira issue

- Required parameters:
  - working_dir: Directory containing .jira-config.json
  - summary: Issue title
  - description: Issue description
  - type: Issue type (Task, Epic, or Subtask)

### list_issues

Lists issues in the project

- Required parameters:
  - working_dir: Directory containing .jira-config.json
- Optional parameters:
  - status: Filter by status (e.g., ""To Do"", ""In Progress"", ""Done"")

### update_issue

Updates an existing issue

- Required parameters:
  - working_dir: Directory containing .jira-config.json
  - issue_key: Issue key (e.g., PRJ-123)
- Optional parameters:
  - summary: New title
  - description: New description
  - status: New status

### get_issue

Gets detailed information about a specific issue

- Required parameters:
  - working_dir: Directory containing .jira-config.json
  - issue_key: Issue key (e.g., PRJ-123)

### delete_issue

Deletes a Jira issue

- Required parameters:
  - working_dir: Directory containing .jira-config.json
  - issue_key: Issue key (e.g., PRJ-123)

### add_comment

Adds a comment to an existing issue

- Required parameters:
  - working_dir: Directory containing .jira-config.json
  - issue_key: Issue key (e.g., PRJ-123)
  - comment: Comment text to add

## Development

For development with auto-rebuild:

```bash
npm run watch
```

### Error Handling

The server includes comprehensive error handling for:

- Invalid project keys
- Missing configuration
- Invalid issue types
- API authentication errors
- Invalid status transitions

### Output Formatting

Issue information is formatted to include:

- Issue key and summary
- Issue type and status
- Creation date and creator
- Description
- Comments (if any) with author and timestamp
",1
https://mcp.so/server/1nce-mcp-server/1NCE-GmbH,https://github.com/1NCE-GmbH/1nce-mcp-server,1NCE IoT Platform MCP Server,MCP Server for integration of LLMs with the 1NCE Management API,English,research-and-data,,"what is 1NCE IoT Platform MCP Server? 1NCE IoT Platform MCP Server is a middleware that enables AI agents to interact with the 1NCE Management API for managing IoT connectivity through natural language. how to use 1NCE IoT Platform MCP Server? To use the MCP Server, clone the repository, install the required dependencies, set your 1NCE API credentials as environment variables, and run the server using FastMCP CLI or directly with Python. key features of 1NCE IoT Platform MCP Server? Secure OAuth 2.0 token-based authentication Product management for IoT connectivity Order management capabilities SIM card monitoring and configuration Pre-defined interaction templates for common tasks use cases of 1NCE IoT Platform MCP Server? Managing IoT device connectivity through natural language. Automating order creation and tracking for IoT products. Monitoring SIM card status and data usage in real-time. FAQ from 1NCE IoT Platform MCP Server? What programming language is used for the MCP Server? The MCP Server is developed in Python. Is there a license for this project? Yes, it is licensed under the MIT License. Can I contribute to the project? Yes, contributions are welcome through Pull Requests.","# 1NCE IoT Platform MCP Server

A Model Context Protocol (MCP) server that enables AI agents to interact with the 1NCE IoT connectivity management platform through natural language.

## Overview

This project provides a middleware layer that connects Large Language Models (LLMs) to the 1NCE API for managing IoT connectivity. It allows AI assistants to:

- Retrieve product information
- Manage orders
- Monitor SIM card status
- Check data and SMS quotas
- Update SIM configurations
- View connectivity information

By exposing these capabilities through the Model Context Protocol, AI agents can interact with IoT infrastructure using natural language, simplifying the management of connected devices.

## Features

- **Authentication**: Secure OAuth 2.0 token-based authentication with 1NCE API
- **Product Management**: Query available IoT connectivity products
- **Order Management**: Create and track orders
- **SIM Management**: Monitor and configure SIM cards
- **Resources**: Direct access to SIM status and product data
- **Prompts**: Pre-defined interaction templates for common tasks

## Prerequisites

- Python 3.8+
- FastMCP v2
- 1NCE account with API credentials

## Installation

1. Clone this repository
```bash
git clone https://github.com/yourusername/1nce-mcp-server.git
cd 1nce-mcp-server
```

2. Install required dependencies
```bash
pip install fastmcp httpx
```

3. Set environment variables with your 1NCE API credentials
```bash
export ONCE_CLIENT_ID=""your_client_id""
export ONCE_CLIENT_SECRET=""your_client_secret""
```

## Usage

### Installing with FastMCP

The recommended way to use this MCP server is to install it using the FastMCP CLI:

```bash
fastmcp install 1nce_mcp.py
```

This will make the server available to compatible MCP clients, including Claude Desktop.

### Running Directly

For development or testing, you can run the server directly:

```bash
python 1nce_mcp.py
```

### Using with a Client

You can interact with the server programmatically using the FastMCP client:

```python
from fastmcp import Client

async with Client(""1nce_mcp.py"") as client:
    # Get list of products
    products = await client.call_tool(""get_all_products"")
    print(products)
    
    # Check SIM status
    sim_status = await client.call_tool(""get_sim_status"", {""iccid"": ""your_sim_iccid""})
    print(sim_status)
```

## Available Tools

### Product Tools
- `get_all_products()` - Retrieve all available 1NCE products

### Order Tools
- `get_all_orders(page, page_size, sort)` - Get list of orders with pagination
- `get_order_by_number(order_number)` - Get details of a specific order
- `create_order(products, delivery_address, customer_reference)` - Create a new order

### SIM Tools
- `get_all_sims(page, page_size, query, sort)` - List SIM cards with filtering options
- `get_sim_details(iccid)` - Get detailed information about a specific SIM
- `get_sim_status(iccid)` - Check the connectivity status of a SIM
- `get_sim_data_quota(iccid)` - Check remaining data quota for a SIM
- `get_sim_sms_quota(iccid)` - Check remaining SMS quota for a SIM
- `update_sim_status(iccid, status, label, imei_lock)` - Update SIM configuration
- `get_sim_usage(iccid, start_date, end_date)` - Get usage statistics for a time period
- `get_sim_events(iccid, page, page_size, sort)` - Get event history for a SIM
- `reset_sim_connectivity(iccid)` - Trigger a connectivity reset for a SIM

## Examples

### Checking SIM Status

```python
from fastmcp import Client

async with Client(""1nce_mcp.py"") as client:
    # Check SIM status
    sim_status = await client.call_tool(""get_sim_status"", {
        ""iccid"": ""8988303000123456789""
    })
    print(f""SIM Status: {sim_status.get('status')}"")
    
    # Check data quota
    quota = await client.call_tool(""get_sim_data_quota"", {
        ""iccid"": ""8988303000123456789""
    })
    print(f""Remaining data: {quota.get('volume')} MB"")
    print(f""Expires on: {quota.get('expiry_date')}"")
```

### Creating an Order

```python
from fastmcp import Client

async with Client(""1nce_mcp.py"") as client:
    # Create an order for 5 SIM cards
    order = await client.call_tool(""create_order"", {
        ""products"": [{""productId"": 1001, ""quantity"": 5}],
        ""customer_reference"": ""IoT-Project-XYZ""
    })
    print(f""Order created with number: {order.get('order_number')}"")
```

## Security

- The MCP server requires 1NCE API credentials to be set as environment variables
- Authentication is performed using OAuth 2.0 with Basic Authentication
- Access tokens are obtained dynamically and not stored persistently
- The server only exposes necessary API operations, prioritizing read operations over write operations

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgements

- [1NCE](https://1nce.com/) for providing the IoT connectivity API
- [FastMCP](https://gofastmcp.com/) for the MCP server framework
- [Anthropic](https://www.anthropic.com/) for Claude AI assistant capabilities",1
https://mcp.so/server/1panel-mcp/ruibaby,https://github.com/ruibaby/1panel-mcp,1Panel MCP Server,A MCP server for automated website deployment to 1Panel (Experimental),English,developer-tools,1panel; mcp; website-deployment,"What is 1Panel MCP Server? 1Panel MCP Server is an experimental Model Context Protocol (MCP) server designed for automated website deployment to 1Panel. How to use 1Panel MCP Server? To use the 1Panel MCP Server, clone the repository, install the dependencies, configure the environment variables, and start the server. You can then deploy websites using commands in Cursor IDE. Key features of 1Panel MCP Server? Automates website deployment to 1Panel servers Creates websites if they don't already exist Uploads static website files to 1Panel Fully compatible with the MCP standard protocol Use cases of 1Panel MCP Server? Automating the deployment of static websites to 1Panel. Simplifying the website creation process through API integration. Managing multiple website deployments efficiently. FAQ from 1Panel MCP Server? Is this project stable for production use? No, this project is currently experimental and not recommended for production use. How do I configure the server? You need to edit the .env file with your 1Panel server information after cloning the repository. What programming language is used? The project is developed in JavaScript.","# 1Panel MCP Server

A [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) server for automated website deployment to 1Panel.

> [!IMPORTANT]
> Currently, this project is an experimental project and does not mean that it can be used directly.

## Video demo

<https://www.bilibili.com/video/BV1SjQRY3EmM/>

## Features

- Automates website deployment to 1Panel servers
- Creates websites if they don't already exist
- Uploads static website files to 1Panel
- Fully compatible with the MCP standard protocol

## Usage

### Configure MCP in Cursor IDE

To use this server with Cursor IDE, add the following MCP configuration:

1. Open Cursor
2. Create `.cursor/mcp.json`

```json
{
  ""mcpServers"": {
    ""1panel-mcp"": {
      ""command"": ""npx"",
      ""args"": [
        ""-y"",
        ""1panel-mcp""
      ],
      ""env"": {
        ""ONEPANEL_BASE_URL"": ""<your 1Panel base URL>"",
        ""ONEPANEL_API_KEY"": ""<your 1Panel API key>"",
        ""ONEPANEL_API_VERSION"": ""v2"" // optional, default is v2
      }
    }
  }
}
```

### Use MCP to Deploy Websites

In Cursor, you can deploy websites using the following command in the AI chat:

```plaintext
Deploy to 1Panel with domain=yourdomain.com
```

Or you can use the following format:

```plaintext
Deploy website to 1Panel server, domain: yourdomain.com
```

## API Reference

### MCP Tool: deploy_website

Deploys a website to 1Panel.

**Parameters:**

- `domain` (required): Website domain
- `buildDir` (optional): Build directory path

**Response:**

```plaintext
Successfully deployed to 1Panel!
Domain: yourdomain.com
URL: http://yourdomain.com
Upload statistics:
- Total files: 25
- Successfully uploaded: 25
- Failed to upload: 0
```

## Implementation Details

### Deployment Process

1. **Check Build Directory**: Verifies if the specified build directory exists
2. **Website Creation**: Creates a new static website through 1Panel API if it doesn't exist
3. **File Upload**: Uploads all files from the build directory to the website
4. **Statistics**: Returns detailed statistics about the upload process

## Troubleshooting

If you encounter deployment issues, check the following:

1. Ensure your API Key is valid and has sufficient permissions
2. Verify that the website directory exists and has write permissions
3. Check the 1Panel server logs for more detailed error information
4. If file uploads fail, it may be due to file permission or format issues
",33
https://mcp.so/server/1scan/huahuayu,https://github.com/huahuayu/1scan,1scan - Blockchain Explorer API Gateway & MCP Server,A unified API gateway for integrating multiple etherscan-like blockchain explorer APIs with Model Context Protocol (MCP) support for AI assistants.,English,research-and-data,etherscan; etherscan-api; mcp-server,"What is 1scan? 1scan is a unified API gateway designed for integrating multiple blockchain explorer APIs, providing support for the Model Context Protocol (MCP) to facilitate AI assistants in querying blockchain data. How to use 1scan? To use 1scan, install it via Go or from source, create a configuration file with your API keys, and run both the API server and MCP server. You can then integrate it with AI tools like Cursor IDE to access blockchain data. Key features of 1scan? Unified API endpoint for multiple blockchain explorers API load balancing and rate limit management Custom API key support via URL parameters MCP server for AI assistants to query blockchain data Use cases of 1scan? Accessing blockchain data for AI-driven applications Analyzing transactions and account balances across multiple blockchain networks Integrating blockchain data into AI conversations for enhanced user interaction FAQ from 1scan? Can I use 1scan with any blockchain explorer? Yes, 1scan supports multiple blockchain explorers, and you can apply for API keys for each. Is there a limit on API requests? Yes, you can manage rate limits for each API key in your configuration file. How do I integrate 1scan with AI tools? You can integrate it with Cursor IDE by adding the MCP server URL in the settings.","# 1scan - Blockchain Explorer API Gateway & MCP Server

A unified API gateway for integrating multiple etherscan-like blockchain explorer APIs with Model Context Protocol (MCP) support for AI assistants.

## Overview

1scan provides two main components:

1. **API Gateway**: A unified endpoint for accessing multiple blockchain explorer APIs
2. **MCP Server**: Allows AI models (like Claude in Cursor IDE) to directly query blockchain data

## Supported Blockchain Networks

**Apply for your own API key from the corresponding blockchain explorer**

| Network Name        | Chain ID | Explorer Link                   | Explorer API Endpoint                |
| ------------------- | -------- | ------------------------------- | ------------------------------------ |
| Ethereum            | 1        | https://etherscan.io            | api.etherscan.io                     |
| Binance Smart Chain | 56       | https://bscscan.com             | api.bscscan.com                      |
| Arbitrum One        | 42161    | https://arbiscan.io             | api.arbiscan.io                      |
| Polygon             | 137      | https://polygonscan.com         | api.polygonscan.com                  |
| Optimism            | 10       | https://optimistic.etherscan.io | api-optimistic.etherscan.io          |
| Avalanche C-Chain   | 43114    | https://snowtrace.io            | api.snowtrace.io                     |
| Base                | 8453     | https://basescan.org            | api.basescan.org                     |
| zkSync Era          | 324      | https://explorer.zksync.io      | block-explorer-api.mainnet.zksync.io |
| Gnosis              | 100      | https://gnosisscan.io           | api.gnosisscan.io                    |
| Fantom              | 250      | https://ftmscan.com             | api.ftmscan.com                      |
| Mantle              | 5000     | https://mantlescan.info         | api.mantlescan.info                  |
| Cronos              | 25       | https://cronoscan.com           | api.cronoscan.com                    |
| Polygon ZkEVM       | 1101     | https://zkevm.polygonscan.com   | api-zkevm.polygonscan.com            |
| Linea               | 59144    | https://lineascan.build         | api.lineascan.build                  |
| Moonbeam            | 1284     | https://moonscan.io             | api.moonscan.io                      |
| Celo                | 42220    | https://celoscan.io             | api.celoscan.io                      |
| Scroll              | 534352   | https://scrollscan.com          | api.scrollscan.com                   |
| OpBNB               | 204      | https://opbnbscan.com           | api.opbnbscan.com                    |
| Moonriver           | 1285     | https://moonriver.moonscan.io   | api-moonriver.moonscan.io            |
| Arbitrum Nova       | 42170    | https://nova.arbiscan.io        | api-nova.arbiscan.io                 |
| Blast               | 81457    | https://blastscan.io            | api.blastscan.io                     |
| Fraxtal             | 252      | https://fraxscan.com            | api.fraxscan.com                     |
| Wemix               | 1111     | https://wemixscan.com           | api.wemixscan.com                    |
| Xai                 | 660279   | https://xaiscan.io              | api.xaiscan.io                       |
| World Chain         | 480      | https://worldscan.org           | api.worldscan.org                    |
| Ape                 | 33139    | https://apescan.io/             | api.apescan.io                       |
| Kroma               | 255      | https://kromascan.com           | api.kromascan.com                    |
| Taiko               | 167000   | https://taikoscan.io            | api.taikoscan.io                     |
| Bittorrent          | 199      | https://bttcscan.com            | api.bttcscan.com                     |
| Xdc                 | 50       | https://xdcscan.io              | api.xdcscan.io                       |

## Features

- ğŸ”„ Unified API endpoint for multiple blockchain explorers
- âš–ï¸ API load balancing
- ğŸ”‘ API rate limit management
- ğŸ¯ Custom API key support via URL parameters
- ğŸ¤– MCP server for AI assistants to query blockchain data

## Installation

### From Go

```bash
go install github.com/huahuayu/1scan@latest
```

### From Source

```bash
# Clone the repository
git clone https://github.com/huahuayu/1scan.git
cd 1scan

# Build both 1scan API server and MCP server
make build
```

## Quick Start

### 1. Create Configuration File

Create a `config.json` file with your API keys:

```json
{
  ""1"": {
    ""endpoint"": ""api.etherscan.io"",
    ""keys"": {
      ""YOUR_ETHERSCAN_API_KEY_1"": 5,
      ""YOUR_ETHERSCAN_API_KEY_2"": 10
    }
  },
  ""56"": {
    ""endpoint"": ""api.bscscan.com"",
    ""keys"": {
      ""YOUR_BSCSCAN_API_KEY"": 5
    }
  }
}
```

Each chain entry contains:

- `chainID`: The blockchain network ID
- `endpoint`: The API endpoint for the blockchain explorer
- `keys`: Map of API keys and their rate limits (requests per second)

### 2. Run the API Server

```bash
# Run with default settings
1scan -config /path/to/config.json

# Or using make
make run-1scan
```

### 3. Run the MCP Server

```bash
# Run with default settings
1scanmcp -config /path/to/config.json # default serve at port 3000

# Or using make
make run-1scanmcp

# With custom settings
1scanmcp -config /path/to/config.json -port 3000 -path /mcp
```

## Integrating with Cursor IDE

To use 1scan MCP in Cursor IDE:

1. Start both the API server and MCP server

   ```bash
   # Terminal 1
   make run-1scan

   # Terminal 2
   make run-1scanmcp
   ```

2. In Cursor IDE, go to Settings â†’ AI â†’ MCP Servers

3. Add a new MCP server with the URL:

```json
{
  ""mcpServers"": {
    ""1scan"": {
      ""url"": ""http://localhost:3000/mcp/sse""
    }
  }
}
```

4. Restart Cursor IDE if necessary

5. Now you can use blockchain data in your AI conversations:
   ```
   Can you check the balance of address 0x742d35Cc6634C0532925a3b844Bc454e4438f44e on Ethereum?
   ```

## MCP Server Configuration

When running the MCP server, you can customize it with these parameters:

- `-port`: Port to run the MCP server on (default: ""3000"")
- `-path`: Path for the MCP server endpoint (default: ""/mcp"")
- `-config`: Path to configuration file (default: ""config.json"")
- `-transport`: Transport type (sse or stdio) (default: ""sse"")

Example:

```bash
1scanmcp -port 3000 -config /path/to/config.json
```

## Available MCP Tools

The MCP server exposes these tools to AI models:

- `getAccountBalance` - Get the balance of an account on a specific blockchain
- `getTokenBalance` - Get the token balance of an account on a specific blockchain
- `getTransactionByHash` - Get transaction details by hash
- `getBlockByNumber` - Get block information by block number
- `getContractABI` - Get the ABI for a verified contract
- `getContractSourceCode` - Get the source code of a verified contract
- `getTokenInfo` - Get information about an ERC20 token
- `getGasPrice` - Get current gas price on a specific blockchain
- `getLogs` - Get event logs matching specified parameters
- `getTransaction` - Get details of a transaction by its hash
- `getTransactionCount` - Get the number of transactions sent from an address
- `getTransactionReceipt` - Get transaction receipt by transaction hash
- `getBlockTransactionCountByNumber` - Get the number of transactions in a block
- `getBlockTransactionCountByHash` - Get the number of transactions in a block by block hash
- `getBlockByHash` - Get information about a block by its hash
- `getNormalTransactions` - Get normal transactions by address
- `getInternalTransactions` - Get internal transactions by address or transaction hash
- `getERC20Transfers` - Get ERC-20 token transfer events by address or contract
- `getERC721Transfers` - Get ERC-721 NFT transfer events by address or contract
- `getValidators` - Get validator information (for networks with validator sets)
- `getContractCreation` - Get contract creation information
- `getContractVerificationStatus` - Check contract verification status

## Example MCP Interactions

Here are examples of how to interact with blockchain data through the MCP interface:

```
# Get account balance
What's the ETH balance of vitalik.eth (0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045) on Ethereum?

# Check a token balance
What's the USDC balance of address 0x28C6c06298d514Db089934071355E5743bf21d60 on Ethereum?

# Examine a transaction
Can you analyze transaction 0xdd6d7f687c9821404ae8c2ea7de5cfb5a23fc4c01ef1e7f535748cb147aa76dd on Ethereum?

# Look up block information
What transactions were in Ethereum block 18700000?

# Get contract ABI
Can you show me the ABI for the USDC contract (0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48) on Ethereum?

# Analyze contract source code
I need to understand how the Uniswap V3 router works. Can you show me the source code for 0xE592427A0AEce92De3Edee1F18E0157C05861564 on Ethereum?

# Check multiple chains
What's the difference in gas fees between Ethereum and Arbitrum right now?

# Examine token information
Tell me about the tokenomics of the APE token on Ethereum.
```

## Complete List of MCP Tools

The MCP server exposes these tools to AI models:

- `getAccountBalance` - Get the balance of an account on a specific blockchain
- `getTokenBalance` - Get the token balance of an account on a specific blockchain
- `getTransactionByHash` - Get transaction details by hash
- `getBlockByNumber` - Get block information by block number
- `getContractABI` - Get the ABI for a verified contract
- `getContractSourceCode` - Get the source code of a verified contract
- `getTokenInfo` - Get information about an ERC20 token
- `getGasPrice` - Get current gas price on a specific blockchain
- `getLogs` - Get event logs matching specified parameters
- `getTransaction` - Get details of a transaction by its hash
- `getTransactionCount` - Get the number of transactions sent from an address
- `getTransactionReceipt` - Get transaction receipt by transaction hash
- `getBlockTransactionCountByNumber` - Get the number of transactions in a block
- `getBlockTransactionCountByHash` - Get the number of transactions in a block by block hash
- `getBlockByHash` - Get information about a block by its hash
- `getNormalTransactions` - Get normal transactions by address
- `getInternalTransactions` - Get internal transactions by address or transaction hash
- `getERC20Transfers` - Get ERC-20 token transfer events by address or contract
- `getERC721Transfers` - Get ERC-721 NFT transfer events by address or contract
- `getValidators` - Get validator information (for networks with validator sets)
- `getContractCreation` - Get contract creation information
- `getContractVerificationStatus` - Check contract verification status

## Troubleshooting MCP Integration

If you encounter issues connecting to the MCP server from Cursor:

1. Verify both servers are running:

   ```bash
   # Check if servers are running
   ps aux | grep 1scanmcp
   ```

2. Ensure the correct URL is configured in Cursor

## License

MIT License
",8
https://mcp.so/server/23andme-raw-data-lookup-mcp/obuchowski,https://github.com/obuchowski/23andme-raw-data-lookup-mcp,23andMe Raw Data MCP Server,MCP server for 23andMe genetic data retrieval by RSID,English,research-and-data,23andme; genetic-data; mcp-server,"what is 23andMe Raw Data MCP Server? 23andMe Raw Data MCP Server is a minimal proof-of-concept Model Context Protocol (MCP) server designed for querying 23andMe raw genotype files using RSID. how to use 23andMe Raw Data MCP Server? To use the server, clone the repository, install the necessary dependencies, and run the server with a sample data file. The server will then parse the data and wait for MCP requests. key features of 23andMe Raw Data MCP Server? Queries 23andMe raw genotype files by RSID. Supports Node.js for easy setup and execution. Provides a simple command-line interface for interaction. use cases of 23andMe Raw Data MCP Server? Retrieving genetic information based on specific RSIDs. Integrating genetic data queries into other applications. Researching genetic variants and their implications. FAQ from 23andMe Raw Data MCP Server? What is required to run the server? You need Node.js v18+ and a 23andMe raw data file in TSV format. How do I start the server? You can start the server by running npm start sample-data.txt or node src/index.js sample-data.txt . What is the output format for genotype queries? The output will return the genotype for the given RSID, or -- if the data is missing.","# 23andMe Raw Data MCP Server

A minimal proof-of-concept Model Context Protocol (MCP) server for querying 23andMe raw genotype files by RSID.

## Requirements

- Node.js v18+
- 23andMe raw data file (TSV format, with header comments)

## Installation

```bash
git clone https://github.com/yourusername/23andme-mcp-poc.git
cd 23andme-mcp
npm install
```

## Usage

```bash
npm start sample-data.txt
```
or
```bash
node src/index.js sample-data.txt
```

This will parse `sample-data.txt` and wait for MCP requests over stdin/stdout.

## MCP Client Setup

Example of `mcp.json`:

```json
{
  ""mcpServers"": {
    ""23andMe Genotype Lookup"": {
      ""command"": ""node"",
      ""args"": [""~/src/index.js"", ""~/sample-data.txt""]
    }
  }
}
```

## Tool Definition

**get_genotype_by_rsid**

- **Input**: `{ ""rsid"": ""rs3131972"" }`
- **Output**: `AG`

Missing data returns: `--`

## License

MIT ",1
https://mcp.so/server/2560_mcp/t1ina2003,https://github.com/t1ina2003/2560_mcp,2560_mcp,A repository for 2560_mcp. Includes instructions in README for enabling GitHub MCP server in VS Code.,English,developer-tools,github; mcp; vscode,"what is 2560_mcp? 2560_mcp is a repository that provides instructions for enabling the GitHub MCP server in Visual Studio Code (VS Code). how to use 2560_mcp? To use 2560_mcp, follow the instructions in the README to configure your VS Code settings to run the GitHub MCP server using Docker. key features of 2560_mcp? Step-by-step instructions for setting up GitHub MCP server in VS Code. Integration with Docker for running the server. Support for GitHub Personal Access Token configuration. use cases of 2560_mcp? Developers wanting to integrate GitHub MCP server into their local development environment. Users needing to run GitHub services locally for testing or development purposes. Teams collaborating on projects that require GitHub MCP functionalities. FAQ from 2560_mcp? What is GitHub MCP server? GitHub MCP server is a service that allows developers to interact with GitHub's features locally. Do I need Docker to use this project? Yes, Docker is required to run the GitHub MCP server as per the instructions provided. Where can I find more information? You can refer to the official documentation linked in the README for more details.","# 2560_mcp

## å¦‚ä½•åœ¨ VS Code å•Ÿç”¨ GitHub MCP Server

1. åœ¨ VS Code çš„ `settings.json` æ–°å¢ä»¥ä¸‹è¨­å®šï¼š

```json
""github"": {
  ""command"": ""docker"",
  ""args"": [
    ""run"",
    ""-i"",
    ""--rm"",
    ""-e"",
    ""GITHUB_PERSONAL_ACCESS_TOKEN"",
    ""ghcr.io/github/github-mcp-server""
  ],
  ""env"": {
    ""GITHUB_PERSONAL_ACCESS_TOKEN"": ""${input:github_token}""
  }
}
```

2. é€™æ®µè¨­å®šåƒè€ƒè‡ª [github-mcp-server å®˜æ–¹èªªæ˜](https://github.com/github/github-mcp-server)ã€‚

3. è¨­å®šèªªæ˜ï¼š
   - é€™æœƒè®“ VS Code é€é Docker å•Ÿå‹• GitHub MCP Serverã€‚
   - ä½ éœ€è¦æœ‰ Docker ç’°å¢ƒï¼Œä¸¦æº–å‚™å¥½ GitHub Personal Access Tokenã€‚
   - `${input:github_token}` å¯åœ¨ VS Code çš„ `settings.json` å…§ä»¥ `inputs` æ©Ÿåˆ¶è¨­å®šã€‚

4. é€²éšè¨­å®šèˆ‡ç–‘é›£æ’è§£ï¼Œè«‹åƒè€ƒå®˜æ–¹æ–‡ä»¶ã€‚

---

å¦‚æœ‰å•é¡Œï¼Œè«‹åƒè€ƒå®˜æ–¹æ–‡ä»¶æˆ–è‡³ Issues å€å›å ±ã€‚",0
https://mcp.so/server/302_basic_mcp/302ai,https://github.com/302ai/302_basic_mcp,302_basic_mcp,"æä¾›æœç´¢/ä»£ç è¿è¡Œ/è®¡ç®—å™¨/ç½‘é¡µè§£æ/ç½‘é¡µéƒ¨ç½²ç­‰åŸºæœ¬åŠŸèƒ½ã€‚
Provide basic functions such as search, code execution, calculator, web parsing and web deploying.",Chinese,,mathgpt; math-solver; math-assistant,"what is 302 Basic MCP? 302 Basic MCP is a versatile server that provides essential functionalities such as search, code execution, calculator, and web parsing. how to use 302 Basic MCP? To use 302 Basic MCP, install the necessary dependencies, build the server, and configure it with your API key in the Claude Desktop application. key features of 302 Basic MCP? Search functionality for quick information retrieval Code execution capabilities for running scripts Built-in calculator for mathematical computations Web parsing to extract data from web pages use cases of 302 Basic MCP? Quickly searching for programming solutions or documentation. Executing code snippets for testing and debugging. Performing calculations directly within the application. Parsing web content for data analysis or extraction. FAQ from 302 Basic MCP? What programming languages does 302 Basic MCP support for code execution? It supports JavaScript and any language that can be executed via Node.js. Is there a graphical user interface for 302 Basic MCP? No, it operates primarily through command line and configuration files. How can I debug issues with the server? You can use the MCP Inspector for debugging, which provides tools accessible via your browser.","# <p align=""center"">ğŸ¤– 302AI Basic MCP ServerğŸš€âœ¨</p>

<p align=""center"">302AI Basic Tools is a toolkit for enhancing the fundamental capabilities of large language models.</p>

<p align=""center""><a href=""https://www.npmjs.com/package/@302ai/basic-mcp"" target=""blank""><img src=""https://file.302.ai/gpt/imgs/github/20250102/72a57c4263944b73bf521830878ae39a.png"" /></a></p >

<p align=""center""><a href=""README_zh.md"">ä¸­æ–‡</a> | <a href=""README.md"">English</a> | <a href=""README_ja.md"">æ—¥æœ¬èª</a></p>

![](docs/302_basic_mcp_en.jpg) 

## Previews

Here are some usage examples
![](docs/302_basic_mcp_en_screenshot_01.png)     
![](docs/302_basic_mcp_en_screenshot_02.png) 

Here is the list of supported tools
![](docs/302_basic_mcp_en_screenshot_03.png)     


## âœ¨ Features âœ¨

- ğŸ”§ Dynamic Loading - Automatically update tool list from remote server.
- ğŸŒ Multi modes supported, you can use `stdin` mode locally, or host it as a remote HTTP server

## ğŸš€ Tool List
- Wiki Search
- Arxiv Search
- Calculator
- Code Execution
- Web Search
- File Parser
- Upload webpage



## Development

Install dependencies:

```bash
npm install
```

Build the server:

```bash
npm run build
```

For development with auto-rebuild:

```bash
npm run watch
```

## Installation

To use with Claude Desktop, add the server config:

On MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`     
On Windows: `%APPDATA%/Claude/claude_desktop_config.json`

```json
{
  ""mcpServers"": {
    ""302ai-basic-mcp"": {
      ""command"": ""npx"",
      ""args"": [""-y"", ""@302ai/basic-mcp""],
      ""env"": {
        ""302AI_API_KEY"": ""YOUR_API_KEY_HERE""
      }
    }
  }
}
```

To use with Cherry Studio, add the server config:

```json
{
  ""mcpServers"": {
    ""Li2ZXXJkvhAALyKOFeO4N"": {
      ""name"": ""302ai-basic-mcp"",
      ""description"": """",
      ""isActive"": true,
      ""registryUrl"": """",
      ""command"": ""npx"",
      ""args"": [
        ""-y"",
        ""@302ai/basic-mcp""
      ],
      ""env"": {
        ""302AI_API_KEY"": ""YOUR_API_KEY_HERE""
      }
    }
  }
}
```

To use with ChatWise, copy the following content to clipboard
```json
{
  ""mcpServers"": {
    ""302ai-sandbox-mcp"": {
      ""command"": ""npx"",
      ""args"": [""-y"", ""@302ai/basic-mcp""],
      ""env"": {
        ""302AI_API_KEY"": ""YOUR_API_KEY_HERE""
      }
    }
  }
}
```

Go to Settings -> Tools -> Add button -> Select Import from Clipboard
![](docs/302_basic_mcp_en_screenshot_04.png)

### Find Your 302AI_API_KEY [here](https://dash.302.ai/apis/list)
[Using Tutorials](https://help.302.ai/en/docs/API-guan-li)

### Debugging

Since MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:

```bash
npm run inspector
```

The Inspector will provide a URL to access debugging tools in your browser.

## âœ¨ About 302.AI âœ¨
[302.AI](https://302.ai/en/) is an enterprise-oriented AI application platform that offers pay-as-you-go services, ready-to-use solutions, and an open-source ecosystem.âœ¨
1. ğŸ§  Integrates the latest and most comprehensive AI capabilities and brands, including but not limited to language models, image models, voice models, and video models.
2. ğŸš€ Develops deep applications based on foundation models - we develop real AI products, not just simple chatbots
3. ğŸ’° Zero monthly fee, all features are pay-per-use, fully open, achieving truly low barriers with high potential.
4. ğŸ›  Powerful management backend for teams and SMEs - one person manages, many people use.
5. ğŸ”— All AI capabilities provide API access, all tools are open source and customizable (in progress).
6. ğŸ’¡ Strong development team, launching 2-3 new applications weekly, products updated daily. Developers interested in joining are welcome to contact us.",1
https://mcp.so/server/302_browser_use_mcp/302ai,https://github.com/302ai/302_browser_use_mcp,302_browser_use_mcp,"Automatically create a remote browser to complete your specified tasks, developed based on Browser Use + Sandbox.

è‡ªåŠ¨åˆ›å»ºä¸€ä¸ªè¿œç¨‹æµè§ˆå™¨ï¼Œå®Œæˆä½ æŒ‡å®šçš„ä»»åŠ¡ï¼ŒåŸºäºBrowser Use + Sandboxå¼€å‘ã€‚",Chinese,,browser-automation; remote-browser; automation-tool,"what is 302AI BrowserUse MCP? 302AI BrowserUse MCP is a tool designed to automatically create a remote browser to complete specified tasks, leveraging Browser Use and Sandbox development. how to use 302AI BrowserUse MCP? To use 302AI BrowserUse MCP, install the necessary dependencies, build the server, and configure it with your API key. You can also set it up for use with Claude Desktop by adding the server configuration to the appropriate config file based on your operating system. key features of 302AI BrowserUse MCP? Automatic creation of remote browsers for task completion Integration with Claude Desktop Debugging tools available through MCP Inspector use cases of 302AI BrowserUse MCP? Automating web scraping tasks Running automated tests on web applications Completing repetitive online tasks without manual intervention FAQ from 302AI BrowserUse MCP? What is the purpose of 302AI BrowserUse MCP? It automates the creation of remote browsers to perform specified tasks efficiently. Is there a way to debug the server? Yes! You can use the MCP Inspector for debugging, which provides a URL to access debugging tools in your browser. How do I find my API key? You can find your 302AI_API_KEY here .","# <p align=""center"">ğŸ¤– 302AI BrowserUse MCP ServerğŸš€âœ¨</p>

<p align=""center"">An AI-powered browser automation server implementing Model Context Protocol (MCP) for natural language browser control and web research.</p>

<a href=""https://glama.ai/mcp/servers/@302ai/302_browser_use_mcp"">
  <img width=""380"" height=""200"" src=""https://glama.ai/mcp/servers/@302ai/302_browser_use_mcp/badge"" alt=""302AI BrowserUse Server MCP server"" />
</a>

<p align=""center""><a href=""https://www.npmjs.com/package/@302ai/browser-use-mcp"" target=""blank""><img src=""https://file.302.ai/gpt/imgs/github/20250102/72a57c4263944b73bf521830878ae39a.png"" /></a></p >

<p align=""center""><a href=""README_zh.md"">ä¸­æ–‡</a> | <a href=""README.md"">English</a> | <a href=""README_ja.md"">æ—¥æœ¬èª</a></p>

![](docs/302_browser_use_mcp_en.png) 

## Previews

Here are some usage examples
![](docs/302_browser_use_mcp_en_screenshot_01.jpg)      

Here is the list of supported tools
![](docs/302_browser_use_mcp_en_screenshot_02.png)


## âœ¨ Features âœ¨

- ğŸ”§ Dynamic Loading - Automatically update tool list from remote server.
- ğŸŒ Multi modes supported, you can use `stdin` mode locally, or host it as a remote HTTP server

## ğŸš€ Tool List
- [Create Browser Automation Task](https://302ai-en.apifox.cn/api-282235063)
- [Query Browser Task Status](https://302ai-en.apifox.cn/api-282235713)


## Development

Install dependencies:

```bash
npm install
```

Build the server:

```bash
npm run build
```

For development with auto-rebuild:

```bash
npm run watch
```

## Installation

To use with Claude Desktop, add the server config:

On MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`    
On Windows: `%APPDATA%/Claude/claude_desktop_config.json`

```json
{
  ""mcpServers"": {
    ""302ai-browser-use-mcp"": {
      ""command"": ""npx"",
      ""args"": [""-y"", ""@302ai/browser-use-mcp""],
      ""env"": {
        ""302AI_API_KEY"": ""YOUR_API_KEY_HERE""
      }
    }
  }
}
```

To use with Cherry Studio, add the server config:

```json
{
  ""mcpServers"": {
    ""Li2ZXXJkvhAALyKOFeO4N"": {
      ""name"": ""302ai-browser-use-mcp"",
      ""description"": """",
      ""isActive"": true,
      ""registryUrl"": """",
      ""command"": ""npx"",
      ""args"": [
        ""-y"",
        ""@302ai/browser-use-mcp""
      ],
      ""env"": {
        ""302AI_API_KEY"": ""YOUR_API_KEY_HERE""
      }
    }
  }
}
```

To use with ChatWise, copy the following content to clipboard
```json
{
  ""mcpServers"": {
    ""302ai-sandbox-mcp"": {
      ""command"": ""npx"",
      ""args"": [""-y"", ""@302ai/browser-use-mcp""],
      ""env"": {
        ""302AI_API_KEY"": ""YOUR_API_KEY_HERE""
      }
    }
  }
}
```
Go to Settings -> Tools -> Add button -> Select Import from Clipboard
![](docs/302_browser_use_mcp_en_screenshot_03.png)

### Find Your 302AI_API_KEY [here](https://dash.302.ai/apis/list)
[Using Tutorials](https://help.302.ai/en/docs/API-guan-li)

### Debugging

Since MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:

```bash
npm run inspector
```

The Inspector will provide a URL to access debugging tools in your browser.

## âœ¨ About 302.AI âœ¨
[302.AI](https://302.ai/en/) is an enterprise-oriented AI application platform that offers pay-as-you-go services, ready-to-use solutions, and an open-source ecosystem.âœ¨
1. ğŸ§  Integrates the latest and most comprehensive AI capabilities and brands, including but not limited to language models, image models, voice models, and video models.
2. ğŸš€ Develops deep applications based on foundation models - we develop real AI products, not just simple chatbots
3. ğŸ’° Zero monthly fee, all features are pay-per-use, fully open, achieving truly low barriers with high potential.
4. ğŸ›  Powerful management backend for teams and SMEs - one person manages, many people use.
5. ğŸ”— All AI capabilities provide API access, all tools are open source and customizable (in progress).
6. ğŸ’¡ Strong development team, launching 2-3 new applications weekly, products updated daily. Developers interested in joining are welcome to contact us.
",8
https://mcp.so/server/302_custom_mcp/302ai,https://github.com/302ai/302_custom_mcp,302_custom_mcp,åœ¨302åå°è‡ªé€‰Toolsï¼Œå¿«é€Ÿç”Ÿæˆå®šåˆ¶çš„MCP Serverã€‚,Chinese,developer-tools,custom-mcp; server; 302ai,"what is 302 Custom MCP? 302 Custom MCP is a customizable server tool that allows users to quickly generate a tailored MCP (Model Context Protocol) server for various applications. how to use 302 Custom MCP? To use 302 Custom MCP, install the necessary dependencies, build the server, and configure it with your API key in the specified configuration file for your operating system. key features of 302 Custom MCP? Quick generation of custom MCP servers Easy installation and configuration Debugging tools available through MCP Inspector use cases of 302 Custom MCP? Creating a custom server for AI applications. Integrating with Claude Desktop for enhanced functionality. Debugging and testing MCP servers using the MCP Inspector. FAQ from 302 Custom MCP? What is an MCP server? An MCP server facilitates communication between AI models and applications using the Model Context Protocol. How do I find my API key? You can find your 302AI_API_KEY here . Is there support for debugging? Yes! You can use the MCP Inspector for debugging your MCP servers.","# <p align=""center"">ğŸ¤– 302AI Custom MCP ServerğŸš€âœ¨</p>

<p align=""center"">A customizable MCP service that supports tool selection and flexible configuration to meet your various needs.</p>

<p align=""center""><a href=""https://www.npmjs.com/package/@302ai/custom-mcp"" target=""blank""><img src=""https://file.302.ai/gpt/imgs/github/20250102/72a57c4263944b73bf521830878ae39a.png"" /></a></p >

<p align=""center""><a href=""README_zh.md"">ä¸­æ–‡</a> | <a href=""README.md"">English</a> | <a href=""README_ja.md"">æ—¥æœ¬èª</a></p>

![](docs/302_AI_Custom_MCP_Server_en.png) 

## Tutorial
Open MCP Server in the menu   
![](docs/302_AI_Custom_MCP_Server_screenshot_01.png)     

Enter a name and select the tools you want to configure.   
![](docs/302_AI_Custom_MCP_Server_screenshot_02.png)

This is the current list of available tools, which is continuously being updated   
![](docs/302_AI_Custom_MCP_Server_screenshot_03.png)

After creation, click on the Server name to view the Server configuration   
![](docs/302_AI_Custom_MCP_Server_screenshot_04.png)

Different Servers use different KEYs to obtain tool configurations. The client only needs to be installed once, no repeated installation required. Switching between different Servers only requires changing the API_KEY.    
![](docs/302_AI_Custom_MCP_Server_screenshot_05.png)

Open the MCP Server button in the chatbot   
![](docs/302_AI_Custom_MCP_Server_screenshot_06.png)

Enter the key you just created in 302ai-custom-server    
![](docs/302_AI_Custom_MCP_Server_screenshot_07.png)
![](docs/302_AI_Custom_MCP_Server_screenshot_08.png)
Turn on the Server switch to start using it.  

Taking Chatwise as an example for use in third-party clients
Click on the Server name, then click the copy button    
![](docs/302_AI_Custom_MCP_Server_screenshot_09.png)
![](docs/302_AI_Custom_MCP_Server_screenshot_10.png)

Open Chatwise settings-tools, click the bottom left corner, import JSON from clipboard   
![](docs/302_AI_Custom_MCP_Server_screenshot_11.png)
![](docs/302_AI_Custom_MCP_Server_screenshot_12.png)
MCP Server imported successfully, you can now use it normally.

## âœ¨ Features âœ¨
- ğŸ”§ Quickly generate your own MCP Server by selecting different APIs
- ğŸŒ Compatible with: Various MCP-supported clients, including 302.AI's chatbot
- ğŸ’» Currently includes multiple tool types such as BrowserUseTools, FileTools, ImageTools, MathTools, SandboxTools, with more updates coming


## Development

Install dependencies:

```bash
npm install
```

Build the server:

```bash
npm run build
```

For development with auto-rebuild:

```bash
npm run watch
```

## Installation

To use with Claude Desktop, add the server config:

On MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`
On Windows: `%APPDATA%/Claude/claude_desktop_config.json`

```json
{
  ""mcpServers"": {
    ""302ai-custom-mcp"": {
      ""command"": ""npx"",
      ""args"": [""-y"", ""@302ai/custom-mcp""],
      ""env"": {
        ""302AI_API_KEY"": ""YOUR_API_KEY_HERE"",
        ""LANGUAGE"": ""YOUR_LANGUAGE_HERE""
      }
    }
  }
}
```

To use with Cherry Studio, add the server config:

```json
{
  ""mcpServers"": {
    ""Li2ZXXJkvhAALyKOFeO4N"": {
      ""name"": ""302ai-custom-mcp"",
      ""description"": """",
      ""isActive"": true,
      ""registryUrl"": """",
      ""command"": ""npx"",
      ""args"": [
        ""-y"",
        ""@302ai/custom-mcp""
      ],
      ""env"": {
        ""302AI_API_KEY"": ""YOUR_API_KEY_HERE""
      }
    }
  }
}
```

To use with ChatWise, copy the following content to clipboard
```json
{
  ""mcpServers"": {
    ""302ai-custom-mcp"": {
      ""command"": ""npx"",
      ""args"": [""-y"", ""@302ai/custom-mcp""],
      ""env"": {
        ""302AI_API_KEY"": ""YOUR_API_KEY_HERE""
      }
    }
  }
}
```
Go to Settings -> Tools -> Add button -> Select Import from Clipboard
![](docs/302_AI_Custom_MCP_Server_screenshot_13.jpg)

### Find Your 302AI_API_KEY [here](https://dash.302.ai/apis/mcp-server)
[Using Tutorials](https://help.302.ai/docs/MCP-Server-de-shi-yong)

### Debugging

Since MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:

```bash
npm run inspector
```

The Inspector will provide a URL to access debugging tools in your browser.


## âœ¨ About 302.AI âœ¨
[302.AI](https://302.ai/en/) is an enterprise-oriented AI application platform that offers pay-as-you-go services, ready-to-use solutions, and an open-source ecosystem.âœ¨
1. ğŸ§  Integrates the latest and most comprehensive AI capabilities and brands, including but not limited to language models, image models, voice models, and video models.
2. ğŸš€ Develops deep applications based on foundation models - we develop real AI products, not just simple chatbots
3. ğŸ’° Zero monthly fee, all features are pay-per-use, fully open, achieving truly low barriers with high potential.
4. ğŸ›  Powerful management backend for teams and SMEs - one person manages, many people use.
5. ğŸ”— All AI capabilities provide API access, all tools are open source and customizable (in progress).
6. ğŸ’¡ Strong development team, launching 2-3 new applications weekly, products updated daily. Developers interested in joining are welcome to contact us.",2
https://mcp.so/server/302_sandbox_mcp/302ai,https://github.com/302ai/302_sandbox_mcp,302_sandbox_mcp,"Create a remote sandbox that can execute code/run commands/upload and download files.
åˆ›å»ºè¿œç¨‹æ²™ç›’ï¼Œå¯ä»¥æ‰§è¡Œä»£ç /è¿è¡Œå‘½ä»¤/ä¸Šä¼ ä¸‹è½½æ–‡ä»¶",Chinese,,302ai; sandbox; mcp; server,"what is 302AI Sandbox MCP? 302AI Sandbox MCP is a server designed for development and integration with Claude Desktop, allowing users to set up and run a model context protocol (MCP) server. how to use 302AI Sandbox MCP? To use the 302AI Sandbox MCP, install the necessary dependencies, build the server, and configure it with your Claude Desktop application by adding the server configuration to the appropriate config file based on your operating system. key features of 302AI Sandbox MCP? Easy installation and setup for developers Auto-rebuild feature for development Integration with Claude Desktop for enhanced functionality Debugging tools available through MCP Inspector use cases of 302AI Sandbox MCP? Developing and testing AI models in a controlled environment. Integrating custom AI solutions with existing applications. Debugging and optimizing AI server performance. FAQ from 302AI Sandbox MCP? How do I install the 302AI Sandbox MCP? You can install it by running npm install and then build the server with npm run build . Can I use this server on Windows? Yes! The server can be configured on both MacOS and Windows. Where can I find my API key? You can find your 302AI_API_KEY here .","# <p align=""center"">ğŸ¤– 302AI Sandbox MCP ServerğŸš€âœ¨</p>

<p align=""center"">An MCP service with code sandbox that allows AI assistants to safely execute arbitrary code.</p>

<p align=""center""><a href=""https://www.npmjs.com/package/@302ai/sandbox-mcp"" target=""blank""><img src=""https://file.302.ai/gpt/imgs/github/20250102/72a57c4263944b73bf521830878ae39a.png"" /></a></p >

<p align=""center""><a href=""README_zh.md"">ä¸­æ–‡</a> | <a href=""README.md"">English</a> | <a href=""README_ja.md"">æ—¥æœ¬èª</a></p>

![](docs/302_Sandbox_MCP_Server_en.png) 

## Previews

Here are some usage examples

![](docs/302_Sandbox_MCP_Server_en_screenshot_01.png)     

![](docs/302_Sandbox_MCP_Server_en_screenshot_02.png)     

Here is the list of supported tools

![](docs/302_Sandbox_MCP_Server_en_screenshot_03.png)


## âœ¨ Features âœ¨

- ğŸ”§ Dynamic Loading - Automatically update tool list from remote server.
- ğŸŒ Multi modes supported, you can use `stdin` mode locally, or host it as a remote HTTP server

## ğŸš€ Tool List
- [One-click Code Execution](https://302ai.apifox.cn/api-276039652)
- [Create Sandbox](https://302ai.apifox.cn/api-276079606)
- [Query Your Sandbox List](https://302ai.apifox.cn/api-276086526)
- [Destroy Sandbox](https://302ai.apifox.cn/api-276092957)
- [Run-Code](https://302ai.apifox.cn/api-276100061)
- [Run Command Line](https://302ai.apifox.cn/api-276106261)
- [Query File Information at Specified Path](https://302ai.apifox.cn/api-276110558)
- [Import File Data into Sandbox](https://302ai.apifox.cn/api-276123813)
- [Export Sandbox Files](https://302ai.apifox.cn/api-276123525)

## Development

Install dependencies:

```bash
npm install
```

Build the server:

```bash
npm run build
```

For development with auto-rebuild:

```bash
npm run watch
```

## Installation

To use with Claude Desktop, add the server config:

On MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`     
On Windows: `%APPDATA%/Claude/claude_desktop_config.json`

```json
{
  ""mcpServers"": {
    ""302ai-sandbox-mcp"": {
      ""command"": ""npx"",
      ""args"": [""-y"", ""@302ai/sandbox-mcp""],
      ""env"": {
        ""302AI_API_KEY"": ""YOUR_API_KEY_HERE""
      }
    }
  }
}
```

To use with Cherry Studio, add the server config:

```json
{
  ""mcpServers"": {
    ""Li2ZXXJkvhAALyKOFeO4N"": {
      ""name"": ""302ai-sandbox-mcp"",
      ""description"": """",
      ""isActive"": true,
      ""registryUrl"": """",
      ""command"": ""npx"",
      ""args"": [
        ""-y"",
        ""@302ai/sandbox-mcp@0.2.0""
      ],
      ""env"": {
        ""302AI_API_KEY"": ""YOUR_API_KEY_HERE""
      }
    }
  }
}
```

To use with ChatWise, copy the following content to clipboard
```json
{
  ""mcpServers"": {
    ""302ai-sandbox-mcp"": {
      ""command"": ""npx"",
      ""args"": [""-y"", ""@302ai/sandbox-mcp""],
      ""env"": {
        ""302AI_API_KEY"": ""YOUR_API_KEY_HERE""
      }
    }
  }
}
```
Go to Settings -> Tools -> Add button -> Select Import from Clipboard
![](docs/302_Sandbox_MCP_Server_en_screenshot_04.jpg)

### Find Your 302AI_API_KEY [here](https://dash.302.ai/apis/list)
[Using Tutorials](https://help.302.ai/en/docs/API-guan-li)

### Debugging

Since MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:

```bash
npm run inspector
```

The Inspector will provide a URL to access debugging tools in your browser.

## âœ¨ About 302.AI âœ¨
[302.AI](https://302.ai/en/) is an enterprise-oriented AI application platform that offers pay-as-you-go services, ready-to-use solutions, and an open-source ecosystem.âœ¨
1. ğŸ§  Integrates the latest and most comprehensive AI capabilities and brands, including but not limited to language models, image models, voice models, and video models.
2. ğŸš€ Develops deep applications based on foundation models - we develop real AI products, not just simple chatbots
3. ğŸ’° Zero monthly fee, all features are pay-per-use, fully open, achieving truly low barriers with high potential.
4. ğŸ›  Powerful management backend for teams and SMEs - one person manages, many people use.
5. ğŸ”— All AI capabilities provide API access, all tools are open source and customizable (in progress).
6. ğŸ’¡ Strong development team, launching 2-3 new applications weekly, products updated daily. Developers interested in joining are welcome to contact us.
",21
https://mcp.so/server/3d-printer-mcp-by-octoeverywhere/OctoEverywhere,https://github.com/OctoEverywhere/mcp,3D Printer MCP From OctoEverywhere,"A free, community 3D printing MCP server that allows for getting live printer state, webcam snapshots, and printer control.",English,developer-tools,3d-printing; mcp; octoeverywhere,"What is 3D Printer MCP From OctoEverywhere? 3D Printer MCP is a free community server that allows users to access and control their 3D printers securely via the Model Context Protocol (MCP). It provides live printer status, webcam snapshots, and control features for various 3D printer models. How to use 3D Printer MCP? To use the 3D Printer MCP, create an OctoEverywhere account, link your 3D printer, and obtain your MCP access token. You can then use the server URL and access token with any AI agent to manage your printer. Key features of 3D Printer MCP? Live 3D printer status and print information. Pause and cancel printer controls. Live webcam snapshots. Secure and private access. Compatibility with various 3D printers including OctoPrint, Klipper, and more. Free for the entire 3D printing community. Use cases of 3D Printer MCP? Monitoring print jobs remotely. Controlling printer functions from anywhere. Integrating with AI agents for enhanced printing capabilities. FAQ from 3D Printer MCP? Is 3D Printer MCP free to use? Yes! It is free for the entire 3D printing community. What types of printers are supported? It works with any 3D printer that supports the Model Context Protocol, including popular brands like Creality, Prusa, and AnyCubic. How do I get started? Create an account on OctoEverywhere, link your printer, and obtain your access token to start using the MCP.","<p align=""center""><img src=""https://octoeverywhere.com/img/logo.png"" alt=""OctoEverywhere's Logo"" style=""width:100px"" /></p>
<h1 align=""center"" style=""margin-bottom:20px""><a href=""https://octoeverywhere.com/mcp?utm_campaign=mcp_repo&utm_content=header&utm_source=github"">MCP For 3D Printing</a></h1>

[A free, private, and secure cloud MCP server for 3D printer access, monitoring, and control.](https://octoeverywhere.com/mcp?utm_campaign=mcp_repo&utm_content=intro&utm_source=github) OctoEverywhere's 3D printing MCP server enables access to your 3D printers via the Model Context Protocol (MCP) for AI chatbots, agents, and workflows. Link your 3D printer to OctoEverywhere, grab your MCP access token, and you're ready to chat!

## Features

- ğŸš€ Live 3D printer status and print information, including:
    - Printer state and status information.
    - Print progress, elapsed time, and estimated time to completion.
    - [Gadget AI](https://octoeverywhere.com/gadget?utm_campaign=mcp_repo&utm_content=gadget&utm_source=github) print failure detection status.
    - Hotend, bed, and chamber temperatures.
    - Print file information, including the file name.
    - Current layer and total layer information.
- ğŸ“· Live webcam snapshots
    - Supports multi-camera setups.
- â¸ï¸ Printer control including:
    - Pausing, resuming, and canceling print jobs.
- â¤ï¸ Works with any 3D printer, including:
    - OctoPrint
    - Klipper
    - Bambu Lab
    - Creality
    - Prusa
    - AnyCubic
    - Elegoo
    - And more
- ğŸ”’ Secure cloud MCP server:
    - Accessible from anywhere, in your  home or over the internet.
    - Easy setup - no local setup required.
    - Secure and private remote access.
- ğŸ˜ Free for the entire 3D printing community:
    - [OctoEverywhere](https://octoeverywhere.com/?utm_campaign=mcp_repo&utm_content=community&utm_source=github) builds awesome cloud tools free for the entire community!


## Try It Now

1) [Create an OctoEverywhere account](https://octoeverywhere.com/getstarted?utm_campaign=mcp_repo&utm_content=try_it_now&utm_source=github) and link your 3D printer.
2) [Visit the OctoEverywhere MCP setup page](https://octoeverywhere.com/mcp?utm_campaign=mcp_repo&utm_content=mcp_setup&utm_source=github) to get your Access Token.
3) Use the OctoEverywhere MCP server URL and Access Token with any AI agent!

## What's OctoEverywhere?

OctoEverywhere cloud empowers your [OctoPrint](https://octoeverywhere.com/?utm_campaign=mcp_repo&utm_content=octoprint&utm_source=github), [Klipper](https://octoeverywhere.com/klipper?utm_campaign=mcp_repo&utm_content=klipper&utm_source=github), [Bambu Lab](https://octoeverywhere.com/bambu?utm_campaign=mcp_repo&utm_content=bambu&utm_source=github), and [Elegoo Centauri](https://octoeverywhere.com/elegoo-centauri?utm_campaign=mcp_repo&utm_content=elegoo&utm_source=github) 3D printers with **free, private, unlimited remote access, AI print failure detection, and more!** OctoEverywhere is developed by the maker community for the maker community.

[Learn More About OctoEverywhere](https://octoeverywhere.com/?utm_campaign=mcp_repo&utm_content=learn_more&utm_source=github)
",20
https://mcp.so/server/4everland-hosting-mcp/4everland,https://github.com/4everland/4everland-hosting-mcp,4everland Hosting Mcp,"The 4EVERLAND Hosting MCP Server enables users to leverage AI-driven workflows to deploy code instantly to decentralized storage networks such as Greenfield, IPFS, and Arweave. Upon deployment, it provides a directly accessible webpage domain, streamlining the process of deploying and sharing decentralized applications.",English,,Hosting Servers; Developer Tools; web3,"A Model Context Protocol (MCP) server implementation for 4EVERLAND Hosting enabling instant deployment of AI-generated code to decentralized storage networks like Greenfield, IPFS, and Arweave. Overview
The 4EVERLAND Hosting MCP Server enables users to leverage AI-driven workflows to deploy code instantly to decentralized storage networks such as Greenfield, IPFS, and Arweave. Upon deployment, it provides a directly accessible webpage domain, streamlining the process of deploying and sharing decentralized applications. Features
Instant Deployment with Domain Generation: Deploy AI-generated code to decentralized storage networks and receive a unique, immediately accessible webpage domain with a single command.
Multiple Decentralized Storage Networks: Support for Greenfield, IPFS, and Arweave, enabling flexible and resilient storage options for your applications.
Secure and Loss-Proof Decentralized Storage: Leverage the robust, tamper-resistant, and highly available nature of decentralized storage to ensure data security and prevent data loss.
Visual Project Management Interface: Manage your deployed projects, view detailed information, or configure custom domains directly in the 4EVERLAND Dashboard.","# 4EVERLAND Hosting MCP Server

[![Version](https://img.shields.io/badge/version-0.1.4-blue.svg)](https://www.npmjs.com/package/@4everland/hosting-mcp)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Node](https://img.shields.io/badge/node-%3E%3D20.12.2-green.svg)](https://nodejs.org/)

---

A Model Context Protocol (MCP) server implementation
for [4EVERLAND Hosting](https://docs.4everland.org/hositng/what-is-hosting) enabling instant deployment of AI-generated
code to decentralized storage networks like Greenfield, IPFS, and Arweave.

## Overview

The 4EVERLAND Hosting MCP Server enables users to leverage AI-driven workflows to deploy code instantly to decentralized
storage networks such as Greenfield, IPFS, and Arweave. Upon deployment, it provides a directly accessible webpage
domain, streamlining the process of deploying and sharing decentralized applications.

## Features

- **Instant Deployment with Domain Generation**: Deploy AI-generated code to decentralized storage networks and receive
  a unique, immediately accessible webpage domain with a single command.
- **Multiple Decentralized Storage Networks**: Support for Greenfield, IPFS, and Arweave, enabling flexible and
  resilient storage options for your applications.
- **Secure and Loss-Proof Decentralized Storage**: Leverage the robust, tamper-resistant, and highly available nature of
  decentralized storage to ensure data security and prevent data loss.
- **Visual Project Management Interface**: Manage your deployed projects, view detailed information, or configure custom
  domains directly in the [4EVERLAND Dashboard](https://dashboard.4everland.org/).

## MCP Tool

### Tool: deploy_site

**Description**: Deploys code to 4EVERLAND hosting platforms.

| Parameter    | Type                         | Description                                                                   |
|--------------|------------------------------|-------------------------------------------------------------------------------|
| code_files   | Record&lt;string, string&gt; | Map of file paths to their content                                            |
| project_name | string                       | Project name (alphanumeric, underscore, hyphen; cannot start/end with hyphen) |
| project_id   | string (optional)            | Existing project ID to deploy to (new project created if omitted)             |
| platform     | ""IPFS""\|""AR""\|""GREENFIELD""   | Storage platform to deploy to (default: `""IPFS""`)                             |

## Get Hosting Auth Token

1. Log in to your [4EVERLAND Dashboard](https://dashboard.4everland.org/) account.
2. Go to **Hosting** -&gt; **Auth Token**.
3. Click on **+Create** to generate a new token.
4. Copy and save the token somewhere safe as it will only be shown once.

## Integration with Cursor

To connect to the MCP server from Cursor:

1. Open Cursor and go to **Settings** (gear icon in the top right).
2. Click on **MCP** in the left sidebar.
3. Click **Add new global MCP server**.
4. Enter the following details:

```json
{
  ""mcpServers"": {
    ""4ever-mcpserver"": {
      ""command"": ""npx"",
      ""args"": [
        ""-y"",
        ""@4everland/hosting-mcp@latest"",
        ""serve""
      ],
      ""env"": {
        ""TOKEN"": ""your-hosting-auth-token""
      }
    }
  }
}
```

## Integration with Claude Desktop

To connect to the MCP server from Claude Desktop:

1. Open Claude Desktop and go to **Settings**.
2. Click on **Developer** in the left sidebar.
3. Click the **Edit Config** button.
4. Add the following configuration to the `claude_desktop_config.json` file:

```json
{
  ""mcpServers"": {
    ""4ever-mcpserver"": {
      ""command"": ""npx"",
      ""args"": [
        ""-y"",
        ""@4everland/hosting-mcp@latest"",
        ""serve""
      ],
      ""env"": {
        ""TOKEN"": ""your-hosting-auth-token""
      }
    }
  }
}
```

5. Save the file and restart Claude Desktop.

## Local Development

To run the server locally for development:

```bash
# Clone repository
git clone https://github.com/4everland/4everland-hosting-mcp.git
cd 4everland-hosting-mcp

# Install dependencies
npm install

# Build the project
npm run build

# Run the server locally
npm run serve
```

## License

This project is licensed under the MIT License.",2
https://mcp.so/server/4oimage-mcp/Antipas,https://github.com/Antipas/4oimage-mcp,4o-image MCP Server,"An MCP server implementation that integrates with 4o-image API, enabling LLMs and other AI systems to generate and edit images through a standardized protocol. Create high-quality art, 3D characters, and custom images using simple text prompts.",English,research-and-data,,"what is 4o-image MCP Server? 4o-image MCP Server is an implementation that integrates with the 4o-image API, allowing LLMs and other AI systems to generate and edit images through a standardized protocol. Users can create high-quality art, 3D characters, and custom images using simple text prompts. how to use 4o-image MCP Server? To use the 4o-image MCP Server, register for an account at 4o-image.app to obtain an API key. Set the API key as an environment variable when running the server. You can then generate images by sending text prompts or edit existing images by providing a base64-encoded image along with a prompt. key features of 4o-image MCP Server? Text-to-Image Generation: Create images from text descriptions with AI. Image Editing: Transform existing images using text prompts. Real-time Progress Updates: Get feedback on generation status. Browser Integration: Automatically open generated images in your default browser. use cases of 4o-image MCP Server? Generating unique artwork based on user-defined descriptions. Creating 3D character models for games or animations. Editing images to enhance or modify visual elements based on user prompts. FAQ from 4o-image MCP Server? How do I get started with 4o-image MCP Server? Register at 4o-image.app to obtain your API key and follow the setup instructions. Can I use 4o-image MCP Server for commercial projects? Yes, you can use it for commercial purposes under the MIT License. What types of images can I generate? You can generate a wide variety of images, including art, characters, and more, based on your text prompts.","# 4o-image MCP Server

An MCP server implementation that integrates with 4o-image API, enabling LLMs and other AI systems to generate and edit images through a standardized protocol. Create high-quality art, 3D characters, and custom images using simple text prompts.

<a href=""https://glama.ai/mcp/servers/@Antipas/4oimage-mcp"">
  <img width=""380"" height=""200"" src=""https://glama.ai/mcp/servers/@Antipas/4oimage-mcp/badge"" alt=""mcp-4o-Image-Generator MCP server"" />
</a>

[![npm version](https://img.shields.io/npm/v/4oimage-mcp.svg)](https://www.npmjs.com/package/4oimage-mcp)
[![Node.js Version](https://img.shields.io/node/v/4oimage-mcp.svg)](https://nodejs.org)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## Features

* **Text-to-Image Generation**: Create images from text descriptions with AI
* **Image Editing**: Transform existing images using text prompts
* **Real-time Progress Updates**: Get feedback on generation status
* **Browser Integration**: Automatically open generated images in your default browser


## Tools

* **generateImage**
  * Generate images based on text prompts with optional image editing
  * Inputs:
    * `prompt` (string, required): Text description of the desired image
    * `imageBase64` (string, optional): Base64-encoded image for editing or style transfer

## Configuration

### Getting an API Key

1. Register for an account at [4o-image.app](https://4o-image.app/dashboard/)
2. Obtain your API key from the user dashboard
3. Set the API key as an environment variable when running the server

### Usage with Claude Desktop

Add this to your `claude_desktop_config.json`:

```json
{
  ""mcpServers"": {
    ""4o-image"": {
      ""command"": ""npx"",
      ""args"": [
        ""-y"",
        ""4oimage-mcp""
      ],
      ""env"": {
        ""API_KEY"": ""YOUR_API_KEY_HERE""
      }
    }
  }
}
```

## Example Usage

Here's an example of using this MCP server with Claude:

```
Generate an image of a dog running on the beach at sunset
```

Claude will use the MCP server to generate the image, which will automatically open in your default browser. You'll also get a direct link to the image in Claude's response.

For image editing, you can include a base image and prompt Claude to modify it:

```
Edit this image to make the sky more dramatic with storm clouds
```

## License

This MCP server is licensed under the MIT License. You are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License.",4
https://mcp.so/server/66julienmartin_MCP-server-Deepseek_R1/MCP-Mirror,https://github.com/MCP-Mirror/66julienmartin_MCP-server-Deepseek_R1,Deepseek R1 MCP Server,Mirror of,English,research-and-data,mcp-server; deepseek; language-model,"What is Deepseek R1 MCP Server? Deepseek R1 MCP Server is an implementation of a Model Context Protocol (MCP) server designed for the Deepseek R1 language model, which is optimized for reasoning tasks and supports a context window of 8192 tokens. How to use Deepseek R1 MCP Server? To use the server, you can install it via Smithery or manually clone the repository, set up your environment, and run the server with your API key. Key features of Deepseek R1 MCP Server? Advanced text generation capabilities with a large context window. Configurable parameters for text generation. Robust error handling with detailed messages. Full support for the MCP protocol. Integration with Claude Desktop. Support for multiple models (DeepSeek-R1 and DeepSeek-V3). Use cases of Deepseek R1 MCP Server? Generating complex text outputs for applications. Assisting in coding and mathematical calculations. Data cleaning and analysis tasks. Creative writing and poetry generation. FAQ from Deepseek R1 MCP Server? What are the prerequisites for using the server? You need Node.js (v18 or higher), npm, Claude Desktop, and a Deepseek API key. Can I use different models with this server? Yes, you can switch between DeepSeek-R1 and DeepSeek-V3 by modifying the model name in the configuration. How does the temperature parameter affect output? The temperature controls the randomness of the output; lower values are better for coding and math, while higher values are suited for creative tasks.","# Deepseek R1 MCP Server
[![smithery badge](https://smithery.ai/badge/@66julienmartin/mcp-server-deepseek_r1)](https://smithery.ai/server/@66julienmartin/mcp-server-deepseek_r1)

A Model Context Protocol (MCP) server implementation for the Deepseek R1 language model. Deepseek R1 is a powerful language model optimized for reasoning tasks with a context window of 8192 tokens.

Why Node.js?
This implementation uses Node.js/TypeScript as it provides the most stable integration with MCP servers. The Node.js SDK offers better type safety, error handling, and compatibility with Claude Desktop.

## Quick Start

### Installing via Smithery

To install Deepseek R1 for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@66julienmartin/mcp-server-deepseek_r1):

```bash
npx -y @smithery/cli install @66julienmartin/mcp-server-deepseek_r1 --client claude
```

### Installing manually
```bash
# Clone and install
git clone https://github.com/66julienmartin/MCP-server-Deepseek_R1.git
cd deepseek-r1-mcp
npm install

# Set up environment
cp .env.example .env  # Then add your API key

# Build and run
npm run build
```

## Prerequisites

- Node.js (v18 or higher)
- npm
- Claude Desktop
- Deepseek API key

## Model Selection

By default, this server uses the **deepseek-R1** model. If you want to use **DeepSeek-V3** instead, modify the model name in `src/index.ts`:

```typescript
// For DeepSeek-R1 (default)
model: ""deepseek-reasoner""

// For DeepSeek-V3
model: ""deepseek-chat""
```

## Project Structure

```
deepseek-r1-mcp/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index.ts             # Main server implementation
â”œâ”€â”€ build/                   # Compiled files
â”‚   â”œâ”€â”€ index.js
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ package.json
â”œâ”€â”€ package-lock.json
â””â”€â”€ tsconfig.json
```

## Configuration

1. Create a `.env` file:
```
DEEPSEEK_API_KEY=your-api-key-here
```

2. Update Claude Desktop configuration:
```json
{
  ""mcpServers"": {
    ""deepseek_r1"": {
      ""command"": ""node"",
      ""args"": [""/path/to/deepseek-r1-mcp/build/index.js""],
      ""env"": {
        ""DEEPSEEK_API_KEY"": ""your-api-key""
      }
    }
  }
}
```

## Development

```bash
npm run dev     # Watch mode
npm run build   # Build for production
```

## Features

- Advanced text generation with Deepseek R1 (8192 token context window)
- Configurable parameters (max_tokens, temperature)
- Robust error handling with detailed error messages
- Full MCP protocol support
- Claude Desktop integration
- Support for both DeepSeek-R1 and DeepSeek-V3 models

## API Usage

```typescript
{
  ""name"": ""deepseek_r1"",
  ""arguments"": {
    ""prompt"": ""Your prompt here"",
    ""max_tokens"": 8192,    // Maximum tokens to generate
    ""temperature"": 0.2     // Controls randomness
  }
}
```

## The Temperature Parameter

The default value of `temperature` is 0.2.

Deepseek recommends setting the `temperature` according to your specific use case:

| USE CASE | TEMPERATURE | EXAMPLE |
|----------|-------------|---------|
| Coding / Math | 0.0 | Code generation, mathematical calculations |
| Data Cleaning / Data Analysis | 1.0 | Data processing tasks |
| General Conversation | 1.3 | Chat and dialogue |
| Translation | 1.3 | Language translation |
| Creative Writing / Poetry | 1.5 | Story writing, poetry generation |

## Error Handling

The server provides detailed error messages for common issues:
- API authentication errors
- Invalid parameters
- Rate limiting
- Network issues

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

MIT
",1
https://mcp.so/server/8sleep-mcp/elizabethtrykin,https://github.com/elizabethtrykin/8sleep-mcp,Eight Sleep MCP,MCP server for 8sleep,English,research-and-data,8sleep; mcp; sleep-data,"What is Eight Sleep MCP? Eight Sleep MCP is a Model Context Protocol (MCP) server designed for accessing data from the Eight Sleep Pod, enabling users to manage their sleep data and device settings effectively. How to use Eight Sleep MCP? To use Eight Sleep MCP, clone the repository from GitHub, install the necessary dependencies, and configure your user ID along with other environment variables. You can then run the server to interact with your Eight Sleep Pod data. Key features of Eight Sleep MCP? Access to user profile and preferences Device control for the Eight Sleep Pod Temperature control settings Detailed sleep data retrieval and analysis Alarm management functionalities Use cases of Eight Sleep MCP? Monitoring sleep patterns and scores over time. Adjusting temperature settings for optimal sleep comfort. Managing alarms and sleep schedules. Analyzing heart rate and respiratory data during sleep. FAQ from Eight Sleep MCP? What prerequisites are needed to run Eight Sleep MCP? You need Node.js (v16+) and an Eight Sleep account to get started. Is there a way to avoid entering email/password for every request? Yes, by obtaining your user ID and adding it to the configuration, you can bypass email/password authentication for subsequent requests. What kind of data can I retrieve using Eight Sleep MCP? You can retrieve sleep data, device status, user preferences, and manage alarms.","# Eight Sleep MCP

A Model Context Protocol (MCP) server for accessing Eight Sleep Pod data.

## Setup

### Prerequisites
- Node.js (v16+)
- Eight Sleep account

### Installation
1. Clone the repository
2. Run:
```bash
npm install
npm run build
```

### Configuration

#### Getting Your User ID
You need to get your Eight Sleep user ID once and add it to your configuration. This prevents the client from having to authenticate with email/password on every request. You have two options:

Option 1: Direct API Call
1. ```bash
curl -X POST https://client-api.8slp.net/v1/auth/login \
  -H ""Content-Type: application/json"" \
  -d '{""email"":""your_email"",""password"":""your_password""}'
```

2. Add the user ID to your configuration as shown below.

Option 2: Using MCP Client
1. First set up your `.env` file without the user ID:
```env
EIGHT_SLEEP_EMAIL=your_email
EIGHT_SLEEP_PASSWORD=your_password
```

2. Run the MCP client once to get your user ID:
```bash
node build/index.js getUsers
```
The response will include your user ID. Save this value.

3. Add the user ID to your configuration as shown below.

#### Environment Variables
Create a `.env` file:

```env
# Eight Sleep Authentication
EIGHT_SLEEP_EMAIL=your_email
EIGHT_SLEEP_PASSWORD=your_password
EIGHT_SLEEP_USER_ID=your_user_id  # Required: Add the userId from one of the methods above
EIGHT_SLEEP_CLIENT_ID=your_client_id
EIGHT_SLEEP_CLIENT_SECRET=your_client_secret
```

### Claude Desktop Integration
Add to Claude Desktop's config (Settings â†’ Developer â†’ Edit Config):

```json
{
    ""mcpServers"": {
        ""eight_sleep"": {
            ""command"": ""node"",
            ""args"": [""/absolute/path/to/eight-sleep-mcp/build/index.js""],
            ""env"": {
                ""EIGHT_SLEEP_EMAIL"": ""your_email"", // email and password not required once you have userid
                ""EIGHT_SLEEP_PASSWORD"": ""your_password"", // email and password not required once you have userid
                ""EIGHT_SLEEP_USER_ID"": ""your_user_id"",
                ""EIGHT_SLEEP_CLIENT_ID"": ""your_client_id"", // optional
                ""EIGHT_SLEEP_CLIENT_SECRET"": ""your_client_secret"" // optional
            }
        }
    }
}
```

> **Important**: Adding your user ID to the configuration is required to avoid having to authenticate with email/password on every request. Make sure to get it using one of the methods above.

Restart Claude Desktop after saving.

## Available Functions

### User Information
- `getUsers` - Get user profile information
- `getUserPreferences` - Get user preferences (units, timezone, bed side)
- `updateUserPreferences` - Update user preferences

### Device Control
- `getDeviceStatus` - Get device status (online, firmware, water level)
- `setDevicePower` - Turn device on/off
- `getPresence` - Check if user is in bed

### Temperature Control
- `getTemperature` - Get current temperature settings
- `setTemperature` - Set immediate temperature (-100 to 100)
- `getTemperatureSchedules` - Get temperature schedules
- `setTemperatureSchedule` - Create temperature schedule
- `updateTemperatureSchedule` - Update temperature schedule
- `deleteTemperatureSchedule` - Delete temperature schedule

### Sleep Data
- `getSleepData` - Get detailed sleep data for date range
- `getSleepScore` - Get sleep score for a date
- `getSleepStages` - Get sleep stages (awake, light, deep, REM)
- `getHrv` - Get Heart Rate Variability data
- `getHeartRate` - Get heart rate data
- `getRespiratoryRate` - Get respiratory rate data
- `getSleepTiming` - Get bedtime and wake time
- `getSleepFitnessTrends` - Get sleep fitness trends

### Alarm Management
- `getAlarms` - Get all alarms
- `setAlarm` - Create new alarm
- `updateAlarm` - Update existing alarm
- `deleteAlarm` - Delete alarm

## Function Parameters

For date-based functions, use the format `YYYY-MM-DD`. For example:
```typescript
getSleepData({
  startDate: ""2024-03-15"",
  endDate: ""2024-03-16""  // optional
})
```

For temperature settings:
```typescript
setTemperature({
  level: 50,  // -100 to 100
  duration: 3600  // seconds, optional
})
```

For alarms:
```typescript
setAlarm({
  time: ""07:00"",
  daysOfWeek: [1,2,3,4,5],  // Mon-Fri
  vibration: true,
  sound: ""chime""  // optional
})
```

For temperature schedules:
```typescript
setTemperatureSchedule({
  startTime: ""22:00"",
  level: -20,
  daysOfWeek: [0,1,2,3,4,5,6]  // Every day
})
```",3
https://mcp.so/server/9506hqwy_redmine_mcp_server/MCP-Mirror,https://github.com/MCP-Mirror/9506hqwy_redmine_mcp_server,Redmine MCP Server,Mirror of,English,developer-tools,redmine; mcp; server; plugin,"what is Redmine MCP Server? Redmine MCP Server is a plugin that provides a Model Context Protocol server using Server Side Event for Redmine, allowing for real-time updates and interactions. how to use Redmine MCP Server? To use the Redmine MCP Server, download the plugin into the Redmine plugin directory and start Redmine. You can then access various tools to manage issues and wiki pages. key features of Redmine MCP Server? Tool list_issues : Lists all issues per project. Tool list_wiki_pages : Lists all wiki pages per project. Tool read_issue : Reads a specific issue. Tool read_wiki_page : Reads a specific wiki page. use cases of Redmine MCP Server? Managing project issues in real-time. Accessing and editing wiki pages dynamically. Integrating with other tools that utilize the Model Context Protocol. FAQ from Redmine MCP Server? Is the Redmine MCP Server secure? The HTTP endpoint does not have authentication, so it is recommended to use it in a secure environment. What databases are supported? The plugin has been tested with SQLite, MySQL 8.0, and PostgreSQL 12. Is this plugin stable? The plugin is currently experimental and may have issues depending on the version of dependencies.","# Redmine MCP Server

This plugin provides a Model Context Protocol server using Server Side Event.

## Notes

- This plugin is concept and experimental.
- HTTP endpoint does not have authenticate.
- Using WEBrick does not work (see [12.3.3 Streaming Considerations](https://guides.rubyonrails.org/v4.2/action_controller_overview.html)).
- `ActionController:Live` streaming has some problems depend on depnedencies library's version (see [Version 2.2.x breaks Rails Actioncontroller:Live streaming](https://github.com/rack/rack/issues/1619)).

## Features

- Tool `list_issues` is all issue listed per project.
- Tool `list_wiki_pages` is all wiki page listed per project.
- Tool `read_issue` is a issue read.
- Tool `read_wiki_page` is a wiki page read.

## Installation

1. Download plugin in Redmine plugin directory.
   ```sh
   git clone https://github.com/9506hqwy/redmine_mcp_server.git
   ```
2. Start Redmine

## Exampls

see [clients](./clients) directory.

## Tested Environment

* Redmine (Docker Image)
  * 6.0
* Database
  * SQLite
  * MySQL 8.0
  * PostgreSQL 12

## References

- [Model Context Protocol](https://modelcontextprotocol.io/introduction)
",0
https://mcp.so/server/9olidity_MCP-Server-Pentest/MCP-Mirror,https://github.com/MCP-Mirror/9olidity_MCP-Server-Pentest,MCP Server Pentest,Mirror of,English,security,9olidity; pentest; security-testing,"what is MCP Server Pentest? MCP Server Pentest is a security testing tool designed to automatically detect vulnerabilities such as XSS and SQL injection in web applications. how to use MCP Server Pentest? To use MCP Server Pentest, install the necessary dependencies using the provided commands, configure the tool, and then run tests against your target URLs to identify vulnerabilities. key features of MCP Server Pentest? Automatic detection of XSS and SQL injection vulnerabilities Ability to take screenshots of web pages or specific elements Comprehensive network interaction capabilities including navigation and form filling Monitoring of console logs and execution of JavaScript in the browser context use cases of MCP Server Pentest? Conducting security assessments on web applications to identify vulnerabilities. Automating the testing process for web application security. Generating reports on security findings for developers and stakeholders. FAQ from MCP Server Pentest? What types of vulnerabilities can MCP Server Pentest detect? MCP Server Pentest can detect XSS and SQL injection vulnerabilities among others. Is MCP Server Pentest easy to set up? Yes! The installation process is straightforward and includes configuration steps. Can I use MCP Server Pentest for any web application? Yes! MCP Server Pentest can be used on any web application that you have permission to test.","<h1 align=""center"">MCP Server Pentest</h1>


## Features

- Full browser xss, sql vulnerability automatic detection
- Screenshots of the entire page or specific elements
- Comprehensive network interaction (navigation, clicks, form filling)
- Console log monitoring
- JavaScript execution in the browser context

## Installation

### Installing 

```
npx playwright install firefox
yarn install 
npm run build 
```

## Configuration

The installation process will automatically add the following configuration to your Claude config file:

```json
{
  ""mcpServers"": {
    ""playwright"": {
      ""command"": ""npx"",
      ""args"": [
        ""-y"",
        ""/Users/...../dist/index.js""
      ],
      ""disabled"": false,
      ""autoApprove"": []
    }
  }
}
```

## Components

### Tools


#### `broser_url_reflected_xss`
Test whether the URL has an XSS vulnerability
```javascript
{
  ""url"": ""https://test.com"",
  ""paramName"":""text""
}
```
![](xss.png)


#### `browser_url_sql_injection`

Test whether the URL has SQL injection vulnerabilities

```javascript
{
  ""url"": ""https://test.com"",
  ""paramName"":""text""
}
```

![](sql.png)




#### `browser_navigate`
Navigate to any URL in the browser
```javascript
{
  ""url"": ""https://stealthbrowser.cloud""
}
```

#### `browser_screenshot`
Capture screenshots of the entire page or specific elements
```javascript
{
  ""name"": ""screenshot-name"",     // required
  ""selector"": ""#element-id"",     // optional
  ""fullPage"": true              // optional, default: false
}
```

#### `browser_click`
Click elements on the page using CSS selector
```javascript
{
  ""selector"": ""#button-id""
}
```

#### `browser_click_text`
Click elements on the page by their text content
```javascript
{
  ""text"": ""Click me""
}
```

#### `browser_hover`
Hover over elements on the page using CSS selector
```javascript
{
  ""selector"": ""#menu-item""
}
```

#### `browser_hover_text`
Hover over elements on the page by their text content
```javascript
{
  ""text"": ""Hover me""
}
```

#### `browser_fill`
Fill out input fields
```javascript
{
  ""selector"": ""#input-field"",
  ""value"": ""Hello World""
}
```

#### `browser_select`
Select an option in a SELECT element using CSS selector
```javascript
{
  ""selector"": ""#dropdown"",
  ""value"": ""option-value""
}
```

#### `browser_select_text`
Select an option in a SELECT element by its text content
```javascript
{
  ""text"": ""Choose me"",
  ""value"": ""option-value""
}
```

#### `browser_evaluate`
Execute JavaScript in the browser console
```javascript
{
  ""script"": ""document.title""
}
```


",0
https://mcp.so/server/@ragrabbit/mcp/packages,,404,,,,,,,
https://mcp.so/server/A2AMCP/webdevtodayjason,https://github.com/webdevtodayjason/A2AMCP,SplitMind MCP Agent Communication Server (Redis Edition),A2AMCP is a Agent2Agent MCP communication Server taking the concept from Google's Agent2Agent Protocol (A2A),English,developer-tools,mcp; agent-communication; redis,"What is A2AMCP? A2AMCP is a persistent, multi-project MCP (Message Communication Protocol) server that facilitates real-time communication between AI agents working on parallel tasks, inspired by Google's Agent2Agent Protocol (A2A). How to use A2AMCP? To use A2AMCP, clone the repository, set up the Docker environment, and start the services. Configure your agents to connect to the MCP server for task management and communication. Key features of A2AMCP? Multi-project support with isolated namespaces. Persistent state management using Redis. Todo list management for each agent. Automatic cleanup of inactive agents. Easy deployment with Docker. Real-time monitoring through an optional Redis Commander UI. Use cases of A2AMCP? Managing multiple AI agents in a collaborative project. Coordinating tasks and communication between agents in real-time. Monitoring agent activity and task progress through a web interface. FAQ from A2AMCP? Can A2AMCP handle multiple projects simultaneously? Yes! A2AMCP supports multiple projects with isolated namespaces for each. Is A2AMCP easy to deploy? Yes! A2AMCP can be easily deployed using Docker, making setup straightforward. How does A2AMCP ensure data persistence? A2AMCP uses Redis as a backend to maintain persistent data across server restarts.","# A2AMCP - Agent-to-Agent Model Context Protocol

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Docker](https://img.shields.io/badge/Docker-Ready-blue.svg)](https://www.docker.com/)
[![Redis](https://img.shields.io/badge/Redis-Powered-red.svg)](https://redis.io/)
[![Status](https://img.shields.io/badge/Status-Running%20%E2%9C%85-green.svg)](https://github.com/webdevtodayjason/A2AMCP)

## Enabling Seamless Multi-Agent Collaboration for AI-Powered Development

A2AMCP brings Google's Agent-to-Agent (A2A) communication concepts to the Model Context Protocol (MCP) ecosystem, enabling AI agents to communicate, coordinate, and collaborate in real-time while working on parallel development tasks.

Originally created for [SplitMind](https://github.com/webdevtodayjason/splitmind), A2AMCP solves the critical problem of isolated AI agents working on the same codebase without awareness of each other's changes.

**âœ… Server Status: WORKING! All 17 tools implemented and tested. Uses modern MCP SDK 1.9.3.**

## ğŸš€ Quick Start

### Using Docker (Recommended)

```bash
# Clone the repository
git clone https://github.com/webdevtodayjason/A2AMCP
cd A2AMCP

# Start the server
docker-compose up -d

# Verify it's running
docker ps | grep splitmind

# Test the connection
python verify_mcp.py
```

### Configure Your Agents

#### Claude Code (CLI)
```bash
# Add the MCP server using Claude Code CLI
claude mcp add splitmind-a2amcp \
  -e REDIS_URL=redis://localhost:6379 \
  -- docker exec -i splitmind-mcp-server python /app/mcp-server-redis.py
```

#### Claude Desktop
Add to your configuration file (`~/Library/Application Support/Claude/claude_desktop_config.json` on macOS):

```json
{
  ""mcpServers"": {
    ""splitmind-a2amcp"": {
      ""command"": ""docker"",
      ""args"": [""exec"", ""-i"", ""splitmind-mcp-server"", ""python"", ""/app/mcp-server-redis.py""],
      ""env"": {
        ""REDIS_URL"": ""redis://redis:6379""
      }
    }
  }
}
```

## ğŸ¯ What Problem Does A2AMCP Solve?

When multiple AI agents work on the same codebase:
- **Without A2AMCP**: Agents create conflicting code, duplicate efforts, and cause merge conflicts
- **With A2AMCP**: Agents coordinate, share interfaces, prevent conflicts, and work as a team

### Generic Use Cases Beyond SplitMind

A2AMCP can coordinate any multi-agent scenario:
- **Microservices**: Different agents building separate services
- **Full-Stack Apps**: Frontend and backend agents collaborating
- **Documentation**: Multiple agents creating interconnected docs
- **Testing**: Test writers coordinating with feature developers
- **Refactoring**: Agents working on different modules simultaneously

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   A2AMCP Server â”‚ â† Persistent Redis-backed MCP server
â”‚   (Port 5050)   â”‚   handling all agent communication
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ STDIO Protocol (MCP)
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼         â–¼         â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Agent 1 â”‚â”‚Agent 2 â”‚â”‚Agent 3 â”‚â”‚Agent N â”‚
â”‚Auth    â”‚â”‚Profile â”‚â”‚API     â”‚â”‚Frontendâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ Core Features

### 1. **Real-time Agent Communication**
- Direct queries between agents
- Broadcast messaging
- Async message queues

### 2. **File Conflict Prevention**
- Automatic file locking
- Conflict detection
- Negotiation strategies

### 3. **Shared Context Management**
- Interface/type registry
- API contract sharing
- Dependency tracking

### 4. **Task Transparency**
- Todo list management
- Progress visibility
- Completion tracking
- Task completion signaling

### 5. **Multi-Project Support**
- Isolated project namespaces
- Redis-backed persistence
- Automatic cleanup

### 6. **Modern MCP Integration**
- Uses MCP SDK 1.9.3 with proper decorators
- `@server.list_tools()` and `@server.call_tool()` patterns
- STDIO-based communication protocol
- Full A2AMCP API compliance with 17 tools implemented

## ğŸ“¦ Installation Options

### Docker Compose (Production)
```yaml
services:
  mcp-server:
    build: .
    container_name: splitmind-mcp-server
    ports:
      - ""5050:5000""  # Changed from 5000 to avoid conflicts
    environment:
      - REDIS_URL=redis://redis:6379
      - LOG_LEVEL=INFO
    depends_on:
      redis:
        condition: service_healthy
    restart: unless-stopped
  
  redis:
    image: redis:7-alpine
    container_name: splitmind-redis
    ports:
      - ""6379:6379""
    volumes:
      - redis-data:/data
    healthcheck:
      test: [""CMD"", ""redis-cli"", ""ping""]
      interval: 10s
      timeout: 5s
      retries: 5

volumes:
  redis-data:
    driver: local
```

### Python SDK
```bash
pip install a2amcp-sdk
```

### JavaScript/TypeScript SDK (Coming Soon)
```bash
npm install @a2amcp/sdk
```

## ğŸš¦ Usage Example

### Python SDK
```python
from a2amcp import A2AMCPClient, Project, Agent

async def run_agent():
    client = A2AMCPClient(""localhost:5000"")
    project = Project(client, ""my-app"")
    
    async with Agent(project, ""001"", ""feature/auth"", ""Build authentication"") as agent:
        # Agent automatically registers and maintains heartbeat
        
        # Coordinate file access
        async with agent.files.coordinate(""src/models/user.ts"") as file:
            # File is locked, safe to modify
            pass
        # File automatically released
        
        # Share interfaces
        await project.interfaces.register(
            agent.session_name,
            ""User"",
            ""interface User { id: string; email: string; }""
        )
```

### Direct MCP Tool Usage
```python
# Register agent
register_agent(""my-project"", ""task-001"", ""001"", ""feature/auth"", ""Building authentication"")

# Query another agent
query_agent(""my-project"", ""task-001"", ""task-002"", ""interface"", ""What's the User schema?"")

# Share interface
register_interface(""my-project"", ""task-001"", ""User"", ""interface User {...}"")
```

## ğŸ“š Documentation

- [Claude Code Setup Guide](./CLAUDE_CODE_SETUP.md)
- [Installation & Setup](./SETUP.md)
- [Full API Reference](https://github.com/webdevtodayjason/A2AMCP/blob/main/docs/API_REFERENCE.md)
- [Python SDK Documentation](https://github.com/webdevtodayjason/A2AMCP/blob/main/sdk/python/README.md)
- [Architecture Overview](https://github.com/webdevtodayjason/A2AMCP/blob/main/docs/ARCHITECTURE.md)
- [SplitMind Integration Guide](https://github.com/webdevtodayjason/A2AMCP/blob/main/docs/SPLITMIND_INTEGRATION.md)

## ğŸ› ï¸ SDKs and Tools

### Available Now
- **Python SDK**: Full-featured SDK with async support
- **Docker Deployment**: Production-ready containers

### In Development
- **JavaScript/TypeScript SDK**: For Node.js and browser
- **CLI Tools**: Command-line interface for monitoring
- **Go SDK**: High-performance orchestration
- **Testing Framework**: Mock servers and test utilities

See [SDK Development Progress](https://github.com/webdevtodayjason/A2AMCP/blob/main/sdk/TODO.md) for details.

## ğŸ¤ Integration with AI Frameworks

A2AMCP is designed to work with:
- [SplitMind](https://github.com/webdevtodayjason/splitmind) - Original use case
- Claude Code (via MCP)
- Any MCP-compatible AI agent
- Future: LangChain, CrewAI, AutoGen

## ğŸ” How It Differs from A2A

While inspired by Google's A2A protocol, A2AMCP makes specific design choices for AI code development:

| Feature | Google A2A | A2AMCP |
|---------|------------|---------|
| Protocol | HTTP-based | MCP tools |
| State | Stateless | Redis persistence |
| Focus | Generic tasks | Code development |
| Deployment | Per-agent servers | Single shared server |

## ğŸš€ Roadmap

- [x] Core MCP server with Redis
- [x] Modern MCP SDK 1.9.3 integration
- [x] Fixed decorator patterns (`@server.list_tools()`, `@server.call_tool()`)
- [x] Python SDK
- [x] Docker deployment
- [x] All 17 A2AMCP API tools implemented and tested
- [x] Health check endpoint for monitoring
- [x] Verification script for testing connectivity
- [ ] JavaScript/TypeScript SDK
- [ ] CLI monitoring tools
- [ ] SplitMind native integration
- [ ] Framework adapters (LangChain, CrewAI)
- [ ] Enterprise features

## ğŸ› ï¸ Troubleshooting

### Agents can't see `mcp__splitmind-a2amcp__` tools

1. **Restart Claude Desktop** - MCP connections are established at startup
2. **Verify server is running**: `docker ps | grep splitmind`
3. **Check health endpoint**: `curl http://localhost:5050/health`
4. **Run verification script**: `python verify_mcp.py`
5. **Check configuration**: Ensure `~/Library/Application Support/Claude/claude_desktop_config.json` contains the A2AMCP server configuration

### Common Issues

- **""Tool 'X' not yet implemented""** - Fixed in latest version, pull latest changes
- **Connection failed** - Ensure Docker is running and ports 5050/6379 are free
- **Redis connection errors** - Wait for Redis to be ready (takes ~5-10 seconds on startup)

## ğŸ¤ Contributing

We welcome contributions! See [CONTRIBUTING.md](https://github.com/webdevtodayjason/A2AMCP/blob/main/CONTRIBUTING.md) for guidelines.

### Development Setup
```bash
# Clone repository
git clone https://github.com/webdevtodayjason/A2AMCP
cd A2AMCP

# Install dependencies
pip install -r requirements.txt

# Run tests
pytest

# Start development server
docker-compose -f docker-compose.dev.yml up
```

## ğŸ“Š Performance

- Handles 100+ concurrent agents
- Sub-second message delivery
- Automatic cleanup of dead agents
- Horizontal scaling ready

## ğŸ”’ Security

- Project isolation
- Optional authentication (coming soon)
- Encrypted communication (roadmap)
- Audit logging

## ğŸ“„ License

MIT License - see [LICENSE](https://github.com/webdevtodayjason/A2AMCP/blob/main/LICENSE) file.

## ğŸ™ Acknowledgments

- Inspired by [Google's A2A Protocol](https://github.com/google/A2A)
- Built for [SplitMind](https://github.com/webdevtodayjason/splitmind)
- Powered by [Model Context Protocol](https://modelcontextprotocol.io)

## ğŸ“ Support

- **Issues**: [GitHub Issues](https://github.com/webdevtodayjason/A2AMCP/issues)
- **Discussions**: [GitHub Discussions](https://github.com/webdevtodayjason/A2AMCP/discussions)
- **Discord**: Coming soon

---

*A2AMCP - Turning isolated AI agents into coordinated development teams*",18
https://mcp.so/server/ABComponentServer/python,,,Home Servers Clients Categories Tags Feed,English,,,,,
https://mcp.so/server/ACC_MCP_V0.2/itaico82,,404,,,,,,,
https://mcp.so/server/ACP-MCP-Server/GongRzhe,https://github.com/GongRzhe/ACP-MCP-Server,ACP-MCP-Server,"A bridge server that connects Agent Communication Protocol (ACP) agents with Model Context Protocol (MCP) clients, enabling seamless integration between ACP-based AI agents and MCP-compatible tools like Claude Desktop.",English,developer-tools,acp-mcp; bridge-server; protocol-integration,"What is ACP-MCP-Server? ACP-MCP-Server is a bridge server that connects Agent Communication Protocol (ACP) agents with Model Context Protocol (MCP) clients, facilitating seamless integration between ACP-based AI agents and MCP-compatible tools like Claude Desktop. How to use ACP-MCP-Server? To use ACP-MCP-Server, install it via PyPI and run it with the desired transport mode (STDIO, SSE, or Streamable HTTP) to connect with your ACP agents. Key features of ACP-MCP-Server? Protocol Bridge : Connects ACP agents with MCP clients. Multiple Transports : Supports STDIO, SSE, and Streamable HTTP. Agent Discovery : Automatically discovers and registers ACP agents. Smart Routing : Routes requests intelligently to appropriate agents. Async Support : Supports both synchronous and asynchronous operations. Interactive Sessions : Enables multi-turn interactions with agents. Multi-Modal : Handles various content types including text and images. Use cases of ACP-MCP-Server? Integrating AI agents with desktop applications like Claude Desktop. Enabling real-time communication between different AI protocols. Facilitating multi-agent interactions in complex workflows. FAQ from ACP-MCP-Server? What protocols does ACP-MCP-Server support? It supports Agent Communication Protocol (ACP) and Model Context Protocol (MCP). Is there a Docker version available? Yes, you can run ACP-MCP-Server using Docker for easier deployment. What are the system requirements? You need Python 3.11+ and a running ACP server with agents.","# ACP-MCP-Server

![](https://badge.mcpx.dev?type=server ""MCP Server"")
[![PyPI version](https://badge.fury.io/py/acp-mcp-server.svg)](https://badge.fury.io/py/acp-mcp-server)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A bridge server that connects **Agent Communication Protocol (ACP)** agents with **Model Context Protocol (MCP)** clients, enabling seamless integration between ACP-based AI agents and MCP-compatible tools like Claude Desktop.

## âœ¨ Features

- ğŸ”„ **Protocol Bridge**: Seamlessly connects ACP agents with MCP clients
- ğŸš€ **Multiple Transports**: Supports STDIO, SSE, and Streamable HTTP
- ğŸ¤– **Agent Discovery**: Automatic discovery and registration of ACP agents
- ğŸ§  **Smart Routing**: Intelligent routing of requests to appropriate agents
- ğŸ”„ **Async Support**: Full support for synchronous and asynchronous operations
- ğŸ’¬ **Interactive Sessions**: Support for multi-turn agent interactions
- ğŸŒ **Multi-Modal**: Handle text, images, and other content types

## ğŸš€ Quick Start

### Installation

```bash
# Install from PyPI
pip install acp-mcp-server

# Or use uvx for isolated execution
uvx acp-mcp-server
```

### Basic Usage

```bash
# Run with STDIO (default, for Claude Desktop)
acp-mcp-server

# Run with SSE transport
acp-mcp-server --transport sse --port 8000

# Run with HTTP transport
acp-mcp-server --transport streamable-http --host 0.0.0.0 --port 9000

# Connect to different ACP server
acp-mcp-server --acp-url http://localhost:8001
```

### Using with Claude Desktop

Add to your Claude Desktop configuration:

```json
{
  ""mcpServers"": {
    ""acp-bridge"": {
      ""command"": ""uvx"",
      ""args"": [""acp-mcp-server""]
    }
  }
}
```

## ğŸ“‹ Requirements

- Python 3.11+
- Running ACP server with agents
- FastMCP for protocol implementation

## ğŸ”§ Configuration

### Environment Variables

- `ACP_BASE_URL`: ACP server URL (default: `http://localhost:8000`)

### Command Line Options

```
usage: acp-mcp-server [-h] [--transport {stdio,sse,streamable-http}] [--host HOST] [--port PORT] [--path PATH] [--acp-url ACP_URL] [--version]

options:
  -h, --help            show this help message and exit
  --transport {stdio,sse,streamable-http}
                        Transport protocol (default: stdio)
  --host HOST           Host address for HTTP transports (default: 127.0.0.1)
  --port PORT           Port number for HTTP transports (default: 8000)
  --path PATH           URL path for HTTP transports (default: /mcp)
  --acp-url ACP_URL     ACP server URL (default: http://localhost:8000)
  --version             show program's version number and exit
```

## ğŸ› ï¸ Available Tools

The bridge server provides several MCP tools:

### Agent Management
- `discover_acp_agents`: Discover available ACP agents
- `get_agent_info`: Get detailed information about specific agents

### Agent Execution
- `run_acp_agent`: Execute agents in sync/async modes
- `get_async_run_result`: Retrieve results from async executions
- `list_active_runs`: List all active agent runs

### Smart Routing
- `smart_route_request`: Intelligently route requests to best agents
- `test_routing`: Test routing logic without execution
- `add_routing_rule`: Add custom routing rules
- `list_routing_strategies`: View all routing strategies

### Interactive Sessions
- `start_interactive_agent`: Start interactive agent sessions
- `provide_user_input`: Provide input to waiting agents
- `list_pending_interactions`: View pending interactions

### Message Processing
- `convert_acp_message`: Convert between ACP and MCP formats
- `analyze_message_content`: Analyze message structure and content

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MCP Client    â”‚    â”‚  ACP-MCP Bridge â”‚    â”‚   ACP Agents    â”‚
â”‚ (Claude Desktop)â”‚â—„â”€â”€â–ºâ”‚     Server      â”‚â—„â”€â”€â–ºâ”‚ (echo, chat,    â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚  translate...)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                         â”‚                       â”‚
   MCP Protocol            Protocol Bridge         ACP Protocol
  (STDIO/SSE/HTTP)        (FastMCP + aiohttp)    (HTTP/WebSocket)
```

## ğŸ”Œ Transport Modes

### STDIO (Default)
Perfect for Claude Desktop integration:
```bash
acp-mcp-server
```

### SSE (Server-Sent Events)
For web applications and streaming:
```bash
acp-mcp-server --transport sse --port 8000
```

### Streamable HTTP
For REST API integration:
```bash
acp-mcp-server --transport streamable-http --port 9000
```

## ğŸ³ Docker

### Quick Start with Docker

```bash
# Build the image
docker build -t acp-mcp-server .

# Run with Streamable HTTP transport
docker run -p 9000:9000 acp-mcp-server

# Run with SSE transport
docker run -p 8000:8000 acp-mcp-server \
  --transport sse --host 0.0.0.0 --port 8000

# Connect to custom ACP server
docker run -p 9000:9000 -e ACP_BASE_URL=http://my-acp-server:8001 acp-mcp-server
```

### Using Docker Compose

```bash
# Run HTTP transport service
docker-compose up acp-mcp-http

# Run SSE transport service
docker-compose up acp-mcp-sse

# Run both services
docker-compose up

# Run development mode with live code reload
docker-compose --profile dev up acp-mcp-dev
```

### Production Docker Image

For production deployments, use the multi-stage Dockerfile:

```bash
# Build production image
docker build -f Dockerfile.prod -t acp-mcp-server:prod .

# Run production container
docker run -d \
  --name acp-mcp-server \
  --restart unless-stopped \
  -p 9000:9000 \
  -e ACP_BASE_URL=http://your-acp-server:8000 \
  acp-mcp-server:prod
```


## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ”— Related Projects

- [FastMCP](https://github.com/jlowin/fastmcp) - Fast, Pythonic MCP server framework
- [ACP SDK](https://github.com/i-am-bee/acp) - Agent Communication Protocol SDK
- [Claude Desktop](https://claude.ai/desktop) - AI assistant with MCP support

## ğŸ“ Support

- ğŸ› [Report Issues](https://github.com/GongRzhe/ACP-MCP-Server/issues)
- ğŸ’¬ [Discussions](https://github.com/GongRzhe/ACP-MCP-Server/discussions)
- ğŸ“– [Documentation](https://github.com/GongRzhe/ACP-MCP-Server#readme)
",13
https://mcp.so/server/AER-MCP/SepineTam,https://github.com/SepineTam/AER-MCP,AER-MCP,A MCP Server for finding from AEA.,English,research-and-data,mcp; search; academic,"what is AER-MCP? AER-MCP is a server designed for searching academic papers from the AEA (American Economic Association) using a Large Language Model (LLM). how to use AER-MCP? To use AER-MCP, clone the repository from GitHub, set up a virtual environment, and configure the server with the appropriate command and arguments to run the search functionality. key features of AER-MCP? Search academic papers from AEA using LLM technology. Easy setup with a virtual environment. Configuration options for server commands and arguments. use cases of AER-MCP? Researchers looking for specific papers from the AEA. Academics wanting to explore economic research efficiently. Developers creating applications that require access to AEA papers. FAQ from AER-MCP? What is the purpose of AER-MCP? AER-MCP helps users find academic papers from the AEA using advanced search capabilities. Is AER-MCP free to use? Yes! AER-MCP is open-source and free to use for everyone. What programming language is AER-MCP written in? AER-MCP is developed in Python.","# AER-MCP

A MCP Server for finding from AEA.

> ğŸ” Search papers from AEA with LLM
> 
> ğŸ› ï¸ Still Under Developing
> 
> ğŸ’¡ Inspiration: [arxiv-mcp-server](https://github.com/blazickjp/arxiv-mcp-server)

## ğŸ’¡ Quickly Start
### Localization
```bash
git clone https://github.com/sepinetam/aer-mcp.git
cd aer-mcp
uv venv .venv
```

Then, config it
```json
{
  ""mcpServers"": {
    ""aer-mcp"": {
      ""command"": ""uv"",
      ""args"": [
        ""--directory"",
        ""/path/to/the/repo/"",
        ""run"",
        ""aer_mcp.py""
      ]
    }
  }
}
```

## â›“ï¸ More Academic MCP Servers
- [Stata-MCP](https://github.com/sepinetam/stata-mcp)
- [NBER-MCP](https://github.com/sepinetam/nber-mcp)
- [arxiv-mcp-server](https://github.com/blazickjp/arxiv-mcp-server)


",5
https://mcp.so/server/AI-FE_dify-mcp-server/MCP-Mirror,https://github.com/MCP-Mirror/AI-FE_dify-mcp-server,dify-server MCP æœåŠ¡å™¨,Mirror of,English,developer-tools,dify; mcp-server; AI-API,"what is Dify-Server? Dify-Server is a Model Context Protocol (MCP) server integrated with the Dify AI API, designed to generate Ant Design business component code. how to use Dify-Server? To use Dify-Server, install the dependencies, run the server, and configure it in your application settings to integrate with the Dify AI API. key features of Dify-Server? Integration with Dify AI API for chat completion functionality Support for text and image input Streamed response handling Code generation for Ant Design components use cases of Dify-Server? Generating business component code for web applications using Ant Design. Enhancing user interaction with chat functionalities powered by AI. Supporting image uploads for more dynamic input processing. FAQ from Dify-Server? What programming language is Dify-Server built with? Dify-Server is built using TypeScript. How do I install Dify-Server? You can install it by running npm install in your project directory. Can I use Dify-Server for other frameworks? While it is designed for Ant Design, you can adapt the generated code for other frameworks as needed.","# dify-server MCP æœåŠ¡å™¨

ä¸€ä¸ªé›†æˆ Dify AI API çš„ Model Context Protocol æœåŠ¡å™¨

è¿™æ˜¯ä¸€ä¸ªåŸºäº TypeScript çš„ MCP æœåŠ¡å™¨ï¼Œé€šè¿‡é›†æˆ Dify AI API æ¥æä¾› Ant Design ä¸šåŠ¡ç»„ä»¶çš„ä»£ç ç”Ÿæˆèƒ½åŠ›ã€‚å®ƒå±•ç¤ºäº†ä»¥ä¸‹æ ¸å¿ƒ MCP æ¦‚å¿µï¼š

- é›†æˆ Dify AI API å®ç°èŠå¤©å®ŒæˆåŠŸèƒ½
- æ”¯æŒæ–‡æœ¬å’Œå›¾ç‰‡è¾“å…¥
- æµå¼å“åº”å¤„ç†

## åŠŸèƒ½ç‰¹æ€§

### Tools

- `antd-component-codegen-mcp-tool` - ç”Ÿæˆ Ant Design ä¸šåŠ¡ç»„ä»¶ä»£ç 
  - æ”¯æŒæ–‡æœ¬å’Œå¯é€‰çš„å›¾ç‰‡è¾“å…¥
  - å¤„ç†å›¾ç‰‡æ–‡ä»¶ä¸Šä¼ 
  - æ”¯æŒæ¥è‡ª Dify AI API çš„æµå¼å“åº”

## å¼€å‘æŒ‡å—

å®‰è£…ä¾èµ–ï¼š

```bash
npm install
```

å¼€å‘æ¨¡å¼ï¼ˆè‡ªåŠ¨é‡æ–°æ„å»ºï¼‰ï¼š

```bash
npm run watch
```

æ„å»ºæœåŠ¡å™¨ï¼š

```bash
npm run build
```

## å®‰è£…è¯´æ˜

### åœ¨ Continue ä¸­é›†æˆ

åœ¨`~/.continue/config.json`ä¸­æ·»åŠ ä»¥ä¸‹é…ç½®ï¼š

```json
{
  ""experimental"": {
    ""modelContextProtocolServers"": [
      {
        ""transport"": {
          ""type"": ""stdio"",
          ""command"": ""node"",
          ""args"": [""your/path/dify-server/build/index.js""],
          ""env"": {
            ""DIFY_API_KEY"": ""***""
          }
        }
      }
    ]
  }
}
```

### åœ¨ Cline ä¸­é›†æˆ

åœ¨`your/path/cline_mcp_settings.json`ä¸­æ·»åŠ ä»¥ä¸‹é…ç½®ï¼š

```json
{
  ""mcpServers"": {
    ""dify-server"": {
      ""command"": ""node"",
      ""args"": [""your/path/dify-server/build/index.js""],
      ""env"": {
        ""DIFY_API_KEY"": ""***""
      }
    }
  }
}
```

### è°ƒè¯•

ç”±äº MCP æœåŠ¡å™¨é€šè¿‡æ ‡å‡†è¾“å…¥è¾“å‡ºï¼ˆstdioï¼‰è¿›è¡Œé€šä¿¡ï¼Œè°ƒè¯•å¯èƒ½ä¼šæ¯”è¾ƒå›°éš¾ã€‚æˆ‘ä»¬æ¨èä½¿ç”¨ [MCP Inspector](https://github.com/modelcontextprotocol/inspector)ï¼Œå¯é€šè¿‡ä»¥ä¸‹å‘½ä»¤å¯åŠ¨ï¼š

```bash
npm run inspector
```

Inspector å°†æä¾›ä¸€ä¸ªå¯åœ¨æµè§ˆå™¨ä¸­è®¿é—®çš„è°ƒè¯•å·¥å…· URLã€‚
",0
https://mcp.so/server/AI-Gateway/Azure-Samples,https://github.com/Azure-Samples/AI-Gateway,ğŸ§ª,"APIM â¤ï¸ AI - This repo contains experiments on Azure API Management's AI capabilities, integrating with Azure OpenAI, AI Foundry, and much more ğŸš€",English,research-and-data,azure; mcp; openai; agents; foundry; autogen,"What is AI-Gateway? AI-Gateway is a project that explores the integration of Azure API Management with AI capabilities, including Azure OpenAI and AI Foundry, through a series of experimental labs. How to use AI-Gateway? To use AI-Gateway, clone the repository, set up the required prerequisites, and navigate through the available labs to experiment with various AI functionalities. Key features of AI-Gateway? Experiments with Model Context Protocol (MCP) for client authorization. Integration with Azure OpenAI models and APIs. Tools for managing AI budgets and costs through the FinOps Framework. Various labs for testing AI functionalities like content filtering, prompt shielding, and model routing. Use cases of AI-Gateway? Experimenting with AI models and APIs in a controlled environment. Managing AI service costs effectively. Implementing advanced AI functionalities in applications. FAQ from AI-Gateway? What prerequisites are needed to use AI-Gateway? You need Python 3.12 or later, VS Code with Jupyter extension, an Azure subscription, and Azure CLI installed. Is AI-Gateway suitable for production use? AI-Gateway is primarily for experimentation and learning; production use should follow best practices from the Azure Well-Architected Framework.","<!-- markdownlint-disable MD033 -->

# ğŸ§ª [AI Gateway](https://learn.microsoft.com/en-us/azure/api-management/genai-gateway-capabilities) labs

[![Open Source Love](https://firstcontributions.github.io/open-source-badges/badges/open-source-v1/open-source.svg)](https://github.com/firstcontributions/open-source-badges)

## What's new âœ¨

â• [**AI Gateway workshop**](https://aka.ms/ai-gateway/workshop) provides a comprehensive learning experience using the Azure Portal  

<div>
  <a href=""https://aka.ms/ai-gateway/workshop"" target=""_blank""><img src=""./images/workshop.png"" alt=""workshop"" width=""300""></a>
</div>
 
â• Refactor most of the labs to use the new [**LLM built-in logging**](https://azure.microsoft.com/en-us/updates?id=491970) that supports streaming completions.  
â• **Realtime API (Audio and Text) with Azure OpenAI ğŸ”¥** experiments with the [**AOAI Realtime**](labs/realtime-audio/realtime-audio.ipynb)  
â• **Realtime API (Audio and Text) with Azure OpenAI + MCP tools ğŸ”¥** experiments with the [**AOAI Realtime + MCP**](labs/realtime-mcp-agents/realtime-mcp-agents.ipynb)  
â• **Model Context Protocol (MCP) âš™ï¸** experiments with the [**client authorization flow**](labs/mcp-client-authorization/mcp-client-authorization.ipynb)  
â• the [**FinOps Framework**](labs/finops-framework/finops-framework.ipynb) lab to manage AI budgets effectively ğŸ’°  
â• **Agentic âœ¨** experiments with [**Model Context Protocol (MCP)**](labs/model-context-protocol/model-context-protocol.ipynb).  
â• **Agentic âœ¨** experiments with [**OpenAI Agents SDK**](labs/openai-agents/openai-agents.ipynb).  
â• **Agentic âœ¨** experiments with [**AI Agent Service**](labs/ai-agent-service/ai-agent-service.ipynb) from [Azure AI Foundry](https://azure.microsoft.com/en-us/products/ai-foundry).  

## Contents

1. [ğŸ§  AI Gateway](#-ai-gateway)
1. [ğŸ§ª Labs with AI Agents](#-labs-with-ai-agents)
1. [ğŸ§ª Labs with the Inference API](#-labs-with-the-inference-api)
1. [ğŸ§ª Labs based on Azure OpenAI](#-labs-based-on-azure-openai)
1. [ğŸš€ Getting started](#-getting-started)
1. [ğŸ”¨ Supporting tools](#-supporting-tools)
1. [ğŸ›ï¸ Well-Architected Framework](#-well-architected-framework)    <!-- markdownlint-disable-line MD051 -->
1. [ğŸ¥‡ Other Resources](#-other-resources)

The rapid pace of AI advances demands experimentation-driven approaches for organizations to remain at the forefront of the industry. With AI steadily becoming a game-changer for an array of sectors, maintaining a fast-paced innovation trajectory is crucial for businesses aiming to leverage its full potential.

**AI services** are predominantly accessed via **APIs**, underscoring the essential need for a robust and efficient API management strategy. This strategy is instrumental for maintaining control and governance over the consumption of **AI models**, **data** and **tools**.

With the expanding horizons of **AI services** and their seamless integration with **APIs**, there is a considerable demand for a comprehensive **AI Gateway** pattern, which broadens the core principles of API management. Aiming to accelerate the experimentation of advanced use cases and pave the road for further innovation in this rapidly evolving field. The well-architected principles of the **AI Gateway** provides a framework for the confident deployment of **Intelligent Apps** into production.

## ğŸ§  AI Gateway

![AI-Gateway flow](images/ai-gateway.gif)

This repo explores the **AI Gateway** pattern through a series of experimental labs. The [AI Gateway capabilities](https://learn.microsoft.com/en-us/azure/api-management/genai-gateway-capabilities) of [Azure API Management](https://learn.microsoft.com/azure/api-management/api-management-key-concepts) plays a crucial role within these labs, handling AI services APIs, with security, reliability, performance, overall operational efficiency and cost controls. The primary focus is on [Azure AI Foundry models](https://learn.microsoft.com/en-us/azure/ai-foundry/what-is-azure-ai-foundry), which sets the standard reference for Large Language Models (LLM). However, the same principles and design patterns could potentially be applied to any third party model.

Acknowledging the rising dominance of Python, particularly in the realm of AI, along with the powerful experimental capabilities of Jupyter notebooks, the following labs are structured around Jupyter notebooks, with step-by-step instructions with Python scripts, [Bicep](https://learn.microsoft.com/azure/azure-resource-manager/bicep/overview?tabs=bicep) files and [Azure API Management policies](https://learn.microsoft.com/azure/api-management/api-management-howto-policies):

## ğŸ§ª Labs with AI Agents

<!-- MCP Client Authorization -->
### [**ğŸ§ª MCP Client Authorization**](labs/mcp-client-authorization/mcp-client-authorization.ipynb)

Playground to experiment the [Model Context Protocol](https://modelcontextprotocol.io/) with the [client authorization flow](https://modelcontextprotocol.io/specification/2025-03-26/basic/authorization#2-10-third-party-authorization-flow). In this flow, Azure API Management act both as an OAuth client connecting to the [Microsoft Entra ID](https://learn.microsoft.com/en-us/entra/architecture/auth-oauth2) authorization server and as an OAuth authorization server for the MCP client ([MCP inspector](https://modelcontextprotocol.io/docs/tools/inspector) in this lab).

[<img src=""images/mcp-client-authorization-small.gif"" alt=""flow"" style=""width: 437px; display: inline-block;"" data-target=""animated-image.originalImage"">](labs/mcp-client-authorization/mcp-client-authorization.ipynb)

[ğŸ¦¾ Bicep](labs/mcp-client-authorization/main.bicep) â• [âš™ï¸ Policy](labs/mcp-client-authorization/src/weather/apim-api/policy.xml) â• [ğŸ§¾ Notebook](labs/mcp-client-authorization/mcp-client-authorization.ipynb)

<!-- Model Context Protocol (MCP) -->
### [**ğŸ§ª Model Context Protocol (MCP)**](labs/model-context-protocol/model-context-protocol.ipynb)

Playground to experiment the [Model Context Protocol](https://modelcontextprotocol.io/) with Azure API Management to enable plug & play of tools to LLMs. Leverages the [credential manager](https://learn.microsoft.com/en-us/azure/api-management/credentials-overview) for  managing OAuth 2.0 tokens to backend tools and [client token validation](https://learn.microsoft.com/en-us/azure/api-management/validate-jwt-policy) to ensure end-to-end authentication and authorization.  

[<img src=""images/model-context-protocol-small.gif"" alt=""flow"" style=""width: 437px; display: inline-block;"" data-target=""animated-image.originalImage"">](labs/model-context-protocol/model-context-protocol.ipynb)

[ğŸ¦¾ Bicep](labs/model-context-protocol/main.bicep) â• [âš™ï¸ Policy](labs/model-context-protocol/inference-policy.xml) â• [ğŸ§¾ Notebook](labs/model-context-protocol/model-context-protocol.ipynb)

<!-- OpenAI Agents -->
### [**ğŸ§ª OpenAI Agents**](labs/openai-agents/openai-agents.ipynb)

Playground to try the [OpenAI Agents](https://openai.github.io/openai-agents-python/) with Azure OpenAI models and API based tools controlled by Azure API Management.

[<img src=""images/openai-agents-small.gif"" alt=""flow"" style=""width: 437px; display: inline-block;"" data-target=""animated-image.originalImage"">](labs/openai-agents/openai-agents.ipynb)

[ğŸ¦¾ Bicep](labs/openai-agents/main.bicep) â• [âš™ï¸ Policy](labs/openai-agents/inference-policy.xml) â• [ğŸ§¾ Notebook](labs/openai-agents/openai-agents.ipynb)

<!-- AI Agent Service -->
### [**ğŸ§ª AI Agent Service**](labs/ai-agent-service/ai-agent-service.ipynb)

Use this playground to explore the [Azure AI Agent Service](https://learn.microsoft.com/en-us/azure/ai-services/agents/overview), leveraging Azure API Management to control multiple services, including Azure OpenAI models, Logic Apps Workflows, and OpenAPI-based APIs.

[<img src=""images/ai-agent-service-small.gif"" alt=""flow"" style=""width: 437px; display: inline-block;"" data-target=""animated-image.originalImage"">](labs/ai-agent-service/ai-agent-service.ipynb)

[ğŸ¦¾ Bicep](labs/ai-agent-service/main.bicep) â• [âš™ï¸ Policy](labs/ai-agent-service/openai-policy.xml) â• [ğŸ§¾ Notebook](labs/ai-agent-service/ai-agent-service.ipynb)

<!-- Function calling -->
### [**ğŸ§ª Function calling**](labs/function-calling/function-calling.ipynb)

Playground to try the OpenAI [function calling](https://learn.microsoft.com/azure/ai-services/openai/how-to/function-calling?tabs=non-streaming%2Cpython) feature with an Azure Functions API that is also managed by Azure API Management.

[<img src=""images/function-calling-small.gif"" alt=""flow"" style=""width: 437px; display: inline-block;"" data-target=""animated-image.originalImage"">](labs/function-calling/function-calling.ipynb)

[ğŸ¦¾ Bicep](labs/function-calling/main.bicep) â• [âš™ï¸ Policy](labs/function-calling/policy.xml) â• [ğŸ§¾ Notebook](labs/function-calling/function-calling.ipynb)

## ğŸ§ª Labs with the Inference API

<!-- AI Foundry Deepseek -->
### [**ğŸ§ª AI Foundry Deepseek**](labs/ai-foundry-deepseek/ai-foundry-deepseek.ipynb)

Playground to try the [Deepseek R1 model](https://azure.microsoft.com/en-us/blog/deepseek-r1-is-now-available-on-azure-ai-foundry-and-github/) via the AI Model Inference from [Azure AI Foundry](https://azure.microsoft.com/en-us/products/ai-foundry). This lab uses the [Azure AI Model Inference API](https://learn.microsoft.com/en-us/azure/ai-foundry/model-inference/how-to/inference?tabs=python) and two APIM LLM policies: [llm-token-limit](https://learn.microsoft.com/en-us/azure/api-management/llm-token-limit-policy) and [llm-emit-token-metric](https://learn.microsoft.com/en-us/azure/api-management/llm-emit-token-metric-policy).

[<img src=""images/ai-foundry-deepseek-small.gif"" alt=""flow"" style=""width: 437px; display: inline-block;"" data-target=""animated-image.originalImage"">](labs/ai-foundry-deepseek/ai-foundry-deepseek.ipynb)

[ğŸ¦¾ Bicep](labs/ai-foundry-deepseek/main.bicep) â• [âš™ï¸ Policy](labs/ai-foundry-deepseek/policy.xml) â• [ğŸ§¾ Notebook](labs/ai-foundry-deepseek/ai-foundry-deepseek.ipynb)

<!-- SLM self-hosting -->
### [**ğŸ§ª SLM self-hosting**](labs/slm-self-hosting/slm-self-hosting.ipynb) (Phi-3)

Playground to try the self-hosted [Phi-3 Small Language Model (SLM)](https://azure.microsoft.com/blog/introducing-phi-3-redefining-whats-possible-with-slms/) through the [Azure API Management self-hosted gateway](https://learn.microsoft.com/azure/api-management/self-hosted-gateway-overview) with OpenAI API compatibility.

[<img src=""images/slm-self-hosting-small.gif"" alt=""flow"" style=""width: 437px; display: inline-block;"" data-target=""animated-image.originalImage"">](labs/slm-self-hosting/slm-self-hosting.ipynb)

[ğŸ¦¾ Bicep](labs/slm-self-hosting/main.bicep) â• [âš™ï¸ Policy](labs/slm-self-hosting/policy.xml) â• [ğŸ§¾ Notebook](labs/slm-self-hosting/slm-self-hosting.ipynb)

## ğŸ§ª Labs based on Azure OpenAI

<!--FinOps framework -->
### [**ğŸ§ª FinOps Framework**](labs/finops-framework/finops-framework.ipynb)

This playground leverages the [FinOps Framework](https://www.finops.org/framework/) and Azure API Management to control AI costs. It uses the [token limit](https://learn.microsoft.com/en-us/azure/api-management/azure-openai-token-limit-policy) policy for each [product](https://learn.microsoft.com/en-us/azure/api-management/api-management-howto-add-products?tabs=azure-portal&pivots=interactive) and integrates [Azure Monitor alerts](https://learn.microsoft.com/en-us/azure/azure-monitor/alerts/alerts-overview) with [Logic Apps](https://learn.microsoft.com/en-us/azure/azure-monitor/alerts/alerts-logic-apps?tabs=send-email) to automatically disable APIM [subscriptions](https://learn.microsoft.com/en-us/azure/api-management/api-management-subscriptions) that exceed cost quotas.  

[<img src=""images/finops-framework-small.gif"" alt=""flow"" style=""width: 437px; display: inline-block;"" data-target=""animated-image.originalImage"">](labs/finops-framework/finops-framework.ipynb)

[ğŸ¦¾ Bicep](labs/finops-framework/main.bicep) â• [âš™ï¸ Policy](labs/finops-framework/openai-policy.xml) â• [ğŸ§¾ Notebook](labs/finops-framework/finops-framework.ipynb)

<!-- Backend pool load balancing -->
### [**ğŸ§ª Backend pool load balancing**](labs/backend-pool-load-balancing/backend-pool-load-balancing.ipynb) - Available with [Bicep](labs/backend-pool-load-balancing/backend-pool-load-balancing.ipynb) and [Terraform](labs/backend-pool-load-balancing-tf/backend-pool-load-balancing-tf.ipynb)

Playground to try the built-in load balancing [backend pool functionality of Azure API Management](https://learn.microsoft.com/azure/api-management/backends?tabs=bicep) to either a list of Azure OpenAI endpoints or mock servers.

[<img src=""images/backend-pool-load-balancing-small.gif"" alt=""flow"" style=""width: 437px; display: inline-block;"" data-target=""animated-image.originalImage"">](labs/backend-pool-load-balancing/backend-pool-load-balancing.ipynb)

[ğŸ¦¾ Bicep](labs/backend-pool-load-balancing/main.bicep) â• [âš™ï¸ Policy](labs/backend-pool-load-balancing/policy.xml) â• [ğŸ§¾ Notebook](labs/backend-pool-load-balancing/backend-pool-load-balancing.ipynb)

<!-- Token rate limiting -->
### [**ğŸ§ª Token rate limiting**](labs/token-rate-limiting/token-rate-limiting.ipynb)

Playground to try the [token rate limiting policy](https://learn.microsoft.com/azure/api-management/azure-openai-token-limit-policy) to one or more Azure OpenAI endpoints. When the token usage is exceeded, the caller receives a 429.

[<img src=""images/token-rate-limiting-small.gif"" alt=""flow"" style=""width: 437px; display: inline-block;"" data-target=""animated-image.originalImage"">](labs/token-rate-limiting/token-rate-limiting.ipynb)

[ğŸ¦¾ Bicep](labs/token-rate-limiting/main.bicep) â• [âš™ï¸ Policy](labs/token-rate-limiting/policy.xml) â• [ğŸ§¾ Notebook](labs/token-rate-limiting/token-rate-limiting.ipynb)

<!-- Token metrics emitting -->
### [**ğŸ§ª Token metrics emitting**](labs/token-metrics-emitting/token-metrics-emitting.ipynb)

Playground to try the [emit token metric policy](https://learn.microsoft.com/azure/api-management/azure-openai-emit-token-metric-policy). The policy sends metrics to Application Insights about consumption of large language model tokens through Azure OpenAI Service APIs.

[<img src=""images/token-metrics-emitting-small.gif"" alt=""flow"" style=""width: 437px; display: inline-block;"" data-target=""animated-image.originalImage"">](labs/token-metrics-emitting/token-metrics-emitting.ipynb)

[ğŸ¦¾ Bicep](labs/token-metrics-emitting/main.bicep) â• [âš™ï¸ Policy](labs/token-metrics-emitting/policy.xml) â• [ğŸ§¾ Notebook](labs/token-metrics-emitting/token-metrics-emitting.ipynb)

<!-- Semantic caching -->
### [**ğŸ§ª Semantic caching**](labs/semantic-caching/semantic-caching.ipynb)

Playground to try the [semantic caching policy](https://learn.microsoft.com/azure/api-management/azure-openai-semantic-cache-lookup-policy). Uses vector proximity of the prompt to previous requests and a specified similarity score threshold.

[<img src=""images/semantic-caching-small.gif"" alt=""flow"" style=""width: 437px; display: inline-block;"" data-target=""animated-image.originalImage"">](labs/semantic-caching/semantic-caching.ipynb)

[ğŸ¦¾ Bicep](labs/semantic-caching/main.bicep) â• [âš™ï¸ Policy](labs/semantic-caching/policy.xml) â• [ğŸ§¾ Notebook](labs/semantic-caching/semantic-caching.ipynb)

<!-- Access controlling -->
### [**ğŸ§ª Access controlling**](labs/access-controlling/access-controlling.ipynb)

Playground to try the [OAuth 2.0 authorization feature](https://learn.microsoft.com/azure/api-management/api-management-authenticate-authorize-azure-openai#oauth-20-authorization-using-identity-provider) using identity provider to enable more fine-grained access to OpenAPI APIs by particular users or client.

[<img src=""images/access-controlling-small.gif"" alt=""flow"" style=""width: 437px; display: inline-block;"" data-target=""animated-image.originalImage"">](labs/access-controlling/access-controlling.ipynb)

[ğŸ¦¾ Bicep](labs/access-controlling/main.bicep) â• [âš™ï¸ Policy](labs/access-controlling/policy.xml) â• [ğŸ§¾ Notebook](labs/access-controlling/access-controlling.ipynb)

<!-- zero-to-production -->
### [**ğŸ§ª Zero-to-Production**](labs/zero-to-production/zero-to-production.ipynb)

Playground to create a combination of several policies in an iterative approach. We start with load balancing, then progressively add token emitting, rate limiting, and, eventually, semantic caching. Each of these sets of policies is derived from other labs in this repo.

[<img src=""images/zero-to-production-small.gif"" alt=""flow"" style=""width: 437px; display: inline-block;"" data-target=""animated-image.originalImage"">](labs/zero-to-production/zero-to-production.ipynb)

[ğŸ¦¾ Bicep](labs/zero-to-production/main.bicep) â• [âš™ï¸ Policy](labs/zero-to-production/policy-3.xml) â• [ğŸ§¾ Notebook](labs/zero-to-production/zero-to-production.ipynb)

<!-- Model Routing -->
### [**ğŸ§ª Model Routing**](labs/model-routing/model-routing.ipynb)

Playground to try routing to a backend based on Azure OpenAI model and version.

[<img src=""images/model-routing.gif"" alt=""flow"" style=""width: 437px; display: inline-block;"" data-target=""animated-image.originalImage"">](labs/model-routing/model-routing.ipynb)

[ğŸ¦¾ Bicep](labs/model-routing/main.bicep) â• [âš™ï¸ Policy](labs/model-routing/policy.xml) â• [ğŸ§¾ Notebook](labs/model-routing/model-routing.ipynb)

<!-- Vector searching -->
### [**ğŸ§ª Vector searching**](labs/vector-searching/vector-searching.ipynb)

Playground to try the [Retrieval Augmented Generation (RAG) pattern](https://learn.microsoft.com/azure/search/retrieval-augmented-generation-overview) with Azure AI Search, Azure OpenAI embeddings and Azure OpenAI completions.

[<img src=""images/vector-searching-small.gif"" alt=""flow"" style=""width: 437px; display: inline-block;"" data-target=""animated-image.originalImage"">](labs/vector-searching/vector-searching.ipynb)

[ğŸ¦¾ Bicep](labs/vector-searching/main.bicep) â• [âš™ï¸ Policy](labs/vector-searching/policy.xml) â• [ğŸ§¾ Notebook](labs/vector-searching/vector-searching.ipynb)

<!-- Built-in logging -->
### [**ğŸ§ª Built-in logging**](labs/built-in-logging/built-in-logging.ipynb)

Playground to try the [buil-in LLM logging capabilities of Azure API Management](https://learn.microsoft.com/azure/api-management/observability). Logs requests into Azure Monitor to track details and token usage.

[<img src=""images/built-in-logging-small.gif"" alt=""flow"" style=""width: 437px; display: inline-block;"" data-target=""animated-image.originalImage"">](labs/built-in-logging/built-in-logging.ipynb)

[ğŸ¦¾ Bicep](labs/built-in-logging/main.bicep) â• [âš™ï¸ Policy](labs/built-in-logging/policy.xml) â• [ğŸ§¾ Notebook](labs/built-in-logging/built-in-logging.ipynb)

<!-- Message storing -->
### [**ğŸ§ª Message storing**](labs/message-storing/message-storing.ipynb)

Playground to test storing message details into Cosmos DB through the LLM Logging to event hub.

[<img src=""images/message-storing-small.gif"" alt=""flow"" style=""width: 437px; display: inline-block;"" data-target=""animated-image.originalImage"">](labs/message-storing/message-storing.ipynb)

[ğŸ¦¾ Bicep](labs/message-storing/main.bicep) â• [âš™ï¸ Policy](labs/message-storing/policy.xml) â• [ğŸ§¾ Notebook](labs/message-storing/message-storing.ipynb)

<!-- Content Filtering -->
### [**ğŸ§ª Content Safety**](labs/content-safety/content-safety.ipynb)

Playground to try the [content safety policy](https://learn.microsoft.com/en-us/azure/api-management/llm-content-safety-policy). The policy enforces content safety checks on any LLM prompts by transmitting them to the [Azure AI Content Safety](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/overview) service before sending to the backend LLM API.

[<img src=""images/content-safety-small.gif"" alt=""flow"" style=""width: 437px; display: inline-block;"" data-target=""animated-image.originalImage"">](labs/content-safety/content-safety.ipynb)

[ğŸ¦¾ Bicep](labs/content-safety/main.bicep) â• [âš™ï¸ Policy](labs/content-safety/content-safety-policy.xml) â• [ğŸ§¾ Notebook](labs/content-safety/content-safety.ipynb)

## Backlog of Labs

This is a list of potential future labs to be developed.

* Logic Apps RAG
* PII handling

## ğŸš€ Getting Started

### Prerequisites

* [Python 3.12 or later version](https://www.python.org/) installed
* [VS Code](https://code.visualstudio.com/) installed with the [Jupyter notebook extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) enabled
* [Python environment](https://code.visualstudio.com/docs/python/environments#_creating-environments) with the [requirements.txt](../../requirements.txt) or run `pip install -r requirements.txt` in your terminal
* [An Azure Subscription](https://azure.microsoft.com/free/) with [Contributor](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/privileged#contributor) + [RBAC Administrator](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/privileged#role-based-access-control-administrator) or [Owner](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/privileged#owner) roles
* [Azure CLI](https://learn.microsoft.com/cli/azure/install-azure-cli) installed and [Signed into your Azure subscription](https://learn.microsoft.com/cli/azure/authenticate-azure-cli-interactively)

### Quickstart

1. Clone this repo and configure your local machine with the prerequisites. Or just create a [GitHub Codespace](https://codespaces.new/Azure-Samples/AI-Gateway/tree/main) and run it on the browser or in VS Code.
2. Navigate through the available labs and select one that best suits your needs. For starters we recommend the [token rate limiting](labs/token-rate-limiting/token-rate-limiting.ipynb).
3. Open the notebook and run the provided steps.
4. Tailor the experiment according to your requirements. If you wish to contribute to our collective work, we would appreciate your [submission of a pull request](CONTRIBUTING.MD).

> [!NOTE]
> ğŸª² Please feel free to open a new [issue](../../issues/new) if you find something that should be fixed or enhanced.

## ğŸ”¨ Supporting Tools

* [Tracing](tools/tracing.ipynb) - Invoke OpenAI API with trace enabled and returns the tracing information.
* [Streaming](tools/streaming.ipynb) - Invoke OpenAI API with stream enabled and returns response in chunks.
* [AI-Gateway Mock server](tools/mock-server/mock-server.ipynb) is designed to mimic the behavior and responses of the OpenAI API, thereby creating an efficient simulation environment suitable for testing and development purposes on the integration with Azure API Management and other use cases. The [app.py](tools/mock-server/app.py) can be customized to tailor the Mock server to specific use cases.

## ğŸ›ï¸ Well-Architected Framework

The [Azure Well-Architected Framework](https://learn.microsoft.com/azure/well-architected/what-is-well-architected-framework) is a design framework that can improve the quality of a workload. The following table maps labs with the Well-Architected Framework pillars to set you up for success through architectural experimentation.

| Lab  | Security | Reliability | Performance | Operations | Costs |
| -------- | -------- |-------- |-------- |-------- |-------- |
| [Access controlling](labs/access-controlling/access-controlling.ipynb) | [â­](#%EF%B8%8F-well-architected-framework ""Zero trust, keyless approach with manage identities and Azure API Management security features"") | |  |  |  |
| [Backend pool load balancing](labs/backend-pool-load-balancing/backend-pool-load-balancing.ipynb)  |[â­](#%EF%B8%8F-well-architected-framework ""Uses backend with manage identities"")|[â­](#%EF%B8%8F-well-architected-framework ""To ensure resilience, the request is distributed to two or more endpoints with the built-in feature"")|[â­](#%EF%B8%8F-well-architected-framework ""Load balances the requests to increase performance with the built-in feature"")|  |  |
| [Semantic caching](labs/semantic-caching/semantic-caching.ipynb)  || |[â­](#%EF%B8%8F-well-architected-framework ""Cache completions to improve performance"")|  | [â­](#%EF%B8%8F-well-architected-framework ""Save tokens by storing completions in cache"") |
| [Token rate limiting](labs/token-rate-limiting//token-rate-limiting.ipynb) ||[â­](#%EF%B8%8F-well-architected-framework ""To ensure resilience, the request is distributed to two or more endpoints with the built-in feature"")| [â­](#%EF%B8%8F-well-architected-framework ""Load balances the requests to increase performance with the built-in feature"")| |  |
| [Built-in LLM logging](labs/built-in-logging/built-in-logging.ipynb) ||||[â­](#%EF%B8%8F-well-architected-framework ""Requests are logged to enable monitoring, alerting and automatic remediation"")||
| [FinOps framework](labs/finops-framework/finops-framework.ipynb) | | | |[â­](#%EF%B8%8F-well-architected-framework ""Operational dashboard and workbooks"") |[â­](#%EF%B8%8F-well-architected-framework ""Cost calculation of the token usage and budget management"")|


> [!TIP]
> Check the [Azure Well-Architected Framework perspective on Azure OpenAI Service](https://learn.microsoft.com/azure/well-architected/service-guides/azure-openai) for aditional guidance.

## ğŸ¥‡ Other resources

* [APIM Samples](http://aka.ms/apim/samples)
* [Landing Zone Accelerator](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/scenarios/app-platform/api-management/landing-zone-accelerator#generative-ai-gateway-scenario)
* [Learning Modules](https://learn.microsoft.com/en-us/training/browse/?products=azure-api-management)
* [News and announcements](https://techcommunity.microsoft.com/tag/API%20Management?nodeId=board%3AIntegrationsonAzureBlog)
* [APIM Releases](https://github.com/Azure/API-Management/releases)

> We believe that there may be valuable content that we are currently unaware of. We would greatly appreciate any suggestions or recommendations to enhance this list.

### Disclaimer

> [!IMPORTANT]
> This software is provided for demonstration purposes only. It is not intended to be relied upon for any purpose. The creators of this software make no representations or warranties of any kind, express or implied, about the completeness, accuracy, reliability, suitability or availability with respect to the software or the information, products, services, or related graphics contained in the software for any purpose. Any reliance you place on such information is therefore strictly at your own risk.
",823
https://mcp.so/server/AI-Infra-Guard/Tencent,https://github.com/Tencent/AI-Infra-Guard,ğŸ›¡ï¸ A.I.Gï¼ˆAI-Infra-Guardï¼‰,"A comprehensive, intelligent, easy-to-use, and lightweight AI Infrastructure Vulnerability Assessment and MCP Server Security Analysis Tool.",English,security,,"What is AI Infra Guard? AI Infra Guard is a comprehensive, intelligent, easy-to-use, and lightweight tool designed for AI Infrastructure Vulnerability Discovery and MCP Server Security Risk Scanning, developed by Tencent Zhuque Lab. How to use AI Infra Guard? To use AI Infra Guard, download the latest version from the Releases page, and run it via command line with subcommands for scanning or using the web interface. Key features of AI Infra Guard? Comprehensive security detection capabilities for MCP and AI components. Supports private deployment and easy integration into security scanning pipelines. User-friendly WebUI for visual operations. Lightweight design with low resource consumption. Use cases of AI Infra Guard? Detecting security vulnerabilities in AI infrastructure components. Scanning MCP Server code for security risks. Integrating into developer platforms for pre-listing security checks. FAQ from AI Infra Guard? Can AI Infra Guard detect all types of vulnerabilities? AI Infra Guard is designed to detect a wide range of vulnerabilities in AI components and MCP servers. Is AI Infra Guard free to use? Yes, AI Infra Guard is open-sourced under the MIT License. How can I contribute to AI Infra Guard? You can report issues or submit code via GitHub.","<p align=""center"">
    <h1 align=""center""><img vertical-align=â€œmiddleâ€ width=""400px"" src=""img/logo-full-new.png"" alt=""A.I.G""/></h1>
</p>
<h4 align=""center"">
    <p>
        <a href=""https://tencent.github.io/AI-Infra-Guard/"">Documentation</a> |
        <a href=""./README_ZH.md"">ä¸­æ–‡</a>
    <p>
</h4>
<p align=""center"">
    <a href=""https://github.com/tencent/AI-Infra-Guard/stargazers"">
      <img src=""https://img.shields.io/github/stars/tencent/AI-Infra-Guard?style=social"" alt=""GitHub stars"">
    </a>
    <a href=""https://github.com/Tencent/AI-Infra-Guard"">
        <img alt=""GitHub downloads"" src=""https://img.shields.io/github/downloads/Tencent/AI-Infra-Guard/total"">
    </a>
    <a href=""https://github.com/Tencent/AI-Infra-Guard"">
        <img alt=""docker pulls"" src=""https://img.shields.io/docker/pulls/zhuquelab/aig-server.svg?color=gold"">
    </a>
    <a href=""https://github.com/Tencent/AI-Infra-Guard"">
        <img alt=""Release"" src=""https://img.shields.io/github/v/release/Tencent/AI-Infra-Guard?color=green"">
    </a>
    <a href=""https://deepwiki.com/Tencent/AI-Infra-Guard"">
       <img src=""https://deepwiki.com/badge.svg"" alt=""Ask DeepWiki"">
    </a>
</p>
<p align=""center"">
  <a href=""https://trendshift.io/repositories/13637"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/13637"" alt=""Tencent%2FAI-Infra-Guard | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>
  <a href=""https://www.blackhat.com/eu-25/arsenal/schedule/index.html#aigai-infra-guard-48381"" target=""_blank""><img src=""img/blackhat.png"" alt=""Tencent%2FAI-Infra-Guard | blackhat"" style=""width: 175px; height: 55px;"" width=""175"" height=""55""/></a>
  <a href=""https://github.com/deepseek-ai/awesome-deepseek-integration"" target=""_blank""><img src=""img/awesome-deepseek.png"" alt=""Tencent%2FAI-Infra-Guard | awesome-deepseek-integration"" style=""width: 273px; height: 55px;"" width=""273"" height=""55""/></a>
</p>

<br>

<p align=""center"">
    <h2 align=""center"">ğŸš€ AI Red Teaming Platform by Tencent Zhuque Lab</h2>
</p>

**A.I.G (AI-Infra-Guard)** integrates capabilities such as AI infra vulnerability scan, MCP Server risk scan, and Jailbreak Evaluation, aiming to provide users with the most comprehensive, intelligent, and user-friendly solution for AI security risk self-examination.

<p>
  We are committed to making A.I.G(AI-Infra-Guard) the industry-leading AI red teaming platform. More stars help this project reach a wider audience, attracting more developers to contribute, which accelerates iteration and improvement. Your star is crucial to us!
</p>
<p align=""center"">
  <a href=""https://github.com/Tencent/AI-Infra-Guard"">
      <img src=""https://img.shields.io/badge/â­-Give%20us%20a%20Star-yellow?style=for-the-badge&logo=github"" alt=""Give us a Star"">
  </a>
</p>

<br>

> ğŸ“¢ **News**: 

> *   ğŸ‰ **[New]** A.I.G was featured at **Black Hat Arsenal Europe 2025**! Check out our [presentation slides.](./Arsenal-BHEU2025-AI-Infra-Guard.pdf)

> * A.I.G v3.5-preview-2 added 100+ AI component CVEs, with support for detecting the latest React2Shell vulnerability (CVE-2025-55182). Improved the onboarding guide for newcomers.[View all changelog.](./CHANGELOG.md)


## Table of Contents
- [ğŸš€ Quick Start](#-quick-start)
- [âœ¨ Features](#-features)
- [ğŸ–¼ï¸ Showcase](#-showcase)
- [ğŸ“– User Guide](#-user-guide)
- [ğŸ”§ API Documentation](#-api-documentation)
- [ğŸ“ Contribution Guide](#-contribution-guide)
- [ğŸ™ Acknowledgements](#-acknowledgements)
- [ğŸ’¬ Join the Community](#-join-the-community)
- [ğŸ“– Citation](#-citation)
- [ğŸ“š Related Papers](#-related-papers)
- [ğŸ“„ License](#-license)
<br><br>
## ğŸš€ Quick Start
### Deployment with Docker

| Docker | RAM | Disk Space |
|:-------|:----|:----------|
| 20.10 or higher | 4GB+ | 10GB+ |

```bash
git clone https://github.com/Tencent/AI-Infra-Guard.git
cd AI-Infra-Guard
# This method pulls pre-built images from Docker Hub for a faster start
docker-compose -f docker-compose.images.yml up -d
```

Once the service is running, you can access the A.I.G web interface at:
`http://localhost:8088`
<br>

<details>
<summary><strong>ğŸ“¦ More installation options and online pro version</strong></summary>

### Other Installation Methods

**Method 2: One-Click Install Script ï¼ˆRecommendedï¼‰**
```bash
# This method will automatically install Docker and launch A.I.G with one command  
curl https://raw.githubusercontent.com/Tencent/AI-Infra-Guard/refs/heads/main/docker.sh | bash
```

**Method 3: Build and run from source**
```bash
git clone https://github.com/Tencent/AI-Infra-Guard.git
cd AI-Infra-Guard
# This method builds a Docker image from local source code and starts the service
docker-compose up -d
```

Note: The AI-Infra-Guard project is positioned as an AI red teaming platform for internal use by enterprises or individuals. It currently lacks an authentication mechanism and should not be deployed on public networks.

For more information, see: [https://tencent.github.io/AI-Infra-Guard/?menu=getting-started](https://tencent.github.io/AI-Infra-Guard/?menu=getting-started)

### Try the Online Pro Version
Experience the Pro version with advanced features and improved performance. The Pro version requires an invitation code and is prioritized for contributors who have submitted issues, pull requests, or discussions, or actively help grow the community. Visit: [https://aigsec.ai/](https://aigsec.ai/)

</details>
<br>

## âœ¨ Features

| Feature | More Info |
|:--------|:------------|
| **AI Infra Scan** | Precisely identifies â€‹overâ€‹ 30 AI framework components â€‹and coversâ€‹ over 400 known CVE vulnerabilities, â€‹includingâ€‹ Ollama, ComfyUI, vLLM, etc. |
| **MCP Server Scan** | Powered by AI Agent, Detects 9 major categories of MCP security risks, Supports source code/remote URL scanning. |
| **Jailbreak Evaluation** | Rapidly assesses Prompt security risks, Includes multiple curated jailbreak evaluation datasets, Cross-model security performance comparison. |

<details>
<summary><strong>ğŸ’ Additional Benefits</strong></summary>

- ğŸ–¥ï¸ **Modern Web Interface**: User-friendly UI with one-click scanning and real-time progress tracking
- ğŸ”Œ **Complete API**: Full interface documentation and Swagger specifications for easy integration
- ğŸŒ **Multi-Language**: Chinese and English interfaces with localized documentation
- ğŸ³ **Cross-Platform**: Linux, macOS, and Windows support with Docker-based deployment
- ğŸ†“ **Free & Open Source**: Completely free under the MIT license
</details>

<br />


## ğŸ–¼ï¸ Showcase

### A.I.G Main Interface
![AIG Main Page](img/aig.gif)

### Plugin Management
![Plugin Management](img/plugin-gif.gif)

<br />


## ğŸ“– User Guide

Visit our online documentation: [https://tencent.github.io/AI-Infra-Guard/](https://tencent.github.io/AI-Infra-Guard/)

For more detailed FAQs and troubleshooting guides, visit our [documentation](https://tencent.github.io/AI-Infra-Guard/?menu=faq).
<br />
<br>

## ğŸ”§ API Documentation

A.I.G provides a comprehensive set of task creation APIs that support AI infra scan, MCP Server Scan, and Jailbreak Evaluation capabilities.

After the project is running, visit `http://localhost:8088/docs/index.html` to view the complete API documentation.

For detailed API usage instructions, parameter descriptions, and complete example code, please refer to the [Complete API Documentation](./api.md).
<br />
<br>

## ğŸ“ Contribution Guide

The extensible plugin frameworkâ€‹â€‹ serves as A.I.G's architectural cornerstone, inviting community innovation through Plugin and Feature contributions.â€‹

### Plugin Contribution Rules
1.  **Fingerprint Rules**: Add new YAML fingerprint files to the `data/fingerprints/` directory.
2.  **Vulnerability Rules**: Add new vulnerability scan rules to the `data/vuln/` directory.
3.  **MCP Plugins**: Add new MCP security scan rules to the `data/mcp/` directory.
4.  **Jailbreak Evaluation Datasets**: Add new Jailbreak evaluation datasets to the `data/eval` directory.

Please refer to the existing rule formats, create new files, and submit them via a Pull Request.

### Other Ways to Contribute
- ğŸ› [Report a Bug](https://github.com/Tencent/AI-Infra-Guard/issues)
- ğŸ’¡ [Suggest a New Feature](https://github.com/Tencent/AI-Infra-Guard/issues)
- â­ [Improve Documentation](https://github.com/Tencent/AI-Infra-Guard/pulls)
<br />
<br />

## ğŸ™ Acknowledgements

### ğŸ“ Academic Collaborations

We extend our sincere appreciation to our academic partners for their exceptional research contributions and technical support.

#### <img src=""img/åŒ—å¤§æœªæ¥ç½‘ç»œé‡ç‚¹å®éªŒå®¤2.png"" height=""30"" align=""middle""/>
<table>
  <tr>
    <td align=""center"" width=""90"">
      <a href=""#"">
        <img src=""https://avatars.githubusercontent.com/u/0?v=4"" width=""70px;"" style=""border-radius: 50%;"" alt=""""/>
      </a>
      <br />
      <a href=""#"">
        <sub><b>Prof.&nbsp;hui&nbsp;Li</b></sub>
      </a>
    </td>
    <td align=""center"" width=""90"">
      <a href=""https://github.com/TheBinKing"">
        <img src=""https://avatars.githubusercontent.com/TheBinKing"" width=""70px;"" style=""border-radius: 50%;"" alt=""""/>
      </a>
      <br />
      <a href=""mailto:1546697086@qq.com"">
        <sub><b>Bin&nbsp;Wang</b></sub>
      </a>
    </td>
    <td align=""center"" width=""90"">
      <a href=""https://github.com/KPGhat"">
        <img src=""https://avatars.githubusercontent.com/KPGhat"" width=""70px;"" style=""border-radius: 50%;"" alt=""""/>
      </a>
      <br />
      <a href=""mailto:kpghat@gmail.com"">
        <sub><b>Zexin&nbsp;Liu</b></sub>
      </a>
    </td>
    <td align=""center"" width=""90"">
      <a href=""https://github.com/GioldDiorld"">
        <img src=""https://avatars.githubusercontent.com/GioldDiorld"" width=""70px;"" style=""border-radius: 50%;"" alt=""""/>
      </a>
      <br />
      <a href=""mailto:g.diorld@gmail.com"">
        <sub><b>Hao&nbsp;Yu</b></sub>
      </a>
    </td>
    <td align=""center"" width=""90"">
      <a href=""https://github.com/Jarvisni"">
        <img src=""https://avatars.githubusercontent.com/Jarvisni"" width=""70px;"" style=""border-radius: 50%;"" alt=""""/>
      </a>
      <br />
      <a href=""mailto:719001405@qq.com"">
        <sub><b>Ao&nbsp;Yang</b></sub>
      </a>
    </td>
    <td align=""center"" width=""90"">
      <a href=""https://github.com/Zhengxi7"">
        <img src=""https://avatars.githubusercontent.com/Zhengxi7"" width=""70px;"" style=""border-radius: 50%;"" alt=""""/>
      </a>
      <br />
      <a href=""mailto:linzhengxi7@126.com"">
        <sub><b>Zhengxi&nbsp;Lin</b></sub>
      </a>
    </td>
  </tr>
</table>

#### <img src=""img/å¤æ—¦å¤§å­¦2.png"" height=""30"" align=""middle"" style=""vertical-align: middle;""/>

<table>
  <tr>
    <td align=""center"" width=""120"">
      <a href=""https://yangzhemin.github.io/"">
        <img src=""https://avatars.githubusercontent.com/yangzhemin"" width=""70px;"" style=""border-radius: 50%;"" alt=""""/>
      </a>
      <br />
      <a href=""mailto:yangzhemin@fudan.edu.cn"">
        <sub><b>Prof.&nbsp;Zhemin&nbsp;Yang</b></sub>
      </a>
    </td>
    <td align=""center"" width=""100"">
      <a href=""https://github.com/kangwei-zhong"">
        <img src=""https://avatars.githubusercontent.com/kangwei-zhong"" width=""70px;"" style=""border-radius: 50%;"" alt=""""/>
      </a>
      <br />
      <a href=""mailto:kwzhong23@m.fudan.edu.cn"">
        <sub><b>Kangwei&nbsp;Zhong</b></sub>
      </a>
    </td>
    <td align=""center"" width=""90"">
      <a href=""https://github.com/MoonBirdLin"">
        <img src=""https://avatars.githubusercontent.com/MoonBirdLin"" width=""70px;"" style=""border-radius: 50%;"" alt=""""/>
      </a>
      <br />
      <a href=""mailto:linjp23@m.fudan.edu.cn"">
        <sub><b>Jiapeng&nbsp;Lin</b></sub>
      </a>
    </td>
    <td align=""center"" width=""90"">
      <a href=""https://vanilla-tiramisu.github.io/"">
        <img src=""https://avatars.githubusercontent.com/vanilla-tiramisu"" width=""70px;"" style=""border-radius: 50%;"" alt=""""/>
      </a>
      <br />
      <a href=""mailto:csheng25@m.fudan.edu.cn"">
        <sub><b>Cheng&nbsp;Sheng</b></sub>
      </a>
    </td>
  </tr>
</table>
<br>

### ğŸ‘¥ Gratitude to Contributing Developers
Thanks to all the developers who have contributed to the A.I.G project, Your contributions have been instrumental in making A.I.G a more robust and reliable AI Red Team platform.
<br />
<table border=""0"" cellspacing=""0"" cellpadding=""0"">
  <tr>
    <td width=""33%""><img src=""img/keen_lab_logo.svg"" alt=""Keen Lab"" height=""85%""></td>
    <td width=""33%""><img src=""img/wechat_security.png"" alt=""WeChat Security"" height=""85%""></td>
    <td width=""33%""><img src=""img/fit_sec_logo.png"" alt=""Fit Security"" height=""85%""></td>
  </tr>
</table>
<a href=""https://github.com/Tencent/AI-Infra-Guard/graphs/contributors"">
  <img src=""https://contrib.rocks/image?repo=Tencent/AI-Infra-Guard"" />
</a>
<br>
<br>

### ğŸ¤ Appreciation for Our Users

We are deeply grateful to the following teams and organizations for their trust, and valuable feedback in using A.I.G.

<br>
<div align=""center"">
<img src=""img/tencent.png"" alt=""Tencent"" height=""30px"">
<img src=""img/deepseek.png"" alt=""DeepSeek"" height=""38px"">
</div>

<br>
<br>

## ğŸ’¬ Join the Community

### ğŸŒ Online Discussions
- **GitHub Discussions**: [Join our community discussions](https://github.com/Tencent/AI-Infra-Guard/discussions)
- **Issues & Bug Reports**: [Report issues or suggest features](https://github.com/Tencent/AI-Infra-Guard/issues)

### ğŸ“± Discussion Community
<table>
  <thead>
  <tr>
    <th>WeChat Group</th>
    <th>Discord <a href=""https://discord.gg/U9dnPnyadZ"">[link]</a></th>
  </tr>
  </thead>
  <tbody>
  <tr>
    <td><img src=""img/wechatgroup.png"" alt=""WeChat Group"" width=""200""></td>
    <td><img src=""img/discord.png"" alt=""discord"" width=""200""></td>
  </tr>
  </tbody>
</table>

### ğŸ“§ Contact Us
For collaboration inquiries or feedback, please contact us at: zhuque@tencent.com




<br>
<br>

## ğŸ“– Citation

If you use A.I.G in your research or product, please cite:

```bibtex
@misc{Tencent_AI-Infra-Guard_2025,
  author={{Tencent Zhuque Lab}},
  title={{AI-Infra-Guard: A Comprehensive, Intelligent, and Easy-to-Use AI Red Teaming Platform}},
  year={2025},
  howpublished={GitHub repository},
  url={https://github.com/Tencent/AI-Infra-Guard}
}
```
<br>

## ğŸ“š Related Papers

We are deeply grateful to the research teams who have used A.I.G in their academic work and contributed to advancing AI security research:

[1] Yongjian Guo, Puzhuo Liu, et al. **""Systematic Analysis of MCP Security.""** arXiv preprint arXiv:2508.12538 (2025). [[pdf]](https://arxiv.org/abs/2508.12538)  
[2] Zexin Wang, Jingjing Li, et al. **""A Survey on AgentOps: Categorization, Challenges, and Future Directions.""** arXiv preprint arXiv:2508.02121 (2025). [[pdf]](https://arxiv.org/abs/2508.02121)  
[3] Yixuan Yang, Daoyuan Wu, Yufan Chen. **""MCPSecBench: A Systematic Security Benchmark and Playground for Testing Model Context Protocols.""** arXiv preprint arXiv:2508.13220 (2025). [[pdf]](https://arxiv.org/abs/2508.13220)  
[4] Ping He, Changjiang Li, et al. **""Automatic Red Teaming LLM-based Agents with Model Context Protocol Tools.""** arXiv preprint arXiv:2509.21011 (2025). [[pdf]](https://arxiv.org/abs/2509.21011)  
[5] Weibo Zhao, Jiahao Liu, Bonan Ruan et al. **""When MCP Servers Attack: Taxonomy, Feasibility, and Mitigation.""** arXiv preprint arXiv:2509.24272v1 (2025). [[pdf]](http://arxiv.org/abs/2509.24272v1)  
[6] Bin Wang, Zexin Liu, Hao Yu et al. **""MCPGuard : Automatically Detecting Vulnerabilities in MCP Servers.""** arXiv preprint arXiv:22510.23673v1 (2025). [[pdf]](http://arxiv.org/abs/2510.23673v1)  
[7] Christian Coleman. **""Behavioral Detection Methods for Automated MCP Server Vulnerability Assessment.""** [[pdf]](https://digitalcommons.odu.edu/cgi/viewcontent.cgi?article=1138&context=covacci-undergraduateresearch)  
[8] Teofil Bodea, Masanori Misono, Julian Pritzi et al. **""Trusted AI Agents in the Cloud.""** arXiv preprint arXiv:2512.05951v1 (2025). [[pdf]](http://arxiv.org/abs/2512.05951v1)  

ğŸ“§ If you have used A.I.G in your research, we would love to hear from you! [Contact us here](#-join-the-community).
<br>
<br>

## ğŸ“„ License

This project is licensed under the **MIT License**. See the [License.txt](./License.txt) file for details.

<div>

[![Star History Chart](https://api.star-history.com/svg?repos=Tencent/AI-Infra-Guard&type=Date)](https://star-history.com/#Tencent/AI-Infra-Guard&Date)
",2
https://mcp.so/server/AI-SolidWorks/sina-salim,https://github.com/sina-salim/AI-SolidWorks,ğŸ› ï¸ AI-SolidWorks,the first local gui mcp server for SolidWorks,English,developer-tools,,"What is AI-SolidWorks? AI-SolidWorks is the first local GUI MCP server for SolidWorks, providing an intelligent interface that allows users to control and create shapes using natural language commands in both Persian and English. How to use AI-SolidWorks? To use AI-SolidWorks, clone the repository, install the required packages, set up your API key, and start sending design commands in natural language. Key features of AI-SolidWorks? Natural Language Communication : Send design commands using simple language. AI-Powered : Utilizes advanced AI models to understand commands. Automatic Script Generation : Generates VBS scripts for SolidWorks. User-Friendly Interface : Simple and intuitive UI with a dark theme. Use cases of AI-SolidWorks? Designing complex shapes using voice commands. Automating repetitive tasks in SolidWorks. Enhancing productivity for engineers and designers. FAQ from AI-SolidWorks? Can AI-SolidWorks understand commands in multiple languages? Yes! It supports both Persian and English commands. Is AI-SolidWorks free to use? Yes! AI-SolidWorks is open-source and free to use. What are the system requirements? You need Python 3.8+, SolidWorks installed, and an API key from OpenAI or OpenRouter.","# ğŸ› ï¸ AI-SolidWorks


<div align=""center"">
  <img src=""https://img.shields.io/badge/Python-3.8+-blue.svg"" alt=""Python 3.8+""/>
  <img src=""https://img.shields.io/badge/SolidWorks-Compatible-red.svg"" alt=""SolidWorks Compatible""/>
  <img src=""https://img.shields.io/badge/License-MIT-green.svg"" alt=""License MIT""/>
  <img src=""https://img.shields.io/badge/AI-Powered-purple.svg"" alt=""AI Powered""/>
</div>

<div dir=""rtl"">

## ğŸ‡®ğŸ‡· Ù†Ø³Ø®Ù‡ ÙØ§Ø±Ø³ÛŒ

**SoliPy** ÛŒÚ© Ø±Ø§Ø¨Ø· Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨ÛŒÙ† Ú©Ø§Ø±Ø¨Ø± Ùˆ Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø± SolidWorks Ø§Ø³Øª Ú©Ù‡ Ø§Ù…Ú©Ø§Ù† Ú©Ù†ØªØ±Ù„ Ùˆ Ø§ÛŒØ¬Ø§Ø¯ Ø§Ø´Ú©Ø§Ù„ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² **Ø¯Ø³ØªÙˆØ±Ø§Øª Ø²Ø¨Ø§Ù† Ø·Ø¨ÛŒØ¹ÛŒ ÙØ§Ø±Ø³ÛŒ** Ø±Ø§ ÙØ±Ø§Ù‡Ù… Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

### âœ¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ

- **ğŸ—£ï¸ Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ø²Ø¨Ø§Ù† Ø·Ø¨ÛŒØ¹ÛŒ**: Ø§Ø±Ø³Ø§Ù„ Ø¯Ø³ØªÙˆØ±Ø§Øª Ø·Ø±Ø§Ø­ÛŒ Ø¨Ø§ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ø³Ø§Ø¯Ù‡
- **ğŸ¤– Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ**: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø±Ú© Ø¯Ø³ØªÙˆØ±Ø§Øª
- **ğŸ“ ØªØ¨Ø¯ÛŒÙ„ Ø®ÙˆØ¯Ú©Ø§Ø± Ø¨Ù‡ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª**: ØªÙˆÙ„ÛŒØ¯ Ø§Ø³Ú©Ø±ÛŒÙ¾Øªâ€ŒÙ‡Ø§ÛŒ VBS Ø¨Ø±Ø§ÛŒ SolidWorks
- **ğŸ¨ Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø¢Ø³Ø§Ù†**: Ù…Ø­ÛŒØ· Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø³Ø§Ø¯Ù‡ Ùˆ Ú©Ø§Ø±Ø¨Ø±Ù¾Ø³Ù†Ø¯ Ø¨Ø§ ØªÙ… ØªØ§Ø±ÛŒÚ©

### ğŸš€ Ù†Ø­ÙˆÙ‡ Ø§Ø³ØªÙØ§Ø¯Ù‡

1. **Ù†ØµØ¨ Ùˆ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ**:
   ```bash
   git clone https://github.com/your-username/solipy.git
   cd solipy
   pip install -r requirements.txt
   python sw_api_panel.py
   ```

2. **ØªÙ†Ø¸ÛŒÙ… API**: 
   - Ú©Ù„ÛŒØ¯ API Ø®ÙˆØ¯ Ø±Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ `.env` ÛŒØ§ Ø§Ø² Ø·Ø±ÛŒÙ‚ Ø¯Ú©Ù…Ù‡ ""ØªÙ†Ø¸ÛŒÙ…Ø§Øª API"" ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯
   - Ø§Ù…Ú©Ø§Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§ÛŒ OpenAI ÛŒØ§ OpenRouter

3. **Ø§Ø±Ø³Ø§Ù„ Ø¯Ø³ØªÙˆØ±Ø§Øª**: 
   - Ù…Ø«Ø§Ù„: ""ÛŒÚ© Ø¯Ø§ÛŒØ±Ù‡ Ø¨Ú©Ø´""
   - Ù…Ø«Ø§Ù„: ""ÛŒÚ© Ù…Ø³ØªØ·ÛŒÙ„ Ø¨Ù‡ Ø¶Ù„Ø¹ Û´Û° Ù…ØªØ± Ø¨Ú©Ø´""
   - Ù…Ø«Ø§Ù„: ""ÛŒÚ© Ø®Ø· Ø¨Ú©Ø´""

### ğŸ“‚ Ø³Ø§Ø®ØªØ§Ø± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§

- `sw_api_panel.py`: Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø§ØµÙ„ÛŒ Ø¨Ø§ Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ Ú¯Ø±Ø§ÙÛŒÚ©ÛŒ
- `scripts/`: Ù¾ÙˆØ´Ù‡ Ø­Ø§ÙˆÛŒ Ø§Ø³Ú©Ø±ÛŒÙ¾Øªâ€ŒÙ‡Ø§ÛŒ VBS Ø¨Ø±Ø§ÛŒ SolidWorks
  - `connect_to_sw.vbs`: Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø§ØªØµØ§Ù„ Ø¨Ù‡ SolidWorks
  - `draw_circle.vbs`: Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø±Ø§ÛŒ Ø±Ø³Ù… Ø¯Ø§ÛŒØ±Ù‡
  - `create_sketch.vbs`: Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø§ÛŒØ¬Ø§Ø¯ Ø§Ø³Ú©Ú† Ø§ØµÙ„ÛŒ
  - `create_sketch_from_input.vbs`: Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ù¾Ø§Ø±Ø§Ù…ØªØ±ÛŒÚ© Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ø§Ø´Ú©Ø§Ù„
  - `create_extrude.vbs`: Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø¨Ø±Ø§ÛŒ Ø§Ú©Ø³ØªØ±ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø§Ø´Ú©Ø§Ù„

### ğŸ“‹ Ù†ÛŒØ§Ø²Ù…Ù†Ø¯ÛŒâ€ŒÙ‡Ø§

- **Python 3.8+**
- **SolidWorks** (Ù†ØµØ¨ Ø´Ø¯Ù‡ Ø±ÙˆÛŒ Ø³ÛŒØ³ØªÙ…)
- **Ú©Ù„ÛŒØ¯ API** Ø§Ø² OpenAI ÛŒØ§ OpenRouter
- **Ø³ÛŒØ³ØªÙ… Ø¹Ø§Ù…Ù„**: ÙˆÛŒÙ†Ø¯ÙˆØ²

</div>

---

## ğŸ‡¬ğŸ‡§ English Version

**SoliPy** is an intelligent interface between users and SolidWorks software that enables control and shape creation using **natural language commands** in both Persian and English.

### âœ¨ Key Features

- **ğŸ—£ï¸ Natural Language Communication**: Send design commands using simple language
- **ğŸ¤– AI-Powered**: Utilizes advanced AI models to understand commands
- **ğŸ“ Automatic Script Generation**: Generates VBS scripts for SolidWorks
- **ğŸ¨ User-Friendly Interface**: Simple and intuitive UI with dark theme

### ğŸš€ How to Use

1. **Installation**:
   ```bash
   git clone https://github.com/your-username/solipy.git
   cd solipy
   pip install -r requirements.txt
   python sw_api_panel.py
   ```

2. **API Setup**:
   - Enter your API key in the `.env` file or through the ""API Settings"" button
   - Compatible with OpenAI or OpenRouter services

3. **Sending Commands**:
   - Example: ""Draw a circle""
   - Example: ""Create a rectangle with 40 meters sides""
   - Example: ""Draw a line""

### ğŸ“‚ File Structure

- `sw_api_panel.py`: Main program with graphical user interface
- `scripts/`: Folder containing VBS scripts for SolidWorks
  - `connect_to_sw.vbs`: Script for connecting to SolidWorks
  - `draw_circle.vbs`: Sample script for drawing a circle
  - `create_sketch.vbs`: Script for creating main sketch
  - `create_sketch_from_input.vbs`: Parametric script for creating shapes
  - `create_extrude.vbs`: Script for extruding shapes

### ğŸ“‹ Requirements

- **Python 3.8+**
- **SolidWorks** (installed on the system)
- **API key** from OpenAI or OpenRouter
- **Operating System**: Windows

## ğŸ“¸ Screenshots

<div align=""center"">
  <p><em>Main application interface with SolidWorks integration</em></p>
  <p><em>Command input and response flow</em></p>
</div>

## ğŸ”„ Workflow

1. **User Input**: Enter a command in natural language
2. **AI Processing**: The command is processed by AI to understand intent
3. **Script Generation**: A VBS script is automatically generated
4. **Execution**: The script is executed in SolidWorks
5. **Result**: The requested operation is performed in SolidWorks

## ğŸ“œ License

This project is licensed under the MIT License - see the LICENSE file for details. ""# SolidWorks-MCP-Server"" 
""# SolidWorks-MCP-Server"" 
",9
https://mcp.so/server/AIDevTools/eliezedeck,https://github.com/eliezedeck/AIDevTools,AIDevTools - Sidekick MCP Server,"A collection of tools designed to enhance AI-powered software development workflows, featuring Sidekick MCP server for process management and notifications.",English,developer-tools,,"What is AIDevTools? AIDevTools is a collection of tools designed to enhance AI-powered software development workflows, featuring the Sidekick MCP server for process management and notifications. How to use AIDevTools? To use AIDevTools, install the Sidekick server using the provided installation script, integrate it with your AI development environment, and start managing processes and notifications seamlessly. Key features of AIDevTools? Production-ready MCP server for AI agent process management Real-time process monitoring and control Smart audio notifications for task completion on macOS Cross-platform support for Linux, macOS, and Windows Advanced control features including ring buffers and graceful shutdowns Use cases of AIDevTools? Managing long-running processes in AI applications. Monitoring development servers and providing real-time feedback. Sending audio notifications for task completions in macOS environments. FAQ from AIDevTools? Is AIDevTools suitable for all AI development environments? Yes! AIDevTools is designed to integrate with various AI systems, particularly those compatible with the Model Context Protocol (MCP). What platforms does AIDevTools support? AIDevTools supports Linux, macOS, and Windows for process management, with additional features for macOS notifications. How can I contribute to AIDevTools? Contributions are welcome! Please refer to the contributing guide in the documentation for details.","# AIDevTools

MCP servers and tools for AI-powered development.

## Components

### ğŸš€ Sidekick
Process management, agent communication, and notification MCP server for AI agents.

**Features:**
- Spawn and manage long-running processes
- Real-time output streaming with ring buffers
- Interactive process control (stdin/stdout/stderr)
- Audio notifications (macOS)
- Agent Q&A system for specialist communication
- TUI mode for visual process monitoring
- Cross-platform: Linux, macOS, Windows

**Install:**
```bash
curl -sSL https://raw.githubusercontent.com/eliezedeck/AIDevTools/main/sidekick/install.sh | bash
```

**Usage:**
```bash
# TUI mode (default)
sidekick

# Stdio mode for Claude Desktop
sidekick --sse=false

# SSE server mode with custom port
sidekick --port 6060

# Add to Claude Desktop (stdio mode)
claude mcp add sidekick ~/.local/bin/sidekick --args ""--sse=false""
```

### ğŸŒ‰ stdio2sse
Proxy between stdio-based MCP clients and SSE-based MCP servers.

**Features:**
- Connect Claude Desktop to SSE-only MCP servers
- Automatic tool discovery and proxying
- Transparent request/response forwarding
- Async architecture for reliable communication

**Install:**
```bash
curl -sSL https://raw.githubusercontent.com/eliezedeck/AIDevTools/main/stdio2sse/install.sh | bash
```

**Usage:**
```bash
# Bridge to an SSE server
stdio2sse --sse-url http://localhost:5050/sse

# Add to Claude Desktop
claude mcp add my-sse-server ~/.local/bin/stdio2sse --args ""--sse-url"" ""http://localhost:5050/sse""
```

## Requirements

- Go 1.23+ (for building from source)
- Claude Desktop or other MCP-compatible clients

## Building from Source

```bash
git clone https://github.com/eliezedeck/AIDevTools.git
cd AIDevTools

# Build sidekick
cd sidekick && go build

# Build stdiobridge
cd ../stdiobridge && go build
```

## API Reference

### Sidekick Tools

**Process Management:**
- `spawn_process` - Start a new process with options (delay, buffer size, environment)
- `spawn_multiple_processes` - Launch multiple processes sequentially
- `get_partial_process_output` - Get incremental output (tail -f functionality)
- `get_full_process_output` - Get all output in memory
- `send_process_input` - Send stdin input to a running process
- `list_processes` - List all tracked processes and their status
- `kill_process` - Terminate a tracked process
- `get_process_status` - Get detailed process information

**Agent Communication:**
- `get_next_question` - Register as a specialist and wait for questions
- `answer_question` - Provide an answer to a received question
- `ask_specialist` - Ask a question to a specialist agent
- `get_answer` - Retrieve answer for a previously asked question
- `list_specialists` - List all available specialist agents

**Notifications (macOS only for now):**
- `notifications_speak` - Play sound and speak text (max 50 words)

## License

MIT License - see [LICENSE](LICENSE) file for details.",1
https://mcp.so/server/AIFoundry-MCPConnector-FabricGraphQL/LazaUK,https://github.com/LazaUK/AIFoundry-MCPConnector-FabricGraphQL,MCP Connector: Integrating AI agent with Data Warehouse in Microsoft Fabric,"MCP Client and Server apps to demo integration of Azure OpenAI-based AI agent with a Data Warehouse, exposed through GraphQL in Microsoft Fabric.",English,research-and-data,graphql; agent; ai; azure; data-warehouse; openai,"What is MCP Connector? MCP Connector is a project that integrates an AI agent powered by Azure OpenAI with a Microsoft Fabric data warehouse, utilizing GraphQL for data access. How to use MCP Connector? To use MCP Connector, set up a Microsoft Fabric data warehouse, configure the local client environment with necessary API keys, and run the MCP client to connect to the GraphQL API endpoint. Key features of MCP Connector? Integration of Azure OpenAI AI agent with Microsoft Fabric data warehouse. Utilization of GraphQL for bidirectional data access. Dynamic discovery of tools and data resources through the Model Context Protocol (MCP). Use cases of MCP Connector? Enabling AI agents to access and manipulate enterprise data. Facilitating data queries and mutations through GraphQL. Demonstrating AI capabilities in data-driven applications. FAQ from MCP Connector? What is the Model Context Protocol (MCP)? MCP is an open integration standard for AI agents that allows for dynamic discovery of tools and data resources. Is there a demo available for MCP Connector? Yes! A practical demo can be found on YouTube. What programming language is used in this project? The project is implemented in Python.","# MCP Connector: Integrating AI agent with Data Warehouse in Microsoft Fabric

This repo demonstrates the integration of an Azure OpenAI-powered AI agent with a Microsoft Fabric data warehouse using the Model Context Protocol (MCP), [open integration standard for AI agents by Anthropic](https://www.anthropic.com/news/model-context-protocol).

MCP enables dynamic discovery of tools, data resources and prompt templates (with more coming soon), unifying their integration with AI agents. GraphQL provides an abstraction layer for universal data connection. Below, you will find detailed steps on how to combine MCP and GraphQL to enable bidirectional access to enterprise data for your AI agent.

> [!NOTE]
> In the MCP server's script, some query parameter values are hard-coded for the sake of this example. In a real-world scenario, these values would be dynamically generated or retrieved.

## Table of contents:
- [Part 1: Configuring Microsoft Fabric Backend](#part-1-configuring-microsoft-fabric-backend)
- [Part 2: Configuring Local Client Environment](#part-2-configuring-local-client-environment)
- [Part 3: User Experience - Gradio UI](#part-3-user-experience---gradio-ui)
- [Part 4: Demo video on YouTube](#part-4-demo-video-on-youtube)

## Part 1: Configuring Microsoft Fabric Backend
1. In Microsoft Fabric, create a new data warehouse pre-populated by sample data by clicking *New item -> Sample warehouse*:
![Step1_SampleWarehouse](images/Step1_SampleWarehouse.png)
2. Next, create a GraphQL API endpoint by clicking *New item -> API for GraphQL*:
![Step2_GraphQlCreate](images/Step2_GraphQLCreate.png)
3. In the data configuration of GraphQL API, choose the *Trip (dbo.Trip)* table:
![Step3_GraphQLData.png](images/Step3_GraphQLData.png)
4. Copy the endpoint URL of your GraphQL API:
![Step4_GraphQLDataURL.png](images/Step4_GraphQLDataURL.png)

## Part 2: Configuring Local Client Environment
1. Install the required Python packages, listed in the provided *requirements.txt*:
```PowerShell
pip install -r requirements.txt
```
2. Configure environment variables for the MCP client:

| Variable                | Description                                      |
| ----------------------- | ------------------------------------------------ |
| `AOAI_API_BASE`         | Base URL of the Azure OpenAI endpoint            |
| `AOAI_API_VERSION`      | API version of the Azure OpenAI endpoint         |
| `AOAI_DEPLOYMENT`       | Deployment name of the Azure OpenAI model        |

3. Set the value of the `AZURE_FABRIC_GRAPHQL_ENDPOINT` variable with the GraphQL endpoint URL from Step 1.4 above. It will be utilised by the MCP Server script to establish connectivity with Microsoft Fabric:

| Variable                           | Description                                |
| -----------------------------------| ------------------------------------------ |
| `AZURE_FABRIC_GRAPHQL_ENDPOINT`    | Microsoft Fabric's GraphQL API endpoint    |

## Part 3: User Experience - Gradio UI
1. Launch the MCP client in your command prompt:
``` PowerShell
python MCP_Client_Gradio.py
```
2. Click the *Initialise System* button to start the MCP server and connect your AI agent to the Microsoft Fabric's GraphQL API endpoint:
![Step5_GradioLaunch.png](images/Step5_GradioLaunch.png)
3. You can now pull and push data to your data warehouse using GraphQL's **queries** and **mutations** enabled by this MCP connector:
![Step5_GradioUse.png](images/Step5_GradioUse.png)

## Part 4: Demo video on YouTube
A practical demo of the provided MCP connector can be found on this [YouTube video](https://youtu.be/R_tPzgEEHMw).
",10
https://mcp.so/server/AIOS-source-code-for-MVP/AndyLeong33dev,https://github.com/AndyLeong33dev/AIOS-source-code-for-MVP,AIOS Monorepo,"This repository contains all source code for AIOS (Langchain server, Website, Desktop application). This will be a good example using MCPs",English,developer-tools,AIOS; Langchain; MVP,"what is AIOS? AIOS (Artificial Intelligence Operating System) is a monorepo project that includes a frontend desktop application and a backend LangGraph server, designed to demonstrate the use of microservices in AI applications. how to use AIOS? To use AIOS, set up the backend LangGraph server and the frontend desktop application in separate terminal sessions, following the provided setup instructions for each component. key features of AIOS? Electron-based desktop application for user interaction. Python-based LangGraph server providing core AI functionalities via an API. Environment configuration for easy setup and deployment. use cases of AIOS? Developing AI-driven applications with a user-friendly interface. Demonstrating microservices architecture in AI projects. Providing a platform for further AI research and development. FAQ from AIOS? What technologies are used in AIOS? AIOS uses Node.js for the frontend and Python for the backend. Is AIOS suitable for production use? AIOS is primarily a demonstration project and may require further development for production readiness. How can I contribute to AIOS? Contributions are welcome! Please refer to the repository for guidelines.","# AIOS Monorepo

This repository contains the AIOS (Artificial Intelligence Operating System) project, split into a frontend desktop application and a backend LangGraph server.

## Project Structure

-   `/aios-desktop-app`: Contains the Electron-based frontend application.
-   `/aios-langgraph-server`: Contains the Python-based LangGraph backend server.

## Prerequisites

Before you begin, ensure you have the following installed:

-   **Node.js** (LTS version recommended) and **npm**: For the desktop application.
-   **Python** (version 3.12.4 recommended) and **pip**: For the backend server.
-   **uv**: A Python package manager.

## Setup and Running

You'll need to set up and run each project in separate terminal sessions.

### 1. AIOS LangGraph Server (Backend)

The backend server provides the core AI functionalities and exposes them via an API, which the desktop application consumes.

**Setup:**

1.  Navigate to the backend directory:
    ```bash
    cd aios-langgraph-server
    ```
2.  Create your environment configuration file:
    Copy `.env.example` to a new file named `.env`.
    ```bash
    cp .env.example .env
    ```
    Then, open `.env` and fill in the necessary environment-specific variables (e.g., API keys, database credentials).
3.  Create and activate a Python virtual environment using `uv`:
    ```bash
    uv venv
    # On macOS/Linux:
    source .venv/bin/activate
    # On Windows:
    .venv\Scripts\activate
    ```
4.  Install the project dependencies using `uv` (this will use `pyproject.toml`):
    ```bash
    uv sync
    ```

**Running the Server:**

1.  Ensure your `uv` virtual environment is activated.
2.  Start the LangGraph server in development mode:
    ```bash
    langgraph dev --no-browser
    ```
    This command starts the server, typically without opening a browser window. Check the terminal output for the server address (usually `http://localhost:2024`).

### 2. AIOS Desktop App (Frontend)

The frontend is an Electron application that provides the user interface for interacting with AIOS.

**Setup:**

1.  Navigate to the frontend directory:
    ```bash
    cd aios-desktop-app
    ```
2.  Create your environment configuration file:
    Copy `.env.example` to a new file named `.env`.
    ```bash
    cp .env.example .env
    ```
    Then, open `.env` and fill in any necessary environment-specific variables for the desktop application.
3.  Install Node.js dependencies:
    ```bash
    npm install
    ```
    The `postinstall` script will automatically run `electron-builder install-app-deps` to ensure all Electron-specific native dependencies are correctly set up.

**Running the App (Development Mode):**

1.  Start the development server:
    ```bash
    npm run dev
    ```
    This will launch the Electron application with hot-reloading enabled, allowing you to see changes live as you develop.

## Development Workflow

1.  Start the `aios-langgraph-server` first.
2.  Then, start the `aios-desktop-app`.
3.  The desktop app should connect to the locally running backend server.

## Building for Production

**aios-desktop-app:**

Refer to the scripts in `aios-desktop-app/package.json` for building the Electron app for different platforms (Windows, macOS, Linux):
```bash
cd aios-desktop-app
npm run build:win
npm run build:mac
npm run build:linux
```

**aios-langgraph-server:**

The LangGraph server is typically deployed as a Python web application. Deployment strategies can vary (e.g., Docker, serverless functions, traditional ASGI server hosting). The `langgraph export` command can be used to package the LangGraph application. Further details on deployment would depend on the chosen hosting environment.

---

This README provides the basic steps to get started. For more detailed information on specific components, refer to the documentation within each project's directory (if available) or consult their respective `package.json` and `pyproject.toml` files.
",1
https://mcp.so/server/AI_Tutor/auston314,https://github.com/auston314/AI_Tutor,AI Tutor,AI powered tutor for higher education based on MCP client/server and multi-agents orchestration.,English,research-and-data,,"what is AI Tutor? AI Tutor is an AI-powered educational assistant designed for higher education, utilizing a client/server model and multi-agent orchestration to provide personalized tutoring. how to use AI Tutor? To use AI Tutor, students can access the platform through its GitHub repository, where they can set up the client/server architecture and interact with the AI agents for tutoring sessions. key features of AI Tutor? Personalized tutoring sessions based on individual learning needs Multi-agent orchestration for diverse educational support Integration with existing educational frameworks and tools use cases of AI Tutor? Assisting students with complex subjects in higher education Providing tailored learning experiences based on student performance Supporting educators in managing and delivering course content FAQ from AI Tutor? Can AI Tutor assist with all subjects in higher education? Yes! AI Tutor is designed to support a wide range of subjects and can adapt to various educational needs. Is AI Tutor open-source? Yes! AI Tutor is available on GitHub under the MIT license, allowing for community contributions and modifications. How does AI Tutor ensure personalized learning? AI Tutor uses advanced algorithms to analyze student performance and tailor tutoring sessions accordingly.","# WeMol Agent
AI powered tutor for higher education based on MCP client/server and multi-agents orchestration. 

* Both Claude models and OpenAI models are supported. 

* MCP servers with both STDIO and SSE connections are supported.
* Plug and Play for adding MCP servers into a configuration JSON file. For example:
```
{
  ""mcpServers"": {
    ""database_sql_server"": {
       ""url"": ""http://localhost:3001/sse""
    },
    ""genchem_qa_server"": {
       ""url"": ""http://localhost:3002/sse""
    },
    ""conformation_server"": {
       ""url"": ""http://localhost:3003/sse""
    }
  }
}
``` 

 
",5
https://mcp.so/server/AIå¯¹å£å‹-é£å½±æ•°å­—äºº/FlyworksAI,,,Home Servers Clients Categories Tags Feed,English,,,,,
https://mcp.so/server/AML-watcher-MCP-Server/Tech-AML,https://github.com/Tech-AML/AML-watcher-MCP-Server,AML Watcher MCP Server,"MCP Server for the AML Watcher API, enabling AML screening for individuals, companies, organizations, crypto wallets, vessels, and aircraft across various categories such as Sanctions, PEP Level 1, PEP Level 2, PEP Level 3, PEP Level 4, SIP, SIE, etc.",English,research-and-data,aml-watcher; aml-screening; api-server,"What is AML Watcher MCP Server? AML Watcher MCP Server is a server designed for the AML Watcher API, facilitating Anti-Money Laundering (AML) screening for various entities including individuals, companies, organizations, and more across multiple categories such as Sanctions and Politically Exposed Persons (PEP). How to use AML Watcher MCP Server? To use the AML Watcher MCP Server, you need to configure it in a Docker container using the claude_desktop_config.json file, where you will specify the necessary environment variables and arguments for the server to function properly. Key features of AML Watcher MCP Server? Supports AML screening for a wide range of entities. Configurable via a JSON file for easy setup. Allows filtering by various categories and parameters. Use cases of AML Watcher MCP Server? Screening individuals and organizations against sanctions lists. Monitoring crypto wallets for suspicious activities. Conducting due diligence for financial institutions. FAQ from AML Watcher MCP Server? How do I generate my API key? You can generate your API key by logging into the AML Watcher Developer Portal and navigating to the API Key section. What categories can I filter by? You can filter by categories such as Sanctions, PEP Level 1 to 4, and more. Is there a limit on the number of results returned? Yes, you can set the maximum number of results to return using the PER_PAGE environment variable.","
## <img src=""https://app.amlwatcher.com/img/single-logo.4cbf1b85.svg"" alt=""AML Watcher Logo"" width=""200""> 


This README provides detailed documentation for the AML Watcher MCP server configuration, focusing on the environment arguments used in the `claude_desktop_config.json` file. It explains each argument, how to set or modify its values, and how to add new arguments if needed.

## Overview

The MCP server is configured to run a Docker container for AML (Anti-Money Laundering) screening. The configuration is defined in the `claude_desktop_config.json` file, which specifies the Docker command, arguments, and environment variables. The environment variables (`env`) control the behavior of the AML screening process, such as search parameters, filtering options, and monitoring settings.

## âš™ï¸ Configuration
### ğŸ“ Sign Up
  
  - If you **already have an account**, Visit the [**AML Watcher Developer Portal**](https://app.amlwatcher.com/login).
  - If you **donâ€™t have an account**, please [**click here to contact us**](https://amlwatcher.com/contact-us/).


## ğŸ”‘ How to Generate Your API Key
- Navigate to the [**AML Watcher Developer Portal**](https://app.amlwatcher.com/login).
- Click on **â€œAPI Keyâ€** and copy it.


## ğŸ–¥ï¸ Usage with Claude Desktop
Add this to your `claude_desktop_config.json`:
### ğŸ³ Docker
```json
{
  ""mcpServers"": {
    ""aml"": {
      ""command"": ""docker"",
      ""args"": [
        ""run"",
        ""-i"",
        ""--rm"",
        ""-e"", ""API_KEY"",
        ""-e"", ""PER_PAGE"",
        ""-e"", ""MATCH_SCORE"",
        ""-e"", ""CATEGORIES"",
        ""-e"", ""ALIAS_SEARCH"",
        ""-e"", ""RCA_SEARCH"",
        ""-e"", ""COUNTRIES"",
        ""techamlw/aml-watcher""
      ],
      ""env"": {
        ""API_KEY"": ""api_key"",
        ""PER_PAGE"": ""1""
      }
    }
  }
}
```
- After this integration, the user has to provide a username.

## ğŸŒ Environment Arguments

Below is a detailed explanation of each environment argument specified in the `env` section of the `claude_desktop_config.json`. Each argument includes its purpose, type, default value, constraints, and instructions for setting or modifying its value.

| Argument       | Type    | Required | Default Value | Description                                                                 |
|----------------|---------|----------|----------------|----------------------------------------------------------------------------|
| `API_KEY`      | String  | Yes      | N/A            | The API key for authentication.                                            |
| `COUNTRIES`    | Array   | No       | N/A            | Array of countries to filter reports. **Note**: ISO 3166-1 alpha-2 country codes are supported. **Example**: `[\""CA\"", \""IN\""]`. <a href=""https://doc.amlwatcher.com/docs/Technical-appendicies/supported-countries/"" target=""_blank"" rel=""noopener noreferrer"">See supported countries</a> |
| `PER_PAGE`     | Integer | No       | 5              | The maximum number of results to return.                                   |
| `MATCH_SCORE`  | Integer | No       | 80             | Match accuracy level (0â€“100).|
| `CATEGORIES`   | Array   | Yes      | N/A            | Filters reports by categories (e.g., `[\""SIP\"", \""PEP Level 1\""]`). <a href=""https://doc.amlwatcher.com/docs/Technical-appendicies/categories/"" target=""_blank"" rel=""noopener noreferrer"" >See available categories</a> |
| `RCA_SEARCH`   | Boolean | No       | True           | Whether to search within Relatives and Close Associates (RCA).             |
| `ALIAS_SEARCH` | Boolean | No       | True           | Whether to search within aliases.                                          |

You can define custom parameters inside the `env` section of your configuration file. These parameters are passed to the Docker container as environment variables.

### ğŸ› ï¸ How to Add Environment Variables
Each variable listed in the `args` array using `-e` must have a matching key in the `env` section.
If a variable is not defined in the `env` block, the system may use a **default value**.

For example, if your `args` list contains:

```json
""args"": [
  ""-e"", ""PER_PAGE"",
  ""-e"", ""MATCH_SCORE""
]
```

Then your env should include:
```json
""env"": {
  ""PER_PAGE"": ""1"",
  ""MATCH_SCORE"": ""80""
}
```

ğŸ’¡Tip : If you want to manually set the value of a variable, make sure to use the exact same name as listed in the args section. Variable names must match exactly, otherwise the Docker container won't receive the value correctly.
",0
https://mcp.so/server/AWS-GeoPlaces-MCP-Server/dxsim,https://github.com/dxsim/AWS-GeoPlaces-MCP-Server,AWS-GeoPlaces-MCP-Server,"MCP server to directly access AWS location services using the GeoPlaces API, provides direct geocoding or reverse-geocoding capabilities like Google Maps API",English,location-services,,"what is AWS-GeoPlaces-MCP-Server? AWS-GeoPlaces-MCP-Server is a project that allows users to directly access AWS location services through the GeoPlaces API, providing geocoding and reverse-geocoding capabilities. how to use AWS-GeoPlaces-MCP-Server? To use the server, follow the setup instructions to install the necessary tools and create a virtual environment. Then, create your MCP using Python and run your server in the MCP Inspector. key features of AWS-GeoPlaces-MCP-Server? Direct access to AWS location services Geocoding and reverse-geocoding capabilities Easy setup with Python project management tools use cases of AWS-GeoPlaces-MCP-Server? Converting addresses into geographic coordinates (geocoding). Finding addresses from geographic coordinates (reverse-geocoding). Integrating location services into applications for enhanced user experience. FAQ from AWS-GeoPlaces-MCP-Server? What is geocoding? Geocoding is the process of converting addresses into geographic coordinates. What is reverse-geocoding? Reverse-geocoding is the process of converting geographic coordinates into a human-readable address. Is there a cost associated with using AWS location services? Yes, AWS location services may incur costs based on usage.","[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/dxsim-aws-geoplaces-mcp-server-badge.png)](https://mseep.ai/app/dxsim-aws-geoplaces-mcp-server)

# AWS-GeoPlaces-MCP-Server
Directly access AWS location services using the GeoPlaces v2 API, provides geocoding or reverse-geocoding capabilities like the Google Maps API. 

[![smithery badge](https://smithery.ai/badge/@dxsim/aws-geoplaces-mcp-server)](https://smithery.ai/server/@dxsim/aws-geoplaces-mcp-server)

## Prerequisites
1. AWS Permissions needed to host MCP for Location Service, Refer to the [example json file](sample_IAM_policy.json) for the minimum viable permissions.

## Development

1. Install [`uv`](https://docs.astral.sh/uv/#__tabbed_1_2) for Python project management:

   MacOS / Linux:

   ```bash
   curl -LsSf https://astral.sh/uv/install.sh | sh
   ```

   Windows:

   ```bash
   powershell -ExecutionPolicy ByPass -c ""irm https://astral.sh/uv/install.ps1 | iex""
   ```

2. Create a virtual environment

   ```bash
   uv venv --python 3.13
   ```

3. Start the virtual environment

   ```bash
   source .venv/bin/activate
   ```

   NOTE: To stop the virtual environment:

   ```bash
   deactivate
   ```

5. Install [MCP Python SDK](https://github.com/modelcontextprotocol/python-sdk) and AWS boto3 client:

   ```bash
   uv add ""mcp[cli]""
   uv add ""boto3""
   uv add ""python-dotenv""
   ```

## Quickstart

1. [Create your MCP using Python](https://modelcontextprotocol.io/introduction)
2. Run your server in the MCP Inspector:
   ```bash
   mcp dev server.py
   ```
3. Install the server in Claude Desktop:
   ```bash
   mcp install <your_server_name.py>
   ```
4. Open `claude_desktop_config.js` in an editor:
   From Claude:

   1. Open Claude
   2. Go to Settings
   3. In the pop-up, select ""Developer""
   4. Click ""Edit Config""

   File location:

   - MacOS / Linux `~/Library/Application/Support/Claude/claude_desktop_config.json`
   - Windows `AppData\Claude\claude_desktop_config.json`

5. Find the full path to `uv`:
   MacOS / Linux:
   ```bash
   which uv
   ```
   Windows:
   ```bash
   where uv
   ```
6. In `claude_desktop_config.js`, set the `command` property to the full `uv` path for your MCP Server
   Example:
   ```json
   ""weather"": {
      ""command"": ""/absolute/path/to/uv"",
      ""args"": [
        ""run"",
        ""--with"",
        ""mcp[cli]"",
        ""mcp"",
        ""run"",
        ""/absolute/path/to/your/server.py""
      ]
    },
   ```
7. Reboot Claude Desktop and use a prompt that will trigger your MCP.
",1
https://mcp.so/server/AWS-kb-MCP/Mohammed2803,https://github.com/Mohammed2803/AWS-kb-MCP,AWS-kb-MCP,AWS Knowledge Base Retrieval MCP Server,English,research-and-data,,"what is AWS-kb-MCP? AWS-kb-MCP is a server designed for retrieving information from an AWS Knowledge Base, facilitating easy access to relevant data and resources. how to use AWS-kb-MCP? To use AWS-kb-MCP, set up the server by following the installation instructions provided in the GitHub repository, and then query the knowledge base using the appropriate API endpoints. key features of AWS-kb-MCP? Efficient retrieval of knowledge base articles from AWS. API support for seamless integration with other applications. User-friendly interface for querying and accessing information. use cases of AWS-kb-MCP? Quickly finding solutions to common AWS-related issues. Assisting developers in accessing AWS documentation and resources. Enhancing customer support by providing instant access to knowledge base articles. FAQ from AWS-kb-MCP? What is the purpose of AWS-kb-MCP? AWS-kb-MCP is designed to streamline the process of retrieving information from the AWS Knowledge Base. Is AWS-kb-MCP free to use? Yes! AWS-kb-MCP is open-source and available for anyone to use. How can I contribute to AWS-kb-MCP? Contributions are welcome! You can submit issues or pull requests on the GitHub repository.","# AWS-kb-MCP
AWS Knowledge Base Retrieval MCP Server
",0
https://mcp.so/server/AWorld/inclusionAI,https://github.com/inclusionAI/AWorld,AWorld: Advancing Agentic AI,"Build, evaluate and run General Multi-Agent Assistance with ease",English,research-and-data,mcp; gym-environment; phone-use; world-model; agent-swarm; agentic-ai,"what is AWorld? AWorld is a framework designed to build, evaluate, and run General Multi-Agent Assistance systems with ease, bridging the gap between theoretical Multi-Agent System (MAS) capabilities and practical implementation in real-world applications. how to use AWorld? To use AWorld, install it via Python, configure your environment with the necessary AI model API keys, and either run pre-defined agents or create your own agents using the provided tutorials and examples. key features of AWorld? Environment multi-tool support including browsers and Android device simulation. AI-powered agents capable of delegation and task execution. Standardized protocols for agent evaluation and model training. Web interface for execution visualization and performance reporting. use cases of AWorld? Automating tasks across web browsers and mobile devices. Evaluating agent capabilities through standardized benchmarks. Training models in a collaborative competition cycle to improve performance. FAQ from AWorld? Can AWorld be used for both computer and mobile tasks? Yes! AWorld supports tasks across various environments including web browsers and Android devices. Is AWorld open-source? Yes! AWorld is available on GitHub and contributions are welcome. What programming language is AWorld built with? AWorld is built using Python.","<div align=""center"">

# AWorld: Rich Environments, Intelligent Agents, Continuous Evolution

</div>

<h4 align=""center"">

*""Self-awareness: the hardest problem isn't solving within limits, it's discovering one's own limitations""*

[![Twitter Follow][twitter-image]][twitter-url]
[![WeChat QR Code][wechat-image]][wechat-url]
[![Discord][discord-image]][discord-url]
[![License: MIT][license-image]][license-url]
[![DeepWiki][deepwiki-image]][deepwiki-url]
[![arXiv][arxiv-image]][arxiv-url]
[![Tutorial][tutorial-image]][tutorial-url]
[![Playground][playground-image]][playground-url]

</h4>

<h4 align=""center"">

[ä¸­æ–‡ç‰ˆ](./README_zh.md) |
[Installation](#installation) |
[Environments](#online-access-to-complex-environments) |
[Agent](#efficient-agent-construction) |
[Experience](#experience-to-samples) |
[Training](#training) |
[Architecture](#architecture-design-principles) |
[Evolution](#evolution) |
[Contributing](#contributing) |

</h4>

---

**AWorld (Agent World)** builds intelligent agents and rich environments where they operate, pushing the frontiers of AI capabilities and enabling continuous evolution. This project provides the fundamental recipe for agentic learning: [Environment Access](#online-access-to-complex-environments), [Agent Construction](#efficient-agent-construction), [Experience Retrieval](#experience-to-samples), and [Model Training](#training). What makes AWorld powerful is that agents can use these same components to automatically improve themselves.

![](./readme_assets/aworld_loop.png)

> ğŸ’¡ Visit our [homepage](https://www.aworldagents.com/) for more details, or try our online [environments](https://www.aworldagents.com/environments) and [agents](https://playground.aworldagents.com/). 


# Installation
> [!TIP]
> Python>=3.11
```bash
git clone https://github.com/inclusionAI/AWorld && cd AWorld

pip install -e .
```

# Online Access to Complex Environments
Provisioning rich environments is hardâ€”packages conflict, APIs need keys, concurrency must scale. We make it painless with three access modes:
1. Use our default hosted setup (tooling with usage costs includes a limited free tier).
2. Bring your own API keys for unrestricted access (coming soon).
3. Pull our Docker images and run everything on your own infrastructure (coming soon).

```python
import os
import asyncio
from aworld.sandbox import Sandbox

INVITATION_CODE = os.environ.get(""INVITATION_CODE"", """")

mcp_config = {
    ""mcpServers"": {
        ""gaia_server"": {
            ""type"": ""streamable-http"",
            ""url"": ""https://playground.aworldagents.com/environments/mcp"",
            ""timeout"": 600,
            ""sse_read_timeout"": 600,
            ""headers"": {
                ""ENV_CODE"": ""gaia"",
                ""Authorization"": f""Bearer {INVITATION_CODE}"",
            }
        }
    }
}

async def _list_tools():
    sand_box = Sandbox(mcp_config=mcp_config, mcp_servers=[""gaia_server""])
    return await sand_box.mcpservers.list_tools()

if __name__ == ""__main__"":
    tools = asyncio.run(_list_tools())
    print(tools)
```

![](./readme_assets/how_to_access_env.gif)

# Efficient Agent Construction
In Aworld, an agent is simply a model enhanced with tools. To spin one up, you only need:
1. a model endpoint (for training, a vLLM service works great)
2. an online environment to call (use our hosted options or plug in your own MCP toolchain)
Thatâ€™s itâ€”no heavyweight scaffolding required.

```python
from aworld.agents.llm_agent import Agent
from aworld.runner import Runners

# refer the section above for details
mcp_config = {...}

searcher = Agent(
    name=""Search Agent"",
    system_prompt=""You specialize at searching."",
    mcp_config=mcp_config
)

if __name__ == ""__main__"":
    result = Runners.sync_run(
        input=""Use google search tool to answer the question: the news about AI today."",
        agent=searcher
    )
    print(f""answer: {result.answer}"")
```

Remember to plug in your LLM credentials first.
```bash
# Set LLM credentials
export LLM_MODEL_NAME=""gpt-4""
export LLM_API_KEY=""your-api-key-here""
export LLM_BASE_URL=""https://api.openai.com/v1""
```

## Complex Agent System Construction

Real-world problems often need more than a single agent. AWorld gives you flexible build paths:
1. design automated workflows end to end  [Docs](https://inclusionai.github.io/AWorld/Quickstart/workflow_construction/)
2. spin up MCP-enabled agents [Docs](https://inclusionai.github.io/AWorld/Quickstart/agent_construction/)
3. orchestrate multi-agent systems (MAS) [Docs](https://inclusionai.github.io/AWorld/Quickstart/multi-agent_system_construction/)

Want to see it live? Load a pre-built DeepResearch team in the AWorld [Playground](https://playground.aworldagents.com/), inspect the source, and run it end to end.
![](./readme_assets/playground_gaiateam.gif)

# Experience to samples
Our runtime captures every step across offline and online runs. Each task yields a complete trajectoryâ€”every LLM call, action, and rewardâ€”so you can synthesize training samples, audit performance, and iterate with confidence.

## Complete Task Trajectories
Tasks unfold over many LLM calls. The framework captures every step, giving you a full trajectory.

```python
import asyncio
from aworld.runner import Runners
from aworld.core.task import Task
from aworld.logs.util import logger
import json

# refer the section above for agent constrution 
searcher = Agent(...)

if __name__ == ""__main__"":
    async def test_complete_trajectory():
        task = Task(
            input=""Use google search tool to answer the question: the news about AI today."",
            agent=searcher
        )

        responses = await Runners.run_task(task)
        resp = responses[task.id]
        logger.info(f""task answer: {resp.answer}"")
        logger.info(f""task trajectory: {json.dumps(resp.trajectory, ensure_ascii=False)}"")
    asyncio.run(test_complete_trajectory())
```

## Single-Step Introspection
Need finer control? Call `step()` to inspect one action/response pair at a time. This lets you inject intermediate rewards during training, enabling richer, more flexible learning signals.

```python
import os
import asyncio
from aworld.runner import Runners
from aworld.core.task import Task
from aworld.logs.util import logger
import json
from aworld.config import TaskConfig, TaskRunMode

# refer the section above for agent constrution 
searcher = Agent(...)

if __name__ == ""__main__"":
    async def test_single_step_introspection():
        task = Task(
            input=""Use google search tool to answer the question: the news about AI today."",
            agent=searcher,
            conf=TaskConfig(
                resp_carry_context=True,
                run_mode=TaskRunMode.INTERACTIVE
            )
        )

        trajectory_log = os.path.join(os.path.dirname(__file__), ""trajectory_log.txt"")
        is_finished = False
        step = 1
        while not is_finished:
            with open(trajectory_log, ""a"", encoding=""utf-8"") as traj_file:
                is_finished, observation, response = await Runners.step(task)
                traj_file.write(f""Step {step}\n"")
                traj_file.write(json.dumps(response.trajectory, ensure_ascii=False, indent=2))
                traj_file.write(""\n\n"")
                step += 1
    asyncio.run(test_single_step_introspection())
```

# Training
Once agents can roam across environments, AWorld closes the loop with two complementary training modes that drive continuous improvement.

## Model Training
Plug any mainstream LLM trainerâ€”AReal, Swift, Verl, Slime, etc.â€”into the runtime to update model parameters directly. Adapters are lightweight, so you can reuse the same environment and agent code across trainers.

```python
from datasets import load_dataset
from aworld.agents.llm_agent import Agent
from aworld.config import AgentConfig

from train.trainer.agent_trainer import AgentTrainer
from train.examples.train_gaia_with_aworld_verl.metrics.gaia_reward_function import gaia_reward_func


# refer the section above for details
mcp_config = {...}

# Configure agent to use Verl as the model service (adapts inference format automatically)
agent_config = AgentConfig(
    llm_provider=""verl""
)
searcher = Agent(
    name=""Search Agent"",
    system_prompt=""You specialize at searching."",
    mcp_config=mcp_config,
    conf=agent_config
)

train_dataset = load_dataset("""", split=""train"")
test_dataset = load_dataset("""", split=""test"")

trainer = AgentTrainer(
    agent=agent,
    config=custom_train_config,
    reward_func=gaia_reward_func,
    train_dataset=train_dataset,
    test_dataset=test_dataset
)

trainer.train()
```
> ğŸ’¡ Check the [real case](./train/examples/train_gaia_with_aworld_verl/main.py) which includes the full training config to run agentic training.

## Meta-Learning
Beyond weights, you can meta-learn whole agent systems. Spin up role-specific agents that critique, rewrite prompts, refine workflow, or adjust strategies for a target agent, then iterate the team (e.g., our Gaia demo).

![](./readme_assets/mas_meta_learning.png)

# Architecture Design Principles
This framework is engineered to be highly adaptable, enabling researchers and developers to explore and innovate across multiple domains, thereby advancing the capabilities and applications of multi-agent systems.

## Concepts & Framework
| Concepts | Description |
| :-------------------------------------- | ------------ |
| [`agent`](./aworld/core/agent/base.py)  | Define the foundational classes, descriptions, output parsing, and multi-agent collaboration (swarm) logic for defining, managing, and orchestrating agents in the AWorld system. |
| [`runner`](./aworld/runners)            | Contains runner classes that manage the execution loop for agents in environments, handling episode rollouts and parallel training/evaluation workflows.   |
| [`task`](./aworld/core/task.py)         | Define the base Task class that encapsulates environment objectives, necessary tools, and termination conditions for agent interactions.  |
| [`swarm`](./aworld/core/agent/swarm.py) | Implement the SwarmAgent class managing multi-agent coordination and emergent group behaviors through decentralized policies. |
| [`sandbox`](./aworld/sandbox)           | Provide a controlled runtime with configurable scenarios for rapid prototyping and validation of agent behaviors. |
| [`tools`](./aworld/tools)               | Offer a flexible framework for defining, adapting, and executing tools for agent-environment interaction in the AWorld system. |
| [`context`](./aworld/core/context)      | Feature a comprehensive context management system for AWorld agents, enabling complete state tracking, configuration management, prompt optimization, multi-task state handling, and dynamic prompt templating throughout the agent lifecycle.  |
| [`memory`](./aworld/memory)             | Implement an extensible memory system for agents, supporting short-term and long-term memory, summarization, retrieval, embeddings, and integration.|
| [`trace`](./aworld/trace)               | Feature an observable tracing framework for AWorld, enabling distributed tracing, context propagation, span management, and integration with popular frameworks and protocols to monitor and analyze agent, tool, and task execution.|


## Characteristics
| Agent Construction            | Topology Orchestration                                                                            | Environment                    |
|:------------------------------|:--------------------------------------------------------------------------------------------------|:-------------------------------|
| âœ… Integrated MCP services     | âœ… Encapsulated runtime                                                                            | âœ… Runtime state management  |
| âœ… Multi-model providers       | âœ… Flexible MAS patterns                                                                           | âœ… High-concurrency support  |
| âœ… Customization options       | âœ… Clear state tracing                                                                             | âœ… Distributed training      |
| âœ… [Support Agent Skills](https://github.com/inclusionAI/AWorld/tree/main/examples/skill_agent)  | [Support Aworld-Cli](https://github.com/inclusionAI/AWorld/tree/main/examples/aworld_cli_demo) ğŸš€ |       |


# Evolution
Our mission: AWorld handles the complexity, you focus on innovation. This section showcases cutting-edge multi-agent systems built with AWorld, advancing toward AGI.

#### Agent Benchmarking

<table style=""width: 100%; border-collapse: collapse; table-layout: fixed;"">
  <thead>
    <tr>
      <th style=""width: 30%; text-align: left; border-bottom: 2px solid #ddd; padding: 8px;"">Category</th>
      <th style=""width: 20%; text-align: left; border-bottom: 2px solid #ddd; padding: 8px;"">Achievement</th>
      <th style=""width: 20%; text-align: left; border-bottom: 2px solid #ddd; padding: 8px;"">Performance</th>
      <th style=""width: 25%; text-align: left; border-bottom: 2px solid #ddd; padding: 8px;"">Key Innovation</th>
      <th style=""width: 5%; text-align: left; border-bottom: 2px solid #ddd; padding: 8px;"">Date</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style=""padding: 8px; vertical-align: top;"">ğŸ¤– Agent
        <br>
        <a href=""https://playground.aworldagents.com/"" target=""_blank"" style=""text-decoration: none;"">
          <img src=""https://img.shields.io/badge/Try-Online-9B59B6?style=flat-square"" alt=""Try Online"">
        </a>
      </td>
      <td style=""padding: 8px; vertical-align: top;"">
        <strong>GAIA Benchmark <br>Excellence</strong>
        <br>
        <a href=""https://huggingface.co/spaces/gaia-benchmark/leaderboard"" target=""_blank"" style=""text-decoration: none;"">
          <img src=""https://img.shields.io/badge/GAIA-Leaderboard-blue"" alt=""GAIA"">
        </a>
      </td>
      <td style=""padding: 8px; vertical-align: top;"">
        Pass@1: <strong>67.89</strong> <br>
        Pass@3: <strong>83.49</strong>
        <br> (109 tasks)
        <a href=""./examples/gaia/README_GUARD.md"" target=""_blank"" style=""text-decoration: none;"">
          <img src=""https://img.shields.io/badge/Code-README-green"" alt=""Code"">
        </a>
      </td>
      <td style=""padding: 8px; vertical-align: top;"">
        Multi-agent system <br>stability & orchestration
        <br>
        <a href=""https://arxiv.org/abs/2508.09889"" target=""_blank"" style=""text-decoration: none;"">
          <img src=""https://img.shields.io/badge/Paper-arXiv-red"" alt=""Paper"">
        </a>
      </td>
      <td style=""padding: 8px; vertical-align: top;"">2025/08/06</td>
    </tr>
    <tr>
      <td style=""padding: 8px; vertical-align: top;"">ğŸ§  Reasoning</td>
      <td style=""padding: 8px; vertical-align: top;"">
        <strong>IMO 2025 <br>Problem Solving</strong>
        <br>
        <a href=""https://www.imo-official.org/year_info.aspx?year=2025"" target=""_blank"" style=""text-decoration: none;"">
          <img src=""https://img.shields.io/badge/IMO-2025-blue"" alt=""IMO"">
        </a>
      </td>
      <td style=""padding: 8px; vertical-align: top;"">
        <strong>5/6</strong> problems <br>solved in 6 hours
        <br>
        <a href=""examples/imo/README.md"" target=""_blank"" style=""text-decoration: none;"">
          <img src=""https://img.shields.io/badge/Code-README-green"" alt=""Code"">
        </a>
      </td>
      <td style=""padding: 8px; vertical-align: top;"">Multi-agent collaboration <br>beats solo models</td>
      <td style=""padding: 8px; vertical-align: top;"">2025/07/25</td>
    </tr>
    <tr>
      <td style=""padding: 8px; vertical-align: top;"">ğŸ–¼ï¸ Multi-Modal</td>
      <td style=""padding: 8px; vertical-align: top;"">
        <strong>OSWorld <br>Rank 1st</strong>
        <br>
        <a href=""https://os-world.github.io/"" target=""_blank"" style=""text-decoration: none;"">
          <img src=""https://img.shields.io/badge/OSWorld-Leaderboard-green"" alt=""OSWorld"">
        </a>
      </td>
      <td style=""padding: 8px; vertical-align: top;"">
        <strong>58.0%</strong> <br> Success Rate
        <br>
        <a href=""examples/osworld/README.md"" target=""_blank"" style=""text-decoration: none;"">
          <img src=""https://img.shields.io/badge/Code-README-green"" alt=""Code"">
        </a>
      </td>
      <td style=""padding: 8px; vertical-align: top;"">The more tools the better?</td>
      <td style=""padding: 8px; vertical-align: top;"">2025/09/18</td>
    </tr>
    <tr>
      <td style=""padding: 8px; vertical-align: top;"">ğŸ–¼ï¸ Multi-Modal</td>
      <td style=""padding: 8px; vertical-align: top;"">
        <strong>VisualWebArena Rank 1st in September</strong>
        <br>
        <a href=""https://docs.google.com/spreadsheets/d/1M801lEpBbKSNwP-vDBkC_pF7LdyGU1f_ufZb_NWNBZQ/edit?gid=2044883967#gid=2044883967"" target=""_blank"" style=""text-decoration: none;"">
          <img src=""https://img.shields.io/badge/VWA-Leaderboard-green"" alt=""VWA"">
        </a>
      </td>
      <td style=""padding: 8px; vertical-align: top;"">
        <strong>36.5%</strong> <br> Success Rate
        <br>
        <a href=""examples/visualwebarena/README.md"" target=""_blank"" style=""text-decoration: none;"">
          <img src=""https://img.shields.io/badge/Code-README-green"" alt=""Code"">
        </a>
      </td>
      <td style=""padding: 8px; vertical-align: top;"">Automated tool generation <br>
        <a href=""https://arxiv.org/pdf/2509.21072"" target=""_blank"" style=""text-decoration: none;"">
          <img src=""https://img.shields.io/badge/Paper-arXiv-red"" alt=""Paper""></td>
      <td style=""padding: 8px; vertical-align: top;"">2025/09/25</td>
    </tr>
    <tr>
      <td style=""padding: 8px; vertical-align: top;"">ğŸ” Deep-Search</td>
      <td style=""padding: 8px; vertical-align: top;"">
        <strong>Xbench Excellence</strong>
        <br>
        <a href=""https://xbench.org/"" target=""_blank"" style=""text-decoration: none;"">
          <img src=""https://img.shields.io/badge/xbench-Leaderboard-green"" alt=""xbench"">
        </a>
      </td>
      <td style=""padding: 8px; vertical-align: top;"">
        Pass@1: 51 <br> Pass@3: 61
        <br>
        <a href=""examples/xbench/README.md"" target=""_blank"" style=""text-decoration: none;"">
          <img src=""https://img.shields.io/badge/Code-README-green"" alt=""Code"">
        </a>
      </td>
      <td style=""padding: 8px; vertical-align: top;"">
          AWorld has its own context engine: Amni.
      </td>
      <td style=""padding: 8px; vertical-align: top;"">2025/10/23</td>
    </tr>
  </tbody>
</table>

#### Data Synthesis

1. **FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling** arxiv, 2025. [paper](https://arxiv.org/abs/2510.24645), [code](https://github.com/inclusionAI/AWorld-RL), [model](https://huggingface.co/Bingguang/FunReason-MT), [dataset](https://huggingface.co/datasets/Bingguang/FunReason-MT)

    *Zengzhuang Xu, Bingguang Hao, Zechuan Wang, Yuntao Wen, Maolin Wang, etc.*


#### Model Training

1. **AWorld: Orchestrating the Training Recipe for Agentic AI.** arxiv, 2025. [paper](https://arxiv.org/abs/2508.20404), [code](https://github.com/inclusionAI/AWorld/tree/main/train), [model](https://huggingface.co/inclusionAI/Qwen3-32B-AWorld)

    *Chengyue Yu, Siyuan Lu, Chenyi Zhuang, Dong Wang, Qintong Wu, etc.*

2. **FunReason: Enhancing Large Language Models' Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement.** arxiv, 2025. [paper](https://arxiv.org/abs/2505.20192), [model](https://huggingface.co/Bingguang/FunReason)

    *Bingguang Hao, Maolin Wang, Zengzhuang Xu, Cunyin Peng, etc.*

3. **Exploring Superior Function Calls via Reinforcement Learning.** arxiv, 2025. [paper](https://arxiv.org/abs/2508.05118), [code](https://github.com/BingguangHao/RLFC)

    *Bingguang Hao, Maolin Wang, Zengzhuang Xu, Yicheng Chen, etc.*

4. **RAG-R1 : Incentivize the Search and Reasoning Capabilities of LLMs through Multi-query Parallelism.** arxiv, 2025. [paper](https://arxiv.org/abs/2507.02962), [code](https://github.com/inclusionAI/AgenticLearning), [model](https://huggingface.co/collections/endertzw/rag-r1-68481d7694b3fca8b809aa29)

    *Zhiwen Tan, Jiaming Huang, Qintong Wu, Hongxuan Zhang, Chenyi Zhuang, Jinjie Gu*

5. **V2P: From Background Suppression to Center Peaking for Robust GUI Grounding Task.** arxiv, 2025. [paper](https://arxiv.org/abs/2508.13634), [code](https://github.com/inclusionAI/AgenticLearning/tree/main/V2P)

    *Jikai Chen, Long Chen, Dong Wang, Leilei Gan, Chenyi Zhuang, Jinjie Gu*

6. **Donâ€™t Just Fine-tune the Agent, Tune the Environment** arxiv, 2025. [paper](https://arxiv.org/abs/2510.10197)

    *Siyuan Lu, Zechuan Wang, Hongxuan Zhang, Qintong Wu, Leilei Gan, Chenyi Zhuang, etc.*


#### Meta Learning

1. **Profile-Aware Maneuvering: A Dynamic Multi-Agent System for Robust GAIA Problem Solving by AWorld.** arxiv, 2025. [paper](https://arxiv.org/abs/2508.09889), [code](https://github.com/inclusionAI/AWorld/blob/main/examples/gaia/README_GUARD.md)

    *Zhitian Xie, Qintong Wu, Chengyue Yu, Chenyi Zhuang, Jinjie Gu*

2. **Recon-Act: A Self-Evolving Multi-Agent Browser-Use System via Web Reconnaissance, Tool Generation, and Task Execution.** arxiv, 2025. [paper](https://arxiv.org/pdf/2509.21072), [code](https://github.com/inclusionAI/AWorld/tree/main/examples/visualwebarena)

    *Kaiwen He, Zhiwei Wang, Chenyi Zhuang, Jinjie Gu*


# Contributing
We warmly welcome developers to join us in building and improving AWorld! Whether you're interested in enhancing the framework, fixing bugs, or adding new features, your contributions are valuable to us.

For academic citations or wish to contact us, please use the following BibTeX entry:

```bibtex
@misc{yu2025aworldorchestratingtrainingrecipe,
      title={AWorld: Orchestrating the Training Recipe for Agentic AI}, 
      author={Chengyue Yu and Siyuan Lu and Chenyi Zhuang and Dong Wang and Qintong Wu and Zongyue Li and Runsheng Gan and Chunfeng Wang and Siqi Hou and Gaochi Huang and Wenlong Yan and Lifeng Hong and Aohui Xue and Yanfeng Wang and Jinjie Gu and David Tsai and Tao Lin},
      year={2025},
      eprint={2508.20404},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2508.20404}, 
}
```

# Star History
![](https://api.star-history.com/svg?repos=inclusionAI/AWorld&type=Date)


<!-- resource section start -->
<!-- image links -->
[arxiv-image]: https://img.shields.io/badge/Paper-arXiv-B31B1B?style=for-the-badge&logo=arxiv&logoColor=white
[blog-image]: https://img.shields.io/badge/Blog-Coming%20Soon-FF5722?style=for-the-badge&logo=blogger&logoColor=white
[deepwiki-image]: https://img.shields.io/badge/DeepWiki-Explore-blueviolet?style=for-the-badge&logo=wikipedia&logoColor=white
[discord-image]: https://img.shields.io/badge/Discord-Join%20us-blue?style=for-the-badge&logo=discord&logoColor=white
[github-code-image]: https://img.shields.io/badge/Code-GitHub-181717?style=for-the-badge&logo=github&logoColor=white
[huggingface-dataset-image]: https://img.shields.io/badge/Dataset-Coming%20Soon-007ACC?style=for-the-badge&logo=dataset&logoColor=white
[huggingface-model-image]: https://img.shields.io/badge/Model-Hugging%20Face-FF6B6B?style=for-the-badge&logo=huggingface&logoColor=white
[license-image]: https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge
[twitter-image]: https://img.shields.io/badge/Twitter-Follow%20us-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white
[wechat-image]: https://img.shields.io/badge/WeChat-Add%20us-green?style=for-the-badge&logo=wechat&logoColor=white
[tutorial-image]: https://img.shields.io/badge/Tutorial-Get%20Started-FF6B35?style=for-the-badge&logo=book&logoColor=white
[playground-image]: https://img.shields.io/badge/Playground-Try%20Online-9B59B6?style=for-the-badge&logo=book&logoColor=white

<!-- aworld links -->
[deepwiki-url]: https://deepwiki.com/inclusionAI/AWorld
[discord-url]: https://discord.gg/b4Asj2ynMw
[license-url]: https://opensource.org/licenses/MIT
[twitter-url]: https://x.com/InclusionAI666
[wechat-url]: https://raw.githubusercontent.com/inclusionAI/AWorld/main/readme_assets/aworld_wechat.png
[arxiv-url]: https://arxiv.org/abs/2508.
[tutorial-url]: https://inclusionai.github.io/AWorld/
[playground-url]: https://playground.aworldagents.com/

<!-- funreason links -->
[funreason-code-url]: https://github.com/BingguangHao/FunReason
[funreason-model-url]: https://huggingface.co/Bingguang/FunReason
[funreason-paper-url]: https://arxiv.org/pdf/2505.20192
<!-- [funreason-dataset-url]: https://github.com/BingguangHao/FunReason -->
<!-- [funreason-blog-url]: https://github.com/BingguangHao/FunReason -->

<!-- deepsearch links -->
[deepsearch-code-url]: https://github.com/inclusionAI/AgenticLearning
[deepsearch-dataset-url]: https://github.com/inclusionAI/AgenticLearning
[deepsearch-model-url]: https://huggingface.co/collections/endertzw/rag-r1-68481d7694b3fca8b809aa29
[deepsearch-paper-url]: https://arxiv.org/abs/2507.02962

<!-- badge -->
[MAS]: https://img.shields.io/badge/Mutli--Agent-System-EEE1CE
[IMO]: https://img.shields.io/badge/IMO-299D8F
[BFCL]: https://img.shields.io/badge/BFCL-8AB07D
[GAIA]: https://img.shields.io/badge/GAIA-E66F51
[Runtime]: https://img.shields.io/badge/AWorld-Runtime-287271
[Leaderboard]: https://img.shields.io/badge/Leaderboard-FFE6B7
[Benchmark]: https://img.shields.io/badge/Benchmark-FFE6B7
[Cloud-Native]: https://img.shields.io/badge/Cloud--Native-B19CD7
[Forward]: https://img.shields.io/badge/Forward-4A90E2
[Backward]: https://img.shields.io/badge/Backward-7B68EE
[Code]: https://img.shields.io/badge/Code-FF6B6B
[Paper]: https://img.shields.io/badge/Paper-4ECDC4


<!-- resource section end -->
",1
https://mcp.so/server/AcademicHub-educhain/mannubaveja007,https://github.com/mannubaveja007/AcademicHub-educhain,ğŸ“ Academic Hub,"Academic Hub A professional network for researchers to publish, collaborate, and monetize their work. Powered by Google's Gemini LLM with MCP context server for intelligent research assistance, and integrated with the OCID Edu Chain ecosystem for secure identity management.",English,research-and-data,,"What is Academic Hub? Academic Hub is a professional network designed for researchers to publish, collaborate, and monetize their work, leveraging Google's Gemini LLM for intelligent research assistance. How to use Academic Hub? To use Academic Hub, sign up for an account, navigate to the Paper Builder, and follow the guided process to create your research paper with AI assistance. You can also connect with other researchers and share your work. Key features of Academic Hub? AI-powered paper creation with step-by-step guidance Real-time plagiarism detection Multiple export formats (PDF, DOCX, LaTeX) Automatic citations and references Community features for connecting with other researchers Use cases of Academic Hub? Creating and publishing research papers with AI assistance Collaborating with peers in the academic community Monetizing research work through visibility and downloads FAQ from Academic Hub? Can I collaborate with other researchers? Yes! Academic Hub allows you to connect and collaborate with researchers in your field. Is there a cost to use Academic Hub? Academic Hub is free to use for all researchers. How does the plagiarism detection work? The platform offers real-time plagiarism checking for AI-generated content and manual checking options for existing papers.","# ğŸ“ Academic Hub

<div align=""center"">

![Academic Hub Logo](https://raw.githubusercontent.com/mannubaveja007/AcademicHub-educhain/main/logo.png)  



[![Next.js](https://img.shields.io/badge/Next.js-13.0+-000000?style=for-the-badge&logo=next.js&logoColor=white)](https://nextjs.org/)
[![TypeScript](https://img.shields.io/badge/TypeScript-4.9+-3178C6?style=for-the-badge&logo=typescript&logoColor=white)](https://www.typescriptlang.org/)
[![Tailwind CSS](https://img.shields.io/badge/Tailwind-3.0+-38B2AC?style=for-the-badge&logo=tailwind-css&logoColor=white)](https://tailwindcss.com/)
[![Google Gemini](https://img.shields.io/badge/Gemini_AI-FF5F1F?style=for-the-badge&logo=google&logoColor=white)](https://ai.google.dev/)

**Academic Hub is a revolutionary platform for researchers to create, share, and collaborate on research papers with AI assistance.**

ğŸ”— **Live Demo**: [https://academic-hub-educhain.vercel.app/](https://academic-hub-educhain.vercel.app)

[Features](#-features) â€¢ 
[Getting Started](#-getting-started) â€¢ 
[Usage](#-usage) â€¢ 
[Technologies](#-technologies) â€¢ 
[Roadmap](#-roadmap) â€¢ 
[Contributing](#-contributing)

</div>

## âœ¨ Features

### ğŸ¤– AI-Powered Paper Builder
- ğŸ“ Step-by-step guided paper creation process
- ğŸ§  AI assistance for each section of your research paper
- ğŸ” Real-time plagiarism detection
- ğŸ“Š Multiple export formats (PDF, DOCX, LaTeX)
- ğŸ“š Automatic citations and references

### ğŸ•µï¸ Plagiarism Detection
- âš¡ Real-time checking of AI-generated content
- ğŸ“Š Detailed similarity reports with source matching
- ğŸ” Highlighted potentially plagiarized text
- ğŸ“ˆ Similarity score and severity classification
- ğŸ”„ Manual checking option for existing content

### ğŸ‘¥ Academic Community
- ğŸŒ Connect with researchers in your field
- ğŸ“„ Share and discover research papers
- ğŸª™ Earn coins when others view, download, or cite your work
- ğŸ“Š Track paper statistics and impact metrics
- ğŸ’¼ Academic career opportunities board

## ğŸš€ Getting Started

### Prerequisites
- Node.js 18.0.0 or higher
- npm or yarn

### Installation

1. Clone the repository:
```bash
git clone https://github.com/yourusername/academic-hub.git
cd academic-hub
```

2. Install dependencies:
```bash
npm install
# or
yarn install
```

3. Set up environment variables:
Create a `.env.local` file in the root directory with the following variables:
```
NEXT_PUBLIC_API_URL=your_api_url
GOOGLE_AI_API_KEY=your_gemini_api_key
```

4. Run the development server:
```bash
npm run dev
# or
yarn dev
```

5. Open [http://localhost:3000](http://localhost:3000) in your browser to see the application.

## ğŸ“– Usage

### ğŸ“ Building a Research Paper

1. Navigate to the Paper Builder from your dashboard
2. Complete each section of your paper in the suggested order
3. Use the AI Assistant to help generate content or improve your writing
4. Check for plagiarism using the built-in detection tool
5. Save your draft regularly
6. Export your completed paper in your preferred format

### ğŸ” Plagiarism Detection

The plagiarism detection system works in two ways:

1. **ğŸ¤– Automatic Detection**: Whenever AI generates content, it's automatically checked for plagiarism
2. **ğŸ‘† Manual Checking**: Use the ""Check Plagiarism"" button to analyze any section of your paper

Understanding the results:
- **ğŸ“Š Similarity Score**: Overall percentage of potentially plagiarized content
- **ğŸš¦ Severity Classification**: 
  - ğŸŸ¢ Very Low (<10%)
  - ğŸŸ¡ Low (10-20%)
  - ğŸŸ  Moderate (20-40%)
  - ğŸ”´ High (40-60%)
  - âš« Very High (>60%)
- **ğŸ“š Matched Sources**: Specific sources that match your content
- **ğŸ–ï¸ Highlighted Text**: Sentences or passages that may be plagiarized

## ğŸ—ï¸ Project Structure

some minor are happening so Structure is dynamic!
```bash
academic-hub/
â”œâ”€â”€ app/                  # Next.js app directory
â”‚   â”œâ”€â”€ api/              # API routes
â”‚   â”‚   â”œâ”€â”€ mcp/          # Model Control Protocol API
â”‚   â”‚   â”œâ”€â”€ gemini/       # Gemini AI API
â”‚   â”‚   â””â”€â”€ papers/       # Papers API
â”œâ”€â”€ app/build-paper/      # Paper builder page
â”œâ”€â”€ app/dashboard/        # User dashboard
â”œâ”€â”€ app/explore/          # Paper exploration page
â”œâ”€â”€ app/...               # Other pages
â”œâ”€â”€ components/           # Reusable React components
â”œâ”€â”€ lib/                  # Utility functions and libraries
â”œâ”€â”€ public/               # Static assets
â””â”€â”€ styles/               # Global styles
```

## ğŸ› ï¸ Technologies

- **ğŸ–¥ï¸ Frontend**: Next.js, React, TypeScript, Tailwind CSS
- **ğŸ§© UI Components**: shadcn/ui
- **ğŸ¤– AI Integration**: Google Gemini AI
- **ğŸ” Authentication**: (Open Campus ( OCID ) Edu Chain
  
## ğŸ—ºï¸ Roadmap

- [ ] ğŸ” Implement user authentication
- [ ] ğŸ’¾ Connect to a real database
- [ ] ğŸ” Integrate with actual plagiarism detection APIs
- [ ] ğŸ‘¥ Add collaborative editing features
- [ ] ğŸ“š Implement citation management system
- [ ] ğŸ“ Add more export formats
- [ ] ğŸ“± Develop mobile application

## ğŸ‘¥ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- Google Gemini AI for powering the AI assistance features
- shadcn/ui for the beautiful UI components
- Next.js team for the amazing framework

---

<div align=""center"">
  
Â© 2023 Academic Hub. All rights reserved.

ğŸ’¡ **Built with AI, for researchers, by researchers** ğŸ’¡

â¤ï¸ **Made with love by Mannu Baveja** â¤ï¸

</div>

",2
https://mcp.so/server/Ad-Veritas_mcp-server-trueRAG/MCP-Mirror,https://github.com/MCP-Mirror/Ad-Veritas_mcp-server-trueRAG,Model Context Protocol (MCP) Server for GraphQL Policies API,Mirror of,English,developer-tools,mcp-server; graphql; api,"what is Model Context Protocol (MCP) Server for GraphQL Policies API? This project provides an implementation of a Model Context Protocol (MCP) server, designed to facilitate a GraphQL API that grants access to various policies. how to use the MCP Server? To use the MCP server, clone the repository from GitHub, install the required dependencies, set up your environment variables, and configure it with your GraphQL API details. key features of the MCP Server? Implementation of Model Context Protocol for easy interaction with policies. Utilizes the Python SDK and GQL library for GraphQL operations. Customizable configurations through environment variables. use cases of the MCP Server? Integrating GraphQL policies within existing applications. Simplifying access to policy management via the MCP architecture. Enhancing communication between different components of software ecosystems. FAQ from the MCP Server? What is the purpose of the MCP? The MCP allows for structured access to policies, ensuring they can be managed and queried efficiently. Is there a specific platform to run the MCP Server? The server is designed to work on various platforms, provided the necessary prerequisites are met. Can I customize the server settings? Yes, you can define server configurations through environment variables as per your application needs.","# Model Context Protocol (MCP) Server for GraphQL Policies API

This repository contains a [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) server implementation for a GraphQL API that provides access to policies.

The server is built using the [python SDK for MCP](https://github.com/modelcontextprotocol/python-sdk) and uses the [GQL](https://github.com/graphql-python/gql) library to interact with the GraphQL API.

## Getting Started

### Clone the repository

```bash
git clone https://github.com/Ad-Veritas/mcp-server-trueRAG.git
cd mcp-server-trueRAG
```

### Make sure you have [uv](https://github.com/astral-sh/uv) installed

```bash
uv --version
```

If not, you can install it using:

```bash
# On macOS and Linux.
curl -LsSf https://astral.sh/uv/install.sh | sh

# On Windows.
powershell -ExecutionPolicy ByPass -c ""irm https://astral.sh/uv/install.ps1 | iex""
```

### Define the environment variables

The server is configured to work against a GraphQL API for one of the TrueRag systems. Once you created the TrueRAG environment, copy the API key and endpoint from the environment variables.

Create a `.env` file in the root directory of the repository and add the following lines:

```txt
GRAPHQL_API_KEY = ""{your_api_key}""
GRAPHQL_ENDPOINT = ""{your_graphql_endpoint}""
```

### Add to the MCP Client, such as Claude Desktop

Add the following lines to the Claude configuration file (`~/Library/Application Support/Claude/claude_desktop_config.json`):

```json
    ""shipping-policies"": {
      ""command"": ""uv"",
      ""args"": [
        ""--directory"",
        ""{path_to_mcp_server}/mcp-server-trueRAG"",
        ""run"",
        ""fastmcp"",
        ""run"",
        ""server.py""
      ]
    }
```
",0
https://mcp.so/server/AdamPippert_multi-service-mcp-server/MCP-Mirror,https://github.com/MCP-Mirror/AdamPippert_multi-service-mcp-server,Model Context Protocol (MCP) Server,Mirror of,English,developer-tools,mcp; multi-service; server,"what is Model Context Protocol (MCP) Server? The Model Context Protocol (MCP) Server is a modular server that implements the Model Context Protocol standard, providing tools for GitHub, GitLab, Google Maps, Memory storage, and Puppeteer web automation. how to use the MCP Server? To use the MCP Server, clone the repository, install the necessary dependencies, configure the environment variables, and start the server. You can also deploy it using Docker or Podman. key features of the MCP Server? Unified MCP Gateway for all tool requests MCP Manifest for describing available tools Direct access to each tool via its own API endpoints Modular design for easy tool management use cases of the MCP Server? Automating interactions with GitHub and GitLab repositories. Performing geocoding and directions using Google Maps. Storing and retrieving data with the Memory tool. Web automation tasks like taking screenshots and generating PDFs with Puppeteer. FAQ from MCP Server? What programming languages are required to run the MCP Server? Python 3.8 or higher and Node.js 14 or higher are required. Can I deploy the MCP Server using Docker? Yes! The MCP Server can be easily deployed using Docker or Podman. Is the MCP Server open-source? Yes! The MCP Server is open-source and available on GitHub.","# Model Context Protocol (MCP) Server

A modular server that implements the [Model Context Protocol](https://modelcontextprotocol.io/) standard, providing tools for GitHub, GitLab, Google Maps, Memory storage, and Puppeteer web automation.

## Architecture

The MCP server is built with a modular architecture, where each tool is implemented as a separate module. The server provides a unified gateway that routes requests to the appropriate tool.

![MCP Server Architecture](./architecture.png)

## Features

- **MCP Gateway**: A unified endpoint for all tool requests following the MCP standard
- **MCP Manifest**: An endpoint that describes all available tools and their capabilities
- **Direct Tool Access**: Each tool can be accessed directly via its own API endpoints
- **Modular Design**: Easy to add or remove tools as needed

### Included Tools

1. **GitHub Tool**: Interact with GitHub repositories, issues, and search
2. **GitLab Tool**: Interact with GitLab projects, issues, and pipelines
3. **Google Maps Tool**: Geocoding, directions, and places search
4. **Memory Tool**: Store and retrieve data persistently
5. **Puppeteer Tool**: Take screenshots, generate PDFs, and extract content from websites

## Getting Started

### Prerequisites

- Python 3.8 or higher
- Node.js 14 or higher
- A Red Hat-based Linux distribution (RHEL, CentOS, Fedora) or any Linux/macOS system

### Installation

1. Clone this repository:
   ```bash
   git clone https://github.com/yourusername/mcp-server.git
   cd mcp-server
   ```

2. Install Python dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Install Node.js dependencies:
   ```bash
   npm install
   ```

4. Create a `.env` file with your configuration:
   ```
   SECRET_KEY=your-secret-key
   DEBUG=False
   
   # GitHub configuration
   GITHUB_TOKEN=your-github-token
   
   # GitLab configuration
   GITLAB_TOKEN=your-gitlab-token
   
   # Google Maps configuration
   GMAPS_API_KEY=your-google-maps-api-key
   
   # Memory configuration
   MEMORY_DB_URI=sqlite:///memory.db
   
   # Puppeteer configuration
   PUPPETEER_HEADLESS=true
   CHROME_PATH=/usr/bin/chromium-browser
   ```

5. Start the server:
   ```bash
   python app.py
   ```

### Containerized Deployment

You can run the server using either Docker or Podman (Red Hat's container engine).

#### Docker Deployment

If you already have Docker and docker-compose installed:

1. Build the Docker image:
   ```bash
   docker build -t mcp-server .
   ```

2. Run the container:
   ```bash
   docker run -p 5000:5000 --env-file .env mcp-server
   ```

3. Alternatively, use docker-compose:
   
   Create a `docker-compose.yml` file:
   ```yaml
   version: '3'
   services:
     mcp-server:
       build: .
       ports:
         - ""5000:5000""
       volumes:
         - ./data:/app/data
       env_file:
         - .env
       restart: unless-stopped
   ```

   Then run:
   ```bash
   docker-compose up -d
   ```

#### Podman Deployment

For Red Hat based systems (RHEL, CentOS, Fedora) using Podman:

1. Build the container image:
   ```bash
   podman build -t mcp-server .
   ```

2. Run the container:
   ```bash
   podman run -p 5000:5000 --env-file .env mcp-server
   ```

3. If you need persistent storage:
   ```bash
   mkdir -p ./data
   podman run -p 5000:5000 --env-file .env -v ./data:/app/data:Z mcp-server
   ```
   Note: The `:Z` suffix is important for SELinux-enabled systems.

4. Using Podman Compose (if installed):
   ```bash
   # Install podman-compose if needed
   pip install podman-compose
   
   # Use the same docker-compose.yml file as above
   podman-compose up -d
   ```

## Using the MCP Server

### MCP Gateway

The MCP Gateway is the main endpoint for accessing all tools using the MCP standard.

**Endpoint**: `POST /mcp/gateway`

**Request format**:
```json
{
  ""tool"": ""github"",
  ""action"": ""listRepos"",
  ""parameters"": {
    ""username"": ""octocat""
  }
}
```

**Response format**:
```json
{
  ""tool"": ""github"",
  ""action"": ""listRepos"",
  ""status"": ""success"",
  ""result"": [
    {
      ""id"": 1296269,
      ""name"": ""Hello-World"",
      ""full_name"": ""octocat/Hello-World"",
      ""owner"": {
        ""login"": ""octocat"",
        ""id"": 1
      },
      ...
    }
  ]
}
```

### MCP Manifest

The MCP Manifest describes all available tools and their capabilities.

**Endpoint**: `GET /mcp/manifest`

**Response format**:
```json
{
  ""manifestVersion"": ""1.0"",
  ""tools"": {
    ""github"": {
      ""actions"": {
        ""listRepos"": {
          ""description"": ""List repositories for a user or organization"",
          ""parameters"": {
            ""username"": {
              ""type"": ""string"",
              ""description"": ""GitHub username or organization name""
            }
          },
          ""returns"": {
            ""type"": ""array"",
            ""description"": ""List of repository objects""
          }
        },
        ...
      }
    },
    ...
  }
}
```

### Direct Tool Access

Each tool can also be accessed directly via its own API endpoints:

- GitHub: `/tool/github/...`
- GitLab: `/tool/gitlab/...`
- Google Maps: `/tool/gmaps/...`
- Memory: `/tool/memory/...`
- Puppeteer: `/tool/puppeteer/...`

See the API documentation for each tool for details on the available endpoints.

## Tool Documentation

### GitHub Tool

The GitHub tool provides access to the GitHub API for repositories, issues, and search.

**Actions**:
- `listRepos`: List repositories for a user or organization
- `getRepo`: Get details for a specific repository
- `searchRepos`: Search for repositories
- `getIssues`: Get issues for a repository
- `createIssue`: Create a new issue in a repository

### GitLab Tool

The GitLab tool provides access to the GitLab API for projects, issues, and pipelines.

**Actions**:
- `listProjects`: List all projects accessible by the authenticated user
- `getProject`: Get details for a specific project
- `searchProjects`: Search for projects on GitLab
- `getIssues`: Get issues for a project
- `createIssue`: Create a new issue in a project
- `getPipelines`: Get pipelines for a project

### Google Maps Tool

The Google Maps tool provides access to the Google Maps API for geocoding, directions, and places search.

**Actions**:
- `geocode`: Convert an address to geographic coordinates
- `reverseGeocode`: Convert geographic coordinates to an address
- `getDirections`: Get directions between two locations
- `searchPlaces`: Search for places using the Google Places API
- `getPlaceDetails`: Get details for a specific place

### Memory Tool

The Memory tool provides a persistent key-value store for storing and retrieving data.

**Actions**:
- `get`: Get a memory item by key
- `set`: Create or update a memory item
- `delete`: Delete a memory item by key
- `list`: List all memory items, with optional filtering
- `search`: Search memory items by value

### Puppeteer Tool

The Puppeteer tool provides web automation capabilities for taking screenshots, generating PDFs, and extracting content from websites.

**Actions**:
- `screenshot`: Take a screenshot of a webpage
- `pdf`: Generate a PDF of a webpage
- `extract`: Extract content from a webpage

## Contributing

Contributions are welcome! Here's how you can extend the MCP server:

### Adding a New Tool

1. Create a new file in the `tools` directory, e.g., `tools/newtool_tool.py`
2. Implement the tool with actions following the same pattern as existing tools
3. Add the tool to the manifest in `app.py`
4. Register the tool's blueprint in `tools/__init__.py`

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgements

- [Model Context Protocol](https://modelcontextprotocol.io/) for the standard specification
- [Flask](https://flask.palletsprojects.com/) for the web framework
- [Puppeteer](https://pptr.dev/) for web automation
",0
https://mcp.so/server/Adaptive-Graph-of-Thoughts-MCP-server/SaptaDey,https://github.com/SaptaDey/Adaptive-Graph-of-Thoughts-MCP-server,ğŸ§  Adaptive Graph of Thoughts,AI Reasoning Framework for Scientific Research,English,research-and-data,graph-algorithms; mcp; neo4j-graph; graph-of-thoughts; mcp-server,"What is Adaptive Graph of Thoughts? Adaptive Graph of Thoughts is an AI reasoning framework designed for scientific research, utilizing graph structures to enhance the way AI systems approach complex reasoning tasks. How to use Adaptive Graph of Thoughts? To use the framework, set up a Neo4j graph database with the required APOC library, configure the application settings, and run the server either locally or via Docker. You can interact with the API to perform scientific queries. Key features of Adaptive Graph of Thoughts? Utilizes Neo4j for advanced graph-based reasoning. Implements the Model Context Protocol (MCP) for integration with AI applications. Supports dynamic confidence scoring and multi-dimensional evaluations. Dockerized for easy deployment and modular design for customization. Use cases of Adaptive Graph of Thoughts? Analyzing complex scientific relationships and queries. Integrating with AI clients for enhanced reasoning capabilities. Supporting research tasks that require sophisticated data processing and analysis. FAQ from Adaptive Graph of Thoughts? What is the required environment for running the project? You need a running Neo4j instance with the APOC library installed, along with Python 3.11+ and Docker for deployment. Can I contribute to the project? Yes! Contributions are welcome, and guidelines are available in the documentation. Is there a roadmap for future features? Yes, the project has a roadmap that includes plans for enhanced graph visualization and integration with more data sources.","[![MseeP.ai Security Assessment Badge](https://mseep.net/pr/saptadey-adaptive-graph-of-thoughts-mcp-server-badge.png)](https://mseep.ai/app/saptadey-adaptive-graph-of-thoughts-mcp-server)

# ğŸ§  Adaptive Graph of Thoughts

<div align=""center"">

```
                    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
                    â•‘                                      â•‘
                    â•‘   ğŸ§  Adaptive Graph of Thoughts ğŸ§   â•‘
                    â•‘                                      â•‘
                    â•‘       Intelligent Scientific         â•‘
                    â•‘         Reasoning through            â•‘
                    â•‘         Graph-of-Thoughts            â•‘
                    â•‘                                      â•‘
                    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

#### **Intelligent Scientific Reasoning through Graph-of-Thoughts**

[![Version](https://img.shields.io/badge/version-0.1.0-blue.svg)](https://saptadey.github.io/Adaptive-Graph-of-Thoughts-MCP-server/)
[![Python](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-Apache_2.0-green.svg)](LICENSE)
[![Docker](https://img.shields.io/badge/docker-ready-brightgreen.svg)](Dockerfile)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.111.1-009688.svg)](https://fastapi.tiangolo.com)
[![NetworkX](https://img.shields.io/badge/NetworkX-3.3-orange.svg)](https://networkx.org)
[![Last Updated](https://img.shields.io/badge/last_updated-Jun_2025-lightgrey.svg)](CHANGELOG.md)
[![smithery badge](https://smithery.ai/badge/@SaptaDey/graph-of-thought-mcp)](https://smithery.ai/server/@SaptaDey/graph-of-thought-mcp)
[![Codacy Security Scan](https://github.com/SaptaDey/Adaptive-Graph-of-Thoughts-MCP-server/actions/workflows/codacy.yml/badge.svg)](https://github.com/SaptaDey/Adaptive-Graph-of-Thoughts-MCP-server/actions/workflows/codacy.yml)
[![CodeQL Advanced](https://github.com/SaptaDey/Adaptive-Graph-of-Thoughts-MCP-server/actions/workflows/codeql.yml/badge.svg)](https://github.com/SaptaDey/Adaptive-Graph-of-Thoughts-MCP-server/actions/workflows/codeql.yml)
[![Dependabot Updates](https://github.com/SaptaDey/Adaptive-Graph-of-Thoughts-MCP-server/actions/workflows/dependabot/dependabot-updates/badge.svg)](https://github.com/SaptaDey/Adaptive-Graph-of-Thoughts-MCP-server/actions/workflows/dependabot/dependabot-updates)
[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/b56538c9-7a30-45b3-851c-447fe2eb24a6)


</div>

<div align=""center"">
  <p><strong>ğŸš€ Next-Generation AI Reasoning Framework for Scientific Research</strong></p>
  <p><em>Leveraging graph structures to transform how AI systems approach scientific reasoning</em></p>
</div>

## ğŸ“š Documentation

**For comprehensive information on Adaptive Graph of Thoughts, including detailed installation instructions, usage guides, configuration options, API references, contribution guidelines, and the project roadmap, please visit our full documentation site:**

**[â¡ï¸ Adaptive Graph of Thoughts Documentation Site](https://adaptive-thought-web-weaver.lovable.app)**

The site now includes interactive Mermaid diagrams and an improved layout. 

## ğŸ” Overview

Adaptive Graph of Thoughts leverages a **Neo4j graph database** to perform sophisticated scientific reasoning, with graph operations managed within its pipeline stages. It implements the **Model Context Protocol (MCP)** to integrate with AI applications like Claude Desktop, providing an Advanced Scientific Reasoning Graph-of-Thoughts (ASR-GoT) framework designed for complex research tasks.

**Key highlights:**
- Process complex scientific queries using graph-based reasoning
- Dynamic confidence scoring with multi-dimensional evaluations 
- **Connects to external databases (PubMed, Google Scholar, Exa Search) for real-time evidence gathering**
- Built with modern Python and FastAPI for high performance
- Dockerized for easy deployment
- Modular design for extensibility and customization
- Integration with Claude Desktop via MCP protocol

## ğŸš€ Quick Start

```bash
git clone https://github.com/SaptaDey/Adaptive-Graph-of-Thoughts-MCP-server.git
cd Adaptive-Graph-of-Thoughts-MCP-server
poetry install
poetry run uvicorn src.adaptive_graph_of_thoughts.main:app --reload
```

Open [http://localhost:8000/setup](http://localhost:8000/setup) and complete the
wizard. You'll land on the dashboard when finished.

## ğŸ“‚ Project Structure

The project is organized as follows (see the documentation site for more details):
```
Adaptive Graph of Thoughts/
â”œâ”€â”€ ğŸ“ .github/                           # GitHub specific files (workflows)
â”œâ”€â”€ ğŸ“ config/                            # Configuration files (settings.yaml)
â”œâ”€â”€ ğŸ“ docs_src/                          # Source files for MkDocs documentation
â”œâ”€â”€ ğŸ“ src/                               # Source code
â”‚   â””â”€â”€ ğŸ“ adaptive_graph_of_thoughts     # Main application package
â”œâ”€â”€ ğŸ“ tests/                             # Test suite
â”œâ”€â”€ Dockerfile                            # Docker container definition
â”œâ”€â”€ docker-compose.yml                    # Docker Compose for development
â”œâ”€â”€ docker-compose.prod.yml               # Docker Compose for production
â”œâ”€â”€ mkdocs.yml                            # MkDocs configuration
â”œâ”€â”€ poetry.lock                           # Poetry dependency lock file
â”œâ”€â”€ pyproject.toml                        # Python project configuration (Poetry)
â”œâ”€â”€ pyrightconfig.json                    # Pyright type checker configuration
â”œâ”€â”€ README.md                             # This file
â””â”€â”€ setup_claude_connection.py            # Script for Claude Desktop connection setup (manual run)
```

## ğŸš€ Getting Started

### Deployment Prerequisites

Before running Adaptive Graph of Thoughts (either locally or via Docker if not using the provided `docker-compose.prod.yml` which includes Neo4j), ensure you have:

-   **A running Neo4j Instance**: Adaptive Graph of Thoughts requires a connection to a Neo4j graph database.
    -   **APOC Library**: Crucially, the Neo4j instance **must** have the APOC (Awesome Procedures On Cypher) library installed. Several Cypher queries within the application's reasoning stages utilize APOC procedures (e.g., `apoc.create.addLabels`, `apoc.merge.node`). Without APOC, the application will not function correctly. You can find installation instructions on the [official APOC website](https://neo4j.com/labs/apoc/installation/).
    -   **Configuration**: Ensure that your `config/settings.yaml` (or corresponding environment variables) correctly points to your Neo4j instance URI, username, and password.
    -   **Indexing**: For optimal performance, ensure appropriate Neo4j indexes are created. You can run `python scripts/run_cypher_migrations.py` to apply the provided Cypher migrations automatically. See [Neo4j Indexing Strategy](docs_src/neo4j_indexing.md) for details.

    *Note: The provided `docker-compose.yml` (for development) and `docker-compose.prod.yml` (for production) already include a Neo4j service with the APOC library pre-configured, satisfying this requirement when using Docker Compose.*

### Prerequisites

- **Python 3.11+** (as specified in `pyproject.toml`, e.g., the Docker image uses Python 3.11.x or 3.12.x, 3.13.x)
- **[Poetry](https://python-poetry.org/docs/#installation)**: For dependency management
- **[Docker](https://www.docker.com/get-started)** and **[Docker Compose](https://docs.docker.com/compose/install/)**: For containerized deployment

### Installation and Setup (Local Development)

1. **Clone the repository**:
   ```bash
   git clone https://github.com/SaptaDey/Adaptive-Graph-of-Thoughts-MCP-server.git
   cd Adaptive-Graph-of-Thoughts-MCP-server
   ```

2. **Install dependencies using Poetry**:
   ```bash
   poetry install
   ```
   This creates a virtual environment and installs all necessary packages specified in `pyproject.toml`.

3. **Activate the virtual environment**:
   ```bash
   poetry shell
   ```

4. **Configure the application**:
   ```bash
   # Copy example configuration
   cp config/settings.example.yaml config/settings.yaml
   
   # Edit configuration as needed
   vim config/settings.yaml
   ```

5. **Set up environment variables** (optional):
   ```bash
   # Create .env file for sensitive configuration
   echo ""LOG_LEVEL=DEBUG"" > .env
   echo ""API_HOST=0.0.0.0"" >> .env
   echo ""API_PORT=8000"" >> .env
   ```

### Secret Management

In production environments, set the `SECRETS_PROVIDER` environment variable to
`aws`, `gcp`, or `vault` to fetch sensitive values from a supported secrets
manager. Optionally provide `<VAR>_SECRET_NAME` variables (for example
`OPENAI_API_KEY_SECRET_NAME`) to control the name of each secret. When a secrets
provider is configured, values for `OPENAI_API_KEY`, `ANTHROPIC_API_KEY`, and
`NEO4J_PASSWORD` are loaded automatically at startup.

6. **Run the development server**:
   ```bash
   python src/adaptive_graph_of_thoughts/main.py
   ```
   
   Alternatively, for more control:
   ```bash
   uvicorn adaptive_graph_of_thoughts.main:app --reload --host 0.0.0.0 --port 8000
   ```
   
   The API will be available at `http://localhost:8000`.

## âœ¨ Setup Wizard

An interactive wizard is available to streamline initial configuration.

```bash
poetry run python -m agt_setup
```

Then visit `http://localhost:8000/setup` to complete the web-based steps.

*Setup wizard demo GIF will appear here in the full documentation.*


### Docker Deployment

```mermaid
graph TB
    subgraph ""Development Environment""
        A[ğŸ‘¨â€ğŸ’» Developer] --> B[ğŸ³ Docker Compose]
    end
    
    subgraph ""Container Orchestration""
        B --> C[ğŸ“¦ Adaptive Graph of Thoughts Container]
        B --> D[ğŸ“Š Monitoring Container]
        B --> E[ğŸ—„ï¸ Database Container]
    end
    
    subgraph ""Adaptive Graph of Thoughts Application""
        C --> F[âš¡ FastAPI Server]
        F --> G[ğŸ§  ASR-GoT Engine]
        F --> H[ğŸ”Œ MCP Protocol]
    end
    
    subgraph ""External Integrations""
        H --> I[ğŸ¤– Claude Desktop]
        H --> J[ğŸ”— Other AI Clients]
    end
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style F fill:#fff3e0
    style G fill:#ffebee
    style H fill:#f1f8e9
```

1. **Quick Start with Docker Compose**:
   ```bash
   # Build and run all services
   docker-compose up --build
   
   # For detached mode (background)
   docker-compose up --build -d
   
   # View logs
   docker-compose logs -f adaptive-graph-of-thoughts
   ```

2. **Individual Docker Container**:
   ```bash
   # Build the image
   docker build -t adaptive-graph-of-thoughts:latest .
   
   # Run the container
   docker run -p 8000:8000 -v $(pwd)/config:/app/config adaptive-graph-of-thoughts:latest
   ```

3. **Production Deployment**:
   ```bash
   # Use production compose file
   docker-compose -f docker-compose.prod.yml up --build -d
   ```

### Kubernetes Deployment (Helm)

A minimal [Helm](https://helm.sh/) chart is provided under `helm/agot-server` for
running Adaptive Graph of Thoughts on a Kubernetes cluster.

```bash
helm install agot helm/agot-server
```

Customize values in `helm/agot-server/values.yaml` to set the image repository,
resource limits, and other options.

### Notes on Specific Deployment Platforms

-   **Smithery.ai**: Deploy using the included `smithery.yaml`.
    *   Connect your GitHub repository on Smithery and click **Deploy**.
    *   The container listens on the `PORT` environment variable (default `8000`).
    *   **Health Checks** rely on the `/health` endpoint.
    *   The `Dockerfile` and `docker-compose.prod.yml` illustrate the container setup.

4. **Access the Services**:
   - **API Documentation**: `http://localhost:8000/docs`
   - **Health Check**: `http://localhost:8000/health`
   - **MCP Endpoint**: `http://localhost:8000/mcp`

## ğŸ”Œ MCP Client Integration

### Supported MCP Clients

Adaptive Graph of Thoughts supports integration with various MCP clients:

- **Claude Desktop** - Full STDIO and HTTP support
- **VS Code** - Via MCP extensions
- **Custom MCP Clients** - Generic configuration available

### Quick Client Setup

#### Claude Desktop / VS Code settings
```json
{
  ""mcpServers"": {
    ""adaptive-graph-of-thoughts"": {
      ""command"": ""python"",
      ""args"": [""-m"", ""adaptive_graph_of_thoughts.main""],
      ""cwd"": ""/path/to/Adaptive-Graph-of-Thoughts-MCP-server"",
      ""env"": {
        ""NEO4J_URI"": ""bolt://localhost:7687"",
        ""NEO4J_USER"": ""neo4j"",
        ""NEO4J_PASSWORD"": ""your_password"",
        ""MCP_TRANSPORT_TYPE"": ""stdio""
      }
    }
  }
}
```
##Available MCP Tools
        1. scientific_reasoning_query - Advanced scientific reasoning with graph analysis
        2. analyze_research_hypothesis - Hypothesis evaluation with confidence scoring
        3. explore_scientific_relationships - Concept relationship mapping
        4. validate_scientific_claims - Evidence-based claim validation


## ğŸ”Œ API Endpoints

The primary API endpoints exposed by Adaptive Graph of Thoughts are:

- **MCP Protocol Endpoint**: `POST /mcp`
  - This endpoint is used for communication with MCP clients like Claude Desktop.
  - Example Request for the `asr_got.query` method:
    ```json
    {
      ""jsonrpc"": ""2.0"",
      ""method"": ""asr_got.query"",
      ""params"": {
        ""query"": ""Analyze the relationship between microbiome diversity and cancer progression."",
        ""parameters"": {
          ""include_reasoning_trace"": true,
          ""include_graph_state"": false
        }
      },
      ""id"": ""123""
    }
    ```
  - Other supported MCP methods include `initialize` and `shutdown`.

- **Health Check Endpoint**: `GET /health`
  - Provides a simple health status of the application.
  - Example Response:
    ```json
    {
      ""status"": ""healthy"",
      ""version"": ""0.1.0"" 
    }
    ```
    *(Note: The timestamp field shown previously is not part of the current health check response.)*

The advanced API endpoints previously listed (e.g., `/api/v1/graph/query`) are not implemented in the current version and are reserved for potential future development.

## Session Handling (`session_id`)

Currently, the `session_id` parameter available in API requests (e.g., for `asr_got.query`) and present in responses serves primarily to identify and track a single, complete query-response cycle. It is also used for correlating progress notifications (like `got/queryProgress`) with the originating query.

While the system generates and utilizes `session_id`s, Adaptive Graph of Thoughts does not currently support true multi-turn conversational continuity where the detailed graph state or reasoning context from a previous query is automatically loaded and reused for a follow-up query using the same `session_id`. Each query is processed independently at this time.

### Future Enhancement: Persistent Sessions

A potential future enhancement for Adaptive Graph of Thoughts is the implementation of persistent sessions. This would enable more interactive and evolving reasoning processes by allowing users to:

1.  **Persist State:** Store the generated graph state and relevant reasoning context from a query, associated with its `session_id`, likely within the Neo4j database.
2.  **Reload State:** When a new query is submitted with an existing `session_id`, the system could reload this saved state as the starting point for further processing.
3.  **Refine and Extend:** Allow the new query to interact with the loaded graphâ€”for example, by refining previous hypotheses, adding new evidence to existing structures, or exploring alternative reasoning paths based on the established context.

Implementing persistent sessions would involve developing robust strategies for:
*   Efficiently storing and retrieving session-specific graph data in Neo4j.
*   Managing the lifecycle (e.g., creation, update, expiration) of session data.
*   Designing sophisticated logic for how new queries merge with, modify, or extend pre-existing session contexts and graphs.

This is a significant feature that could greatly enhance the interactive capabilities of Adaptive Graph of Thoughts. Contributions from the community in designing and implementing persistent session functionality are welcome.

### Future Enhancement: Asynchronous and Parallel Stage Execution

Currently, the 8 stages of the Adaptive Graph of Thoughts reasoning pipeline are executed sequentially. For complex queries or to further optimize performance, exploring asynchronous or parallel execution for certain parts of the pipeline is a potential future enhancement.

**Potential Areas for Parallelism:**

*   **Hypothesis Generation:** The `HypothesisStage` generates hypotheses for each dimension identified by the `DecompositionStage`. The process of generating hypotheses for *different, independent dimensions* could potentially be parallelized. For instance, if three dimensions are decomposed, three parallel tasks could work on generating hypotheses for each respective dimension.
*   **Evidence Integration (Partial):** Within the `EvidenceStage`, if multiple hypotheses are selected for evaluation, the ""plan execution"" phase (simulated evidence gathering) for these different hypotheses might be performed concurrently.

**Challenges and Considerations:**

Implementing parallel stage execution would introduce complexities that need careful management:

*   **Data Consistency:** Concurrent operations, especially writes to the Neo4j database (e.g., creating multiple hypothesis nodes or evidence nodes simultaneously), must be handled carefully to ensure data integrity and avoid race conditions. Unique ID generation schemes would need to be robust for parallel execution.
*   **Transaction Management:** Neo4j transactions for concurrent writes would need to be managed appropriately.
*   **Dependency Management:** Ensuring that stages (or parts of stages) that truly depend on the output of others are correctly sequenced would be critical.
*   **Resource Utilization:** Parallel execution could increase resource demands (CPU, memory, database connections).
*   **Complexity:** The overall control flow of the `GoTProcessor` would become more complex.

While the current sequential execution ensures a clear and manageable data flow, targeted parallelism in areas like hypothesis generation for independent dimensions could offer performance benefits for future versions of Adaptive Graph of Thoughts. This remains an open area for research and development.

## ğŸ§ª Testing & Quality Assurance

<div align=""center"">
  <table>
    <tr>
      <td align=""center"">ğŸ§ª<br><b>Testing</b></td>
      <td align=""center"">ğŸ”<br><b>Type Checking</b></td>
      <td align=""center"">âœ¨<br><b>Linting</b></td>
      <td align=""center"">ğŸ“Š<br><b>Coverage</b></td>
    </tr>
    <tr>
      <td align=""center"">
        <pre>poetry run pytest</pre>
        <pre>make test</pre>
      </td>
      <td align=""center"">
        <pre>poetry run mypy src/</pre>
        <pre>pyright src/</pre>
      </td>
      <td align=""center"">
        <pre>poetry run ruff check .</pre>
        <pre>poetry run ruff format .</pre>
      </td>
      <td align=""center"">
        <pre>poetry run pytest --cov=src</pre>
        <pre>coverage html</pre>
      </td>
    </tr>
  </table>
</div>

### Development Commands

Continuous integration pipelines on GitHub Actions run tests, CodeQL analysis, and Microsoft Defender for DevOps security scans.


```bash
# Run full test suite with coverage using Poetry
poetry run pytest --cov=src --cov-report=html --cov-report=term

# Or using Makefile for the default test run
make test

# Run specific test categories (using poetry)
poetry run pytest tests/unit/stages/          # Stage-specific tests
poetry run pytest tests/integration/         # Integration tests
poetry run pytest -k ""test_confidence""       # Tests matching pattern

# Type checking and linting (can also be run via Makefile targets: make lint, make check-types)
poetry run mypy src/ --strict                # Strict type checking
poetry run ruff check . --fix                # Auto-fix linting issues
poetry run ruff format .                     # Format code

# Pre-commit hooks (recommended)
poetry run pre-commit install                # Install hooks
poetry run pre-commit run --all-files       # Run all hooks (runs Ruff and MyPy)

# See Makefile for other useful targets like 'make all-checks'.
```

## ğŸ–¥ Dashboard Tour

*Dashboard demo GIF coming soon.*

## ğŸ’» IDE Integration

Use the `vscode-agot` extension to query the server from VS Code. Run the extension and execute **AGoT: Ask Graphâ€¦** from the Command Palette.

## â“ Troubleshooting

If the server fails to start or setup reports errors, ensure your Neo4j instance is running and the credentials in `.env` are correct. Consult the console output for details.

## ğŸ—ºï¸ Roadmap and Future Directions

We have an exciting vision for the future of Adaptive Graph of Thoughts! Our roadmap includes plans for enhanced graph visualization, integration with more data sources like Arxiv, and further refinements to the core reasoning engine.

For more details on our planned features and long-term goals, please see our [Roadmap](ROADMAP.md) (also available on the documentation site).

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) (also available on the documentation site) for details on how to get started, our branching strategy, code style, and more.

## ğŸ“„ License

This project is licensed under the Apache License 2.0. [License](LICENSE).

## ğŸ”’ Security

Please see our [Security Policy](SECURITY.md) for reporting vulnerabilities and details on supported versions.

## ğŸ™ Acknowledgments

- **NetworkX** community for graph analysis capabilities
- **FastAPI** team for the excellent web framework
- **Pydantic** for robust data validation
- The scientific research community for inspiration and feedback

---

<div align=""center"">
  <p><strong>Built with â¤ï¸ for the scientific research community</strong></p>
  <p><em>Adaptive Graph of Thoughts - Advancing scientific reasoning through intelligent graph structures</em></p>
</div>
",22
https://mcp.so/server/Ads-manager-mcp-server/amekala,https://github.com/amekala/Ads-manager-mcp-server,Amazon Ads Manager MCP Server,,,research-and-data,amazon-ads; ads-manager; data-analysis,"What is Amazon Ads Manager MCP Server? Amazon Ads Manager MCP Server is a Model Context Protocol (MCP) server designed for managing and analyzing Amazon Advertising data, providing a standardized interface for LLMs to interact with advertising data through the Claude Desktop App. How to use Amazon Ads Manager MCP Server? To use the server, install the Claude Desktop App, create a configuration file with your API key, and start analyzing your advertising data. Key features of Amazon Ads Manager MCP Server? Database Integration : Pre-configured connection to a secure PostgreSQL database. API Key Authentication : Secure access through API key validation. Real-time Analytics : Access to campaign metrics and performance data. Natural Language Interface : Query your advertising data using natural language. Use cases of Amazon Ads Manager MCP Server? Analyzing campaign performance metrics. Optimizing advertising budgets based on performance data. Running custom SQL queries to extract specific advertising insights. FAQ from Amazon Ads Manager MCP Server? How do I authenticate with the server? You must include an Authorization header with your API key in each request. What database does the server connect to? The server connects to a pre-configured PostgreSQL database. Can I run custom queries? Yes, you can run custom SQL queries against the database.","# Amazon Ads Manager MCP Server

A Model Context Protocol (MCP) server for managing and analyzing Amazon Advertising data. This server provides a standardized interface for LLMs to interact with your advertising data through the Claude Desktop App.

## Features

- **Database Integration**: Pre-configured connection to a secure PostgreSQL database
- **API Key Authentication**: Secure access through API key validation
- **Real-time Analytics**: Access to campaign metrics and performance data
- **Natural Language Interface**: Query your advertising data using natural language

## Installation

```bash
npm install ads-manager-mcp
```

## Quick Start

1. Install the Claude Desktop App
2. Create a configuration file `claude-desktop-config.json`:

```json
{
  ""name"": ""Amazon Ads Manager"",
  ""version"": ""1.0.4"",
  ""description"": ""Connect to your Amazon Advertising data and analyze campaign performance"",
  ""mcpServers"": {
    ""ads-manager"": {
      ""name"": ""Ads Manager MCP Server"",
      ""version"": ""1.0.4"",
      ""description"": ""MCP Server for Amazon Advertising data analysis"",
      ""transport"": ""sse"",
      ""endpoint"": ""https://mcp-server-sync-abhilashreddi.replit.app/mcp/sse"",
      ""headers"": {
        ""Authorization"": ""Bearer YOUR_API_KEY_HERE""
      }
    }
  }
}
```

3. Replace `YOUR_API_KEY_HERE` with your API key
4. Start using the Claude Desktop App to analyze your advertising data

## Available Resources

- **Schema**: View database structure and table definitions
- **Metrics**: Access advertising performance metrics
- **Campaigns**: View campaign configurations and settings
- **Ad Groups**: Access ad group details and settings

## Available Tools

- **analyzeCampaignPerformance**: Analyze campaign metrics and performance
- **analyzeAdGroupPerformance**: Get detailed ad group performance analysis
- **optimizeBudget**: Get budget optimization recommendations
- **query**: Run custom SQL queries against the database

## Example Queries

```plaintext
""Show me the structure of all tables in the database""
""Analyze the performance of campaign ABC123 for the last 30 days""
""What are the key metrics for ad group XYZ789?""
""Give me budget recommendations for all campaigns in profile P123""
```

## API Authentication

The server uses API key authentication. Each request must include an Authorization header:

```
Authorization: Bearer YOUR_API_KEY
```

## Database Connection

The server maintains a connection to a PostgreSQL database. No additional configuration is required as the connection details are pre-configured in the package.

## Development

To run the server locally:

```bash
# Install dependencies
npm install

# Start the development server
npm run dev

# Build the package
npm run build
```

## Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Support

For support, please open an issue in the GitHub repository or contact the maintainers.",5
https://mcp.so/server/AdvisorMcp/Lautarotetamusa,https://github.com/Lautarotetamusa/AdvisorMcp,MCP Server,Test MCP server and client to control advisors api,English,developer-tools,,"what is AdvisorMcp? AdvisorMcp is a test server and client designed to control the advisors API, allowing developers to interact with and manage advisor functionalities. how to use AdvisorMcp? To use AdvisorMcp, clone the repository from GitHub, set up the server and client, and follow the provided documentation to interact with the advisors API. key features of AdvisorMcp? Test server and client for advisors API Easy setup and configuration Supports TypeScript for development use cases of AdvisorMcp? Testing API endpoints for advisor functionalities Developing applications that require advisor management Integrating advisor services into existing projects FAQ from AdvisorMcp? What programming language is used in AdvisorMcp? AdvisorMcp is developed using TypeScript. Is there any documentation available? Yes, documentation is provided in the GitHub repository. Can I contribute to AdvisorMcp? Yes! Contributions are welcome, and you can submit pull requests on GitHub.","# MCP Server
MCP client and server tests. Manage advisors in natural language. \
The MCP client has two tools. get-advisors and update-advisor.

## Build
Set the DEEPSEEK_API_KEY
```
echo ""DEEPSEEK_API_KEY=<your-api-key>"" > .env
```

Server
```
cd server && npm run build && cd ..
```
Client
```
cd client && npm run build && cd ..
```

## Run
Server
you must run the server first
```
node server/build/index.js
```
Client
```
node client/build/index.js
```
",0
https://mcp.so/server/Agent-MCP-Math-Email/movva09,https://github.com/movva09/Agent-MCP-Math-Email,MCP Math Gmail Client,Agent with MCP server solves math task and emails the output,English,research-and-data,mathgpt; math-solver; math-assistant,"What is MCP Math Gmail Client? The MCP Math Gmail Client is a Python application that integrates mathematical computations with Gmail functionality, allowing users to solve math tasks and send results via email using the Gemini AI model. How to use MCP Math Gmail Client? To use the MCP Math Gmail Client, set up the MCP server, configure your environment with the necessary API keys, and run the client script to start processing mathematical queries and sending results via Gmail. Key features of MCP Math Gmail Client? Integration with Gemini AI for solving mathematical problems Gmail integration for sending results via email Support for various mathematical operations including basic and advanced functions Automated email generation with results Secure Gmail authentication and multiple recipient support Use cases of MCP Math Gmail Client? Sending results of complex mathematical calculations via email. Automating the generation of mathematical reports and analyses. Facilitating communication of mathematical findings in educational or research settings. FAQ from MCP Math Gmail Client? Can the MCP Math Gmail Client handle all types of math problems? Yes, it supports a wide range of mathematical operations from basic arithmetic to complex functions. Is there a cost to use the MCP Math Gmail Client? The client is free to use, but you may incur costs related to Gmail API usage depending on your email volume. How secure is the email communication? The client uses OAuth 2.0 for secure Gmail authentication and encrypts communication with the Gmail API.","# MCP Math Gmail Client

The Math Gmail Client (`talk2mcp_math_gmail_client.py`) is a Python application that integrates mathematical computations with Gmail functionality. It uses the Gemini AI model to process mathematical queries and can send results via email.

## Features
- Integration with Gemini AI for mathematical problem-solving
- Gmail integration for sending mathematical results
- Iterative problem-solving approach
- Support for various mathematical operations
- Automated email generation with mathematical results
- Secure Gmail authentication
- Multiple recipient support

## Server Requirements
1. MCP Server Setup:
   - Python 3.x environment
   - MCP server package installed
   - Required server dependencies:
     - `mcp` package

## Prerequisites
- Python 3.x
- Required Python packages:
  - `python-dotenv`
  - `google-generativeai`
  - `mcp` package
  - `google-auth`
  - `google-auth-oauthlib`
  - `google-auth-httplib2`
  - `google-api-python-client`

## Environment Setup
1. Create a `.env` file in the project root
2. Add your API keys:
   ```
   GEMINI_API_KEY=your_api_key_here
   GMAIL_CLIENT_ID=your_client_id_here
   GMAIL_CLIENT_SECRET=your_client_secret_here
   ```

## Server Setup
1. Install server dependencies:
   ```bash
   pip install mcp numpy pandas matplotlib
   ```

2. Start the MCP server:
   ```bash
   python example2-3_gmail_server_2.py
   ```

## Available Tools

### Currently Implemented Tools

1. Mathematical Operations:
   - Basic arithmetic operations (add, subtract, multiply, divide)
   - Advanced mathematical functions (exponential, logarithmic)
   - ASCII value calculations
   - String manipulation and conversion
   - Array operations
   - Prime number calculations
   - Factorial computations
   - Fibonacci sequence generation

2. Email Tools:
   - Single recipient email sending
   - Basic email formatting
   - Simple attachment handling
   - Basic error handling
   - Gmail authentication
   - Email queue management
   - Basic delivery status tracking

## Installation
1. Clone the repository
2. Install required packages:
   ```bash
   pip install python-dotenv google-generativeai mcp google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client
   ```
3. Set up your environment variables as described above
4. Set up Gmail API credentials:
   - Go to Google Cloud Console
   - Create a new project
   - Enable Gmail API
   - Create OAuth 2.0 credentials
   - Download the credentials file

## Usage
1. Start the MCP server (see Server Setup section)
2. Ensure the server is running and accessible
3. Run the client:
   ```bash
   python talk2mcp_math_gmail_client.py
   ```

## Server-Client Communication
1. Connection Protocol:
   - Client establishes connection with server
   - Authentication handshake
   - Tool list synchronization
   - Session initialization
   - Email service verification

2. Data Flow:
   - Client sends mathematical queries
   - Server processes requests
   - Results are formatted for email
   - Email queue is managed
   - Delivery status is tracked
   - Notifications are sent

3. Error Handling:
   - Connection retry mechanism
   - Session recovery
   - Email queue recovery
   - Rate limit handling
   - Resource cleanup

## Example Queries
1. Basic Mathematical Email:
```
Calculate the factorial of 10 and send the result to user@example.com with the subject ""Factorial Calculation Result"".
```

2. Complex Mathematical Analysis:
```
Find the first 50 Fibonacci numbers, calculate their sum, and send the detailed analysis to team@example.com with a graph attachment.
```

3. Multiple Operations with Formatting:
```
Calculate the roots of the quadratic equation xÂ² + 5x + 6 = 0, format the solution in LaTeX, and send it to professor@example.com with proper mathematical notation.
```

4. Scheduled Mathematical Report:
```
Generate a report of prime numbers between 1 and 1000, create a histogram of their distribution, and schedule it to be sent to research@example.com tomorrow at 9:00 AM.
```

## How It Works
1. The client connects to the MCP server
2. It processes mathematical queries using the Gemini AI model
3. Results are formatted for email
4. The system uses an iterative approach to solve complex problems
5. Results are sent via Gmail to specified recipients

## Error Handling
The client includes robust error handling for:
- API timeouts
- Invalid inputs
- Connection issues
- Tool execution errors
- Gmail authentication and sending errors

## Security
- Uses OAuth 2.0 for Gmail authentication
- Secure storage of API keys and credentials
- Encrypted communication with Gmail API

## Contributing
Feel free to submit issues and enhancement requests.

## License
Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

## Troubleshooting
1. Common Issues:
   - Email delivery failures
   - Attachment size limits
   - Authentication problems
   - Rate limiting issues
   - Connection timeouts

2. Solutions:
   - Check email server status
   - Verify API quotas
   - Review error logs
   - Check network connectivity
   - Validate credentials

## Performance Tips
1. Optimization:
   - Batch processing for large emails
   - Attachment compression
   - Connection pooling
   - Cache management
   - Resource monitoring

2. Best Practices:
   - Regular backup of email templates
   - Monitor API usage
   - Implement retry mechanisms
   - Use efficient data structures
   - Regular system maintenance
",0
https://mcp.so/server/Agent-MCP/Zeppelinpp,https://github.com/Zeppelinpp/Agent-MCP,Agent-MCP,local mcp server-client agent,English,developer-tools,,"what is Agent-MCP? Agent-MCP is a local server-client agent designed to facilitate communication between multiple components in a system, enabling efficient data handling and processing. how to use Agent-MCP? To use Agent-MCP, set up the local server and connect client agents to it. Follow the instructions in the GitHub repository for configuration and deployment. key features of Agent-MCP? Local server-client architecture for efficient communication Easy setup and configuration Supports multiple client connections use cases of Agent-MCP? Managing local network services Facilitating communication between IoT devices Developing distributed applications that require local data processing FAQ from Agent-MCP? What programming languages does Agent-MCP support? Agent-MCP is designed to work with various programming languages that can communicate over a network. Is Agent-MCP open source? Yes! Agent-MCP is released under the MIT license and is open for contributions. How can I contribute to Agent-MCP? You can contribute by submitting issues, feature requests, or pull requests on the GitHub repository.",,
https://mcp.so/server/AgentChat/Shy2593666979,https://github.com/Shy2593666979/AgentChat,æ¬¢è¿æ¥åˆ° æ™ºè¨€å¹³å°,AgentChat æ˜¯ä¸€ä¸ªä¸Agentäº¤æµçš„å¹³å°ï¼ŒåŒ…å«é»˜è®¤Agentï¼Œå¹¶æ”¯æŒè‡ªå®šä¹‰Agentï¼Œå¯ä»¥å®ç°å¤šè½®é—®ç­”ä½¿Agentå¸®åŠ©ç”¨æˆ·æƒ³å®ç°çš„åŠŸèƒ½ã€‚è¯¥é¡¹ç›®æŠ€æœ¯æ ˆåŒ…æ‹¬LLMã€LangChainã€Function callã€ReActã€MCPã€Milvusã€ElasticSearchã€RAGã€FastAPI,Chinese,developer-tools,agent; llm; langchain,"what is AgentChat? AgentChat is an open-source platform for interacting with agents, featuring a default agent and support for custom agents, enabling multi-turn Q&A to assist users in achieving their desired functionalities. how to use AgentChat? To use AgentChat, configure the necessary files, start the backend and frontend services, and interact with the agents through the platform. You can also use Docker for a quicker setup. key features of AgentChat? Supports multiple agents including GoogleAgent, WeatherAgent, DeliveryAgent, and ArxivAgent. Allows users to create and customize their own agents. Provides a user-friendly interface for agent interaction. use cases of AgentChat? Automating email sending based on user-defined parameters. Searching for information effectively. Checking current weather and forecasts for specified locations. Finding top research papers. Retrieving package tracking information based on courier and tracking number. Loading documents into a knowledge base for retrieval. FAQ from AgentChat? Can I create my own agents? Yes! AgentChat allows users to define their own agents with custom functionalities. Is there a quick way to set up AgentChat? Yes! You can use Docker to quickly set up the environment without configuring a MySQL database manually. What programming languages does AgentChat support? AgentChat is primarily developed in Python.","
<div align=""center"">

<img width=""500"" height=""160"" alt=""AgentChat Logo"" src=""https://github.com/user-attachments/assets/2e19a214-a87d-473f-a4fd-ee879f4e7149"" />

<p align=""center"">
  <img src=""https://img.shields.io/badge/python-3.12+-blue.svg?style=for-the-badge&logo=python&logoColor=white"" alt=""Python Version"" />
  <img src=""https://img.shields.io/badge/vue-3.4+-4FC08D.svg?style=for-the-badge&logo=vue.js&logoColor=white"" alt=""Vue Version"" />
  <img src=""https://img.shields.io/badge/fastapi-0.115+-009688.svg?style=for-the-badge&logo=fastapi&logoColor=white"" alt=""FastAPI"" />
  <img src=""https://img.shields.io/badge/license-MIT-green.svg?style=for-the-badge"" alt=""License"" />
</p>

<p align=""center"">
  <b>ğŸŒŸ ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç°ä»£åŒ–æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ ğŸŒŸ</b>
</p>

<p align=""center"">
  æ”¯æŒå¤šAgentåä½œ â€¢ çŸ¥è¯†åº“æ£€ç´¢ â€¢ å·¥å…·è°ƒç”¨ â€¢ MCPæœåŠ¡å™¨é›†æˆ â€¢ å®æ—¶å¯¹è¯
</p>

<p align=""center"">
  <a href=""https://shy2593666979.github.io/agentchat-docs/%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8.html"">ğŸš€ å¿«é€Ÿå¼€å§‹</a> â€¢
  <a href=""https://shy2593666979.github.io/agentchat-docs/%E5%BC%80%E5%8F%91%E6%8C%87%E5%8D%97/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.html"">ğŸ“¦ éƒ¨ç½²</a> â€¢
  <a href=""https://shy2593666979.github.io/agentchat-docs/"">ğŸ“– åœ¨çº¿æ–‡æ¡£</a> â€¢
  <a href=""https://agentchat.cloud"">ğŸ’» åœ¨çº¿ä½“éªŒ</a>
</p>

</div>

---

## ğŸ“‹ ç›®å½•

- [ğŸ¯ é¡¹ç›®ç®€ä»‹](#-é¡¹ç›®ç®€ä»‹)
- [âœ¨ åŠŸèƒ½å±•ç¤º](#-åŠŸèƒ½å±•ç¤º)
- [ğŸš¨ é‡è¦ç‰ˆæœ¬è¯´æ˜](#-é‡è¦ç‰ˆæœ¬è¯´æ˜)
- [ğŸ’¡ åŠŸèƒ½ç‰¹æ€§](#-åŠŸèƒ½ç‰¹æ€§)
- [ğŸ›  æŠ€æœ¯æ ˆ](#-æŠ€æœ¯æ ˆ)
- [ğŸ“ é¡¹ç›®ç»“æ„](#-é¡¹ç›®ç»“æ„)
- [ğŸš€ å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹)
- [ğŸ“¦ éƒ¨ç½²](#-é«˜çº§éƒ¨ç½²æŒ‡å—)
- [ğŸ“– æ–‡æ¡£](#-æ–‡æ¡£)
- [ğŸ“„ è®¸å¯è¯](#-è®¸å¯è¯)

---

## ğŸ¯ é¡¹ç›®ç®€ä»‹

AgentChat æ˜¯ä¸€ä¸ªç°ä»£åŒ–çš„æ™ºèƒ½å¯¹è¯ç³»ç»Ÿï¼ŒåŸºäºå¤§è¯­è¨€æ¨¡å‹æ„å»ºï¼Œæä¾›äº†ä¸°å¯Œçš„AIå¯¹è¯åŠŸèƒ½ã€‚ç³»ç»Ÿé‡‡ç”¨å‰åç«¯åˆ†ç¦»æ¶æ„ï¼Œæ”¯æŒå¤šç§AIæ¨¡å‹ã€çŸ¥è¯†åº“æ£€ç´¢ã€å·¥å…·è°ƒç”¨ã€MCPæœåŠ¡å™¨é›†æˆç­‰é«˜çº§åŠŸèƒ½ã€‚

### ğŸŒŸ æ ¸å¿ƒäº®ç‚¹

- ğŸ¤– **å¤šæ¨¡å‹æ”¯æŒ**: é›†æˆOpenAIã€DeepSeekã€Qwenç­‰ä¸»æµå¤§è¯­è¨€æ¨¡å‹
- ğŸ§  **æ™ºèƒ½Agent**: æ”¯æŒå¤šAgentåä½œï¼Œå…·å¤‡æ¨ç†å’Œå†³ç­–èƒ½åŠ›
- ğŸ“š **çŸ¥è¯†åº“æ£€ç´¢**: RAGæŠ€æœ¯å®ç°ç²¾å‡†çŸ¥è¯†æ£€ç´¢å’Œé—®ç­”
- ğŸ”§ **å·¥å…·ç”Ÿæ€**: å†…ç½®å¤šç§å®ç”¨å·¥å…·ï¼Œæ”¯æŒè‡ªå®šä¹‰æ‰©å±•
- ğŸŒ **MCPé›†æˆ**: æ”¯æŒModel Context ProtocolæœåŠ¡å™¨
- ğŸ’¬ **å®æ—¶å¯¹è¯**: æµå¼å“åº”ï¼Œæä¾›æµç•…çš„å¯¹è¯ä½“éªŒ
- ğŸ¨ **ç°ä»£ç•Œé¢**: åŸºäºVue 3å’ŒElement Plusçš„ç¾è§‚UI

---

## âœ¨ åŠŸèƒ½å±•ç¤º

> ğŸ¨ **ç•Œé¢é¢„è§ˆ** - ä½“éªŒç°ä»£åŒ–çš„æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ

<div align=""center"">

### ğŸ–¥ï¸æ–°å¢å·¥ä½œåŒº
*æ–°å¢å·¥ä½œåŒºï¼Œå·¥ä½œåŒºå’Œåº”ç”¨ä¸­å¿ƒå¯éšæ„åˆ‡æ¢*
<img width=""800"" height=""450"" alt=""c453afd6-84ed-4bb7-b268-407fb531a1c9"" src=""https://github.com/user-attachments/assets/766c7628-2256-4c8b-a838-c400eaa78d6b"" />


### âœ¨çµå¯»ä»»åŠ¡è§„åˆ’
*å®æ—¶çš„ä»»åŠ¡æµç¨‹å›¾ï¼Œæ›´åŠ ç›´è§‚çš„æ„Ÿå—*
<img width=""800"" height=""450"" alt=""cc59faad-4112-48cd-b9b1-6f89d3cbdb39"" src=""https://github.com/user-attachments/assets/53f7fe9f-d70d-4cc2-bf7e-b47a712a6d7a"" />

### ğŸ“Šæ•°æ®çœ‹æ¿
*èƒ½å¤Ÿæ ¹æ®Agentã€æ¨¡å‹ã€æ—¶é—´èŒƒå›´è¿›è¡Œç­›é€‰è°ƒç”¨æ¬¡æ•°å’ŒTokenä½¿ç”¨é‡* 
<img width=""800"" height=""450"" alt=""5e90c531-0a10-4457-962d-984b0568f25d"" src=""https://github.com/user-attachments/assets/2811a276-a2e3-4b10-8764-605bcbc80254"" />


### ğŸ  æ™ºè¨€å¹³å°é¦–é¡µ
*ç®€æ´ç°ä»£çš„ä¸»ç•Œé¢ï¼Œæä¾›ç›´è§‚çš„åŠŸèƒ½å¯¼èˆª*

<img width=""800"" height=""450"" alt=""5a6ee271-7c81-4032-aeca-57496634a64f"" src=""https://github.com/user-attachments/assets/0295e9ea-6f15-484c-b223-82000bc9d33c"" />



### ğŸ” æ™ºè¨€å¹³å°ç™»å½•é¡µ
*å®‰å…¨ä¾¿æ·çš„ç”¨æˆ·è®¤è¯ç³»ç»Ÿ*

<img width=""800"" height=""450"" alt=""d67ba546-b3bd-40e0-b09c-9b1615afda29"" src=""https://github.com/user-attachments/assets/87b6e15c-a4aa-47c5-b588-0bc977599311"" />


### ğŸ¤– æ™ºèƒ½ä½“ç®¡ç†é¡µé¢
*å¼ºå¤§çš„Agenté…ç½®å’Œç®¡ç†ä¸­å¿ƒ*

<img width=""800"" height=""450"" alt=""æ™ºèƒ½ä½“é¡µé¢"" src=""https://github.com/user-attachments/assets/e58f120e-2e53-4041-b3f8-2243083dccf3"" />

</div>

### ğŸŒŸ æ™ºèƒ½AgentåŠŸèƒ½æ¼”ç¤º

<table>
<tr>
<td width=""50%"">

#### ğŸŒ¤ï¸ å¤©æ°”æŸ¥è¯¢Agent
*å®æ—¶å¤©æ°”ä¿¡æ¯æŸ¥è¯¢å’Œé¢„æŠ¥*

<img width=""400"" height=""240"" alt=""å¤©æ°”æŸ¥è¯¢Agent"" src=""https://github.com/user-attachments/assets/91a95c2b-f194-4c25-ba0f-f8cb393cba50"" />

</td>
<td width=""50%"">

#### ğŸ¨ æ–‡ç”Ÿå›¾Agent
*AIé©±åŠ¨çš„å›¾åƒç”ŸæˆæœåŠ¡*

<img width=""400"" height=""240"" alt=""æ–‡ç”Ÿå›¾Agent"" src=""https://github.com/user-attachments/assets/58194798-5c3e-4d7d-895c-944b6665e5a6"" />


</td>
</tr>
</table>

<div align=""center"">

<div align=""center"">

### ğŸ”æ™ºèƒ½ä½“å·¥å…·å¤šè½®è°ƒç”¨

å¹³å°ä¸­æ™ºèƒ½ä½“æ”¯æŒå·¥å…·å¤šè½®è°ƒç”¨ï¼ˆæŒ‡çš„æ˜¯æ ¹æ®å·¥å…·Cä¾èµ–å·¥å…·Bç»“æœï¼Œæ‰§è¡Œå·¥å…·Bä¾èµ–å·¥å…·Aç»“æœï¼Œæ‰€ä»¥è°ƒç”¨å·¥å…·çš„é¡ºåºæ˜¯ A --> B --> Cï¼‰
<img width=""800"" height=""450"" alt=""dc426a1b220af20a06b068a4ffc2bb10"" src=""https://github.com/user-attachments/assets/029c70ce-e5fa-4f2c-926a-a5dfd719e237"" />

<div align=""center"">

### ğŸ”Œ MCPæœåŠ¡å™¨é›†æˆ
*æ”¯æŒModel Context Protocolï¼Œå¯ä¸Šä¼ è‡ªå®šä¹‰MCPæœåŠ¡*

<img width=""800"" height=""450"" alt=""MCPé›†æˆ"" src=""https://github.com/user-attachments/assets/7a9f4588-1098-4388-85d9-78a1a4130ec3"" />

### ğŸ“š çŸ¥è¯†åº“ç®¡ç†ç³»ç»Ÿ
*æ™ºèƒ½çŸ¥è¯†ç®¡ç†ï¼Œä¸ºAgentæä¾›ä¸°å¯Œçš„å¤–éƒ¨çŸ¥è¯†æ”¯æŒ*

<img width=""800"" height=""450"" alt=""çŸ¥è¯†åº“ç®¡ç†"" src=""https://github.com/user-attachments/assets/471ad0d3-e99b-4da0-9338-4fae41eaad68"" />

### ğŸ“„ æ–‡æ¡£è§£æå¼•æ“
*æ”¯æŒPDFã€Markdownã€Docxã€Txtç­‰å¤šç§æ ¼å¼çš„æ™ºèƒ½è§£æ*

<img width=""800"" height=""450"" alt=""æ–‡æ¡£è§£æ"" src=""https://github.com/user-attachments/assets/0d030916-b6e2-482c-b828-b760fc574cae"" />

### ğŸ› ï¸ å·¥å…·ç®¡ç†ä¸­å¿ƒ
*ä¸°å¯Œçš„å†…ç½®å·¥å…·é›†ï¼ŒæŒç»­æ‰©å±•ä¸­*

<img width=""800"" height=""450"" alt=""å·¥å…·ç®¡ç†"" src=""https://github.com/user-attachments/assets/70fe68ce-56e0-44be-b78a-817ed32d4708"" />

### ğŸ§  AIæ¨¡å‹ç®¡ç†
*å¤šæ¨¡å‹æ”¯æŒï¼Œçµæ´»é…ç½®ä¸åŒAIæœåŠ¡*

<img width=""800"" height=""450"" alt=""æ¨¡å‹ç®¡ç†"" src=""https://github.com/user-attachments/assets/41a49873-f758-49f2-86a4-1a1a57677018"" />

### ğŸ“° ç”ŸæˆAIæ—¥æŠ¥
*è·å–æœ€æ–°çš„AIå’¨è¯¢ï¼Œæ”¯æŒç”Ÿæˆå›¾ç‰‡ç±»å‹çš„æ—¥æŠ¥*

<img width=""500"" height=""1500"" alt=""image"" src=""https://github.com/user-attachments/assets/a4f4489a-19bf-4516-96db-77ff2525beb8"" />


</div>


## ğŸš¨ é‡è¦ç‰ˆæœ¬è¯´æ˜

> **âš ï¸ ä» AgentChat v2.2.0 ç‰ˆæœ¬å¼€å§‹ï¼ŒLangChain å·²å‡çº§è‡³ 1.0 ç‰ˆæœ¬ï¼Œä»£ç æ”¹åŠ¨è¾ƒå¤§ï¼**

<div align=""center"">

| ğŸ”„ **ç‰ˆæœ¬** | ğŸ“¦ **LangChainç‰ˆæœ¬** | ğŸ”§ **å…¼å®¹æ€§** | ğŸ“ **è¯´æ˜** |
|:---:|:---:|:---:|:---|
| **v2.1.x åŠä»¥ä¸‹** | 0.x | âš ï¸ æ—§ç‰ˆæœ¬ | ä½¿ç”¨æ—§ç‰ˆLangChain API |
| **v2.2.0+** | 1.0+ | âœ… æœ€æ–°ç‰ˆæœ¬ | **é‡å¤§æ›´æ–°**ï¼ŒAPIå˜åŒ–è¾ƒå¤§ |

</div>

**å‡çº§æ³¨æ„äº‹é¡¹ï¼š**
- ğŸ”„ LangChain 1.0 å¼•å…¥äº†é‡å¤§APIå˜æ›´
- ğŸ“š éƒ¨åˆ†å·¥å…·å’ŒAgenté…ç½®æ–¹å¼å·²æ›´æ–°
- ğŸ› ï¸ å»ºè®®æŸ¥çœ‹[è¿ç§»æŒ‡å—](docs/migration.md)äº†è§£è¯¦ç»†å˜æ›´
- ğŸ’¡ æ–°ç”¨æˆ·å»ºè®®ç›´æ¥ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬

---

## ğŸ’¡ åŠŸèƒ½ç‰¹æ€§

> â­ **å…¨æ–¹ä½çš„AIæ™ºèƒ½æœåŠ¡** - ä»å¯¹è¯åˆ°å·¥å…·ï¼Œä»çŸ¥è¯†åˆ°å†³ç­–

<div align=""center"">

### ğŸ¯ æ ¸å¿ƒåŠŸèƒ½æ¨¡å—

</div>

<table>
<tr>
<td width=""50%"">

#### ğŸ¤– **AIå¯¹è¯å¼•æ“**
> *æ™ºèƒ½ã€è‡ªç„¶ã€é«˜æ•ˆçš„å¯¹è¯ä½“éªŒ*

- âœ¨ **å¤šæ¨¡å‹ç”Ÿæ€**: æ”¯æŒOpenAIã€Anthropicã€é€šä¹‰åƒé—®ç­‰ä¸»æµLLM
- ğŸŒŠ **æµå¼å“åº”**: å®æ—¶æ˜¾ç¤ºç”Ÿæˆå†…å®¹ï¼Œæ— éœ€ç­‰å¾…
- ğŸ§  **ä¸Šä¸‹æ–‡è®°å¿†**: æ”¯æŒé•¿å¯¹è¯ï¼Œæ™ºèƒ½ç†è§£å¯¹è¯å†å²
- ğŸ” **æ€è€ƒå¯è§†åŒ–**: æ·±åº¦æ€è€ƒé¢æ¿ï¼Œå±•ç¤ºAIæ¨ç†è¿‡ç¨‹
- ğŸ“ **å¯¹è¯ç®¡ç†**: å®Œæ•´çš„å¯¹è¯å†å²å­˜å‚¨å’Œæ£€ç´¢
- ğŸ›ï¸ **å‚æ•°è°ƒä¼˜**: æ¸©åº¦ã€Top-pç­‰å‚æ•°ç²¾ç»†æ§åˆ¶

</td>
<td width=""50%"">

#### ğŸ§  **æ™ºèƒ½Agentç³»ç»Ÿ**
> *å¤šæ™ºèƒ½ä½“åä½œï¼Œè‡ªåŠ¨åŒ–ä»»åŠ¡æ‰§è¡Œ*

- ğŸ¤ **å¤šAgentåä½œ**: æ™ºèƒ½ä½“é—´ä»»åŠ¡åˆ†å·¥ä¸åè°ƒ
- ğŸ”§ **ä»»åŠ¡è‡ªåŠ¨åŒ–**: æ™ºèƒ½åˆ†è§£å¤æ‚ä»»åŠ¡ï¼Œè‡ªåŠ¨æ‰§è¡Œ
- âš™ï¸ **èƒ½åŠ›é…ç½®**: çµæ´»çš„Agentèƒ½åŠ›å®šä¹‰å’Œç®¡ç†
- ğŸ”„ **å·¥ä½œæµç¼–æ’**: å¯è§†åŒ–å·¥ä½œæµè®¾è®¡å’Œæ‰§è¡Œ
- ğŸ“Š **æ‰§è¡Œç›‘æ§**: å®æ—¶ç›‘æ§Agentæ‰§è¡ŒçŠ¶æ€
- ğŸ¯ **ç›®æ ‡å¯¼å‘**: åŸºäºç›®æ ‡çš„æ™ºèƒ½å†³ç­–å’Œè¡ŒåŠ¨

</td>
</tr>
</table>

<table>
<tr>
<td width=""50%"">

#### ğŸ“š **çŸ¥è¯†åº“ç³»ç»Ÿ**
> *ä¼ä¸šçº§çŸ¥è¯†ç®¡ç†ï¼ŒRAGæŠ€æœ¯åŠ æŒ*

- ğŸ“ **å¤šæ ¼å¼æ”¯æŒ**: PDFã€Wordã€Excelã€Markdownã€TXTç­‰
- ğŸ§© **æ™ºèƒ½åˆ†å—**: è¯­ä¹‰çº§åˆ«çš„æ–‡æ¡£åˆ†å‰²å’Œå¤„ç†
- ğŸ” **å‘é‡æ£€ç´¢**: åŸºäºè¯­ä¹‰çš„ç²¾å‡†çŸ¥è¯†æ£€ç´¢
- ğŸ’¡ **RAGé—®ç­”**: æ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œæé«˜å›ç­”å‡†ç¡®æ€§
- ğŸ—‚ï¸ **çŸ¥è¯†ç»„ç»‡**: åˆ†ç±»ç®¡ç†ï¼Œæ ‡ç­¾ç³»ç»Ÿ
- ğŸ“ˆ **ä½¿ç”¨ç»Ÿè®¡**: çŸ¥è¯†åº“ä½¿ç”¨æƒ…å†µåˆ†æ

</td>
<td width=""50%"">

#### ğŸ› ï¸ **ä¸°å¯Œå·¥å…·ç”Ÿæ€**
> *10+å†…ç½®å·¥å…·ï¼Œæ— é™æ‰©å±•å¯èƒ½*

- ğŸ“§ **é€šä¿¡å·¥å…·**: é‚®ä»¶å‘é€ã€æ¶ˆæ¯æ¨é€
- ğŸ” **ä¿¡æ¯æ£€ç´¢**: Googleæœç´¢ã€å­¦æœ¯è®ºæ–‡æœç´¢
- ğŸŒ¤ï¸ **ç”Ÿæ´»æœåŠ¡**: å¤©æ°”æŸ¥è¯¢ã€å¿«é€’è¿½è¸ª
- ğŸ“„ **æ–‡æ¡£å¤„ç†**: æ ¼å¼è½¬æ¢ã€å†…å®¹æå–
- ğŸ¨ **å¤šåª’ä½“**: æ–‡ç”Ÿå›¾ã€å›¾åƒè¯†åˆ«ã€OCR
- ğŸ“Š **æ•°æ®åˆ†æ**: Excelå¤„ç†ã€æ•°æ®å¯è§†åŒ–
- ğŸ¤– **è‡ªåŠ¨åŒ–**: ç®€å†ä¼˜åŒ–ã€å†…å®¹é‡å†™
- ğŸ•·ï¸ **ç½‘ç»œå·¥å…·**: ç½‘é¡µçˆ¬å–ã€å†…å®¹æŠ“å–

</td>
</tr>
</table>

<div align=""center"">

### ğŸ”§ **é«˜çº§ç‰¹æ€§**

</div>

<table>
<tr>
<td width=""33%"">

#### ğŸŒ **MCPæœåŠ¡å™¨**
*Model Context Protocolé›†æˆ*

- ğŸ”Œ **åè®®æ”¯æŒ**: å®Œæ•´MCPåè®®å®ç°
- ğŸ—ï¸ **è‡ªå®šä¹‰æœåŠ¡**: æ”¯æŒç”¨æˆ·è‡ªå®šä¹‰MCPæœåŠ¡å™¨
- ğŸ“¦ **å†…ç½®æœåŠ¡**: å¤©æ°”ã€ArXivç­‰é¢„æ„å»ºæœåŠ¡
- ğŸ”„ **åŠ¨æ€åŠ è½½**: è¿è¡Œæ—¶åŠ¨æ€åŠ è½½MCPæœåŠ¡
- âš¡ **é«˜æ€§èƒ½**: å¼‚æ­¥å¤„ç†ï¼Œå¿«é€Ÿå“åº”

</td>
<td width=""33%"">

#### ğŸ‘¤ **ç”¨æˆ·ç®¡ç†**
*å®‰å…¨çš„èº«ä»½è®¤è¯ä¸æƒé™æ§åˆ¶*

- ğŸ” **å®‰å…¨è®¤è¯**: JWTä»¤ç‰Œï¼Œå®‰å…¨å¯é 
- ğŸ‘¥ **ç”¨æˆ·ç³»ç»Ÿ**: æ³¨å†Œã€ç™»å½•ã€ä¸ªäººèµ„æ–™
- ğŸ›¡ï¸ **æƒé™æ§åˆ¶**: ç»†ç²’åº¦æƒé™ç®¡ç†
- âš™ï¸ **ä¸ªæ€§é…ç½®**: ä¸ªäººåå¥½è®¾ç½®
- ğŸ“Š **ä½¿ç”¨ç»Ÿè®¡**: ç”¨æˆ·è¡Œä¸ºåˆ†æ

</td>
<td width=""33%"">

#### ğŸ—ï¸ **ç³»ç»Ÿæ¶æ„**
*ç°ä»£åŒ–çš„æŠ€æœ¯æ¶æ„*

- ğŸ”„ **å‰åç«¯åˆ†ç¦»**: Vue3 + FastAPI
- ğŸ“¡ **å®æ—¶é€šä¿¡**: WebSocketæ”¯æŒ
- ğŸ’¾ **å¤šæ•°æ®åº“**: MySQLã€Redisã€ChromaDB
- ğŸ³ **å®¹å™¨åŒ–**: Dockeréƒ¨ç½²ï¼Œæ˜“äºæ‰©å±•
- ğŸ“ˆ **å¯ç›‘æ§**: å®Œæ•´çš„æ—¥å¿—å’Œç›‘æ§ä½“ç³»

</td>
</tr>
</table>

### ğŸ¨ **æŠ€æœ¯äº®ç‚¹**

<div align=""center"">

| ğŸŒŸ **ç‰¹æ€§** | ğŸ“ **æè¿°** | ğŸ”§ **æŠ€æœ¯** |
|:---:|:---|:---|
| **æµå¼å“åº”** | å®æ—¶ç”Ÿæˆå†…å®¹ï¼Œæå‡ç”¨æˆ·ä½“éªŒ | Server-Sent Events |
| **å‘é‡æ£€ç´¢** | è¯­ä¹‰çº§åˆ«çš„çŸ¥è¯†æ£€ç´¢ | ChromaDB + Embedding |
| **å¼‚æ­¥å¤„ç†** | é«˜å¹¶å‘ä»»åŠ¡å¤„ç† | FastAPI + AsyncIO |
| **æ¨¡å—åŒ–è®¾è®¡** | æ¾è€¦åˆæ¶æ„ï¼Œæ˜“äºæ‰©å±• | å¾®æœåŠ¡æ¶æ„ |
| **æ™ºèƒ½ç¼“å­˜** | Redisç¼“å­˜ï¼Œæå‡å“åº”é€Ÿåº¦ | Redis + æ™ºèƒ½ç¼“å­˜ç­–ç•¥ |

</div>

---

## ğŸ›  æŠ€æœ¯æ ˆ

### åç«¯æŠ€æœ¯
- **æ¡†æ¶**: FastAPI (Python 3.12+)
- **AIé›†æˆ**: LangChain, OpenAI, Anthropic
- **æ•°æ®åº“**: MySQL 8.0, Redis 7.0
- **å‘é‡æ•°æ®åº“**: ChromaDB, Milvus
- **æœç´¢å¼•æ“**: Elasticsearch
- **æ–‡æ¡£å¤„ç†**: PyMuPDF, Unstructured
- **å¼‚æ­¥ä»»åŠ¡**: Celery
- **éƒ¨ç½²**: Docker, Gunicorn, Uvicorn

### å‰ç«¯æŠ€æœ¯
- **æ¡†æ¶**: Vue 3.4+ (Composition API)
- **UIç»„ä»¶**: Element Plus
- **çŠ¶æ€ç®¡ç†**: Pinia
- **è·¯ç”±**: Vue Router 4
- **æ„å»ºå·¥å…·**: Vite 5
- **å¼€å‘è¯­è¨€**: TypeScript
- **æ ·å¼**: SCSS
- **Markdown**: md-editor-v3

### å¼€å‘å·¥å…·
- **åŒ…ç®¡ç†**: Poetry (åç«¯), npm (å‰ç«¯)
- **ä»£ç æ ¼å¼**: Black, Prettier
- **ç±»å‹æ£€æŸ¥**: mypy, TypeScript
- **å®¹å™¨åŒ–**: Docker, Docker Compose

---

## ğŸ“ é¡¹ç›®ç»“æ„

> ğŸ—ï¸ **å®Œæ•´çš„é¡¹ç›®æ¶æ„** - æ¨¡å—åŒ–è®¾è®¡ï¼Œæ¸…æ™°çš„èŒè´£åˆ†ç¦»

<details>
<summary><b>ğŸ” ç‚¹å‡»å±•å¼€å®Œæ•´é¡¹ç›®ç»“æ„</b></summary>

```
AgentChat/                          # ğŸ  é¡¹ç›®æ ¹ç›®å½•
â”œâ”€â”€ ğŸ“„ README.md                   # ğŸ“– é¡¹ç›®è¯´æ˜æ–‡æ¡£
â”œâ”€â”€ ğŸ“„ LICENSE                     # âš–ï¸ å¼€æºè®¸å¯è¯
â”œâ”€â”€ ğŸ“„ .gitignore                  # ğŸš« Gitå¿½ç•¥æ–‡ä»¶é…ç½®
â”œâ”€â”€ ğŸ“„ pyproject.toml              # ğŸ Pythoné¡¹ç›®é…ç½®
â”œâ”€â”€ ğŸ“„ requirements.txt            # ğŸ“¦ Pythonä¾èµ–åŒ…åˆ—è¡¨
â”‚
â”œâ”€â”€ ğŸ“ .vscode/                    # ğŸ”§ VSCodeç¼–è¾‘å™¨é…ç½®
â”œâ”€â”€ ğŸ“ .idea/                      # ğŸ’¡ JetBrains IDEé…ç½®
â”‚
â”œâ”€â”€ ğŸ“ docs/                       # ğŸ“š é¡¹ç›®æ–‡æ¡£ç›®å½•
â”‚   â”œâ”€â”€ ğŸ“„ API_Documentation_v3.0.md  # ğŸ”„ æœ€æ–°APIæ–‡æ¡£
â”‚   â”œâ”€â”€ ğŸ“„ API_Documentation_v2.0.md  # ğŸ“‹ v2.0 APIæ–‡æ¡£
â”‚   â””â”€â”€ ğŸ“„ API_Documentation_v1.0.md  # ğŸ“ v1.0 APIæ–‡æ¡£
â”‚
â”œâ”€â”€ ğŸ“ docker/                     # ğŸ³ å®¹å™¨åŒ–é…ç½®
â”‚   â”œâ”€â”€ ğŸ“„ Dockerfile              # ğŸ³ Dockeré•œåƒæ„å»ºæ–‡ä»¶
â”‚   â””â”€â”€ ğŸ“„ docker-compose.yml      # ğŸ”§ Dockerç¼–æ’é…ç½®
â”‚
â””â”€â”€ ğŸ“ src/                        # ğŸ’» æºä»£ç ç›®å½•
    â”œâ”€â”€ ğŸ“ backend/                # ğŸ”§ åç«¯æœåŠ¡
    â”‚   â”œâ”€â”€ ğŸ“ chroma_db/          # ğŸ—„ï¸ ChromaDBå‘é‡æ•°æ®åº“
    â”‚   â””â”€â”€ ğŸ“ agentchat/          # ğŸ¤– æ ¸å¿ƒåç«¯åº”ç”¨
    â”‚       â”œâ”€â”€ ğŸ“„ __init__.py     # ğŸ PythonåŒ…åˆå§‹åŒ–æ–‡ä»¶
    â”‚       â”œâ”€â”€ ğŸ“„ main.py         # ğŸš€ FastAPIåº”ç”¨å…¥å£
    â”‚       â”œâ”€â”€ ğŸ“„ settings.py     # âš™ï¸ åº”ç”¨é…ç½®è®¾ç½®
    â”‚       â”œâ”€â”€ ğŸ“„ config.yaml     # ğŸ“‹ YAMLé…ç½®æ–‡ä»¶
    â”‚       â”‚
    â”‚       â”œâ”€â”€ ğŸ“ api/            # ğŸŒ APIè·¯ç”±å±‚
    â”‚       â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
    â”‚       â”‚   â”œâ”€â”€ ğŸ“„ router.py   # ğŸ”€ ä¸»è·¯ç”±é…ç½®
    â”‚       â”‚   â”œâ”€â”€ ğŸ“„ JWT.py      # ğŸ” JWTè®¤è¯å¤„ç†
    â”‚       â”‚   â”œâ”€â”€ ğŸ“ v1/         # ğŸ“Š v1ç‰ˆæœ¬APIæ¥å£
    â”‚       â”‚   â”œâ”€â”€ ğŸ“ services/   # ğŸ”§ æœåŠ¡å±‚API
    â”‚       â”‚   â””â”€â”€ ğŸ“ errcode/    # âŒ é”™è¯¯ç å®šä¹‰
    â”‚       â”‚
    â”‚       â”œâ”€â”€ ğŸ“ core/           # ğŸ—ï¸ æ ¸å¿ƒåŠŸèƒ½æ¨¡å—
    â”‚       â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
    â”‚       â”‚   â””â”€â”€ ğŸ“ models/     # ğŸ§  AIæ¨¡å‹ç®¡ç†
    â”‚       â”‚
    â”‚       â”œâ”€â”€ ğŸ“ database/       # ğŸ—ƒï¸ æ•°æ®åº“å±‚
    â”‚       â”‚   â”œâ”€â”€ ğŸ“„ __init__.py # ğŸ”— æ•°æ®åº“è¿æ¥é…ç½®
    â”‚       â”‚   â”œâ”€â”€ ğŸ“„ init_data.py # ğŸ—ï¸ æ•°æ®åº“åˆå§‹åŒ–è„šæœ¬
    â”‚       â”‚   â”œâ”€â”€ ğŸ“ models/     # ğŸ“Š æ•°æ®æ¨¡å‹å®šä¹‰
    â”‚       â”‚   â””â”€â”€ ğŸ“ dao/        # ğŸ’¾ æ•°æ®è®¿é—®å¯¹è±¡
    â”‚       â”‚
    â”‚       â”œâ”€â”€ ğŸ“ services/       # ğŸ¯ ä¸šåŠ¡æœåŠ¡å±‚
    â”‚       â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
    â”‚       â”‚   â”œâ”€â”€ ğŸ“„ retrieval.py      # ğŸ” ä¿¡æ¯æ£€ç´¢æœåŠ¡
    â”‚       â”‚   â”œâ”€â”€ ğŸ“„ rag_handler.py    # ğŸ“š RAGå¤„ç†æœåŠ¡
    â”‚       â”‚   â”œâ”€â”€ ğŸ“„ aliyun_oss.py     # â˜ï¸ é˜¿é‡Œäº‘OSSæœåŠ¡
    â”‚       â”‚   â”œâ”€â”€ ğŸ“„ redis.py          # ğŸ’¾ Redisç¼“å­˜æœåŠ¡
    â”‚       â”‚   â”œâ”€â”€ ğŸ“ rag/              # ğŸ“– RAGæ£€ç´¢å¢å¼ºç”Ÿæˆ
    â”‚       â”‚   â”œâ”€â”€ ğŸ“ mars/             # ğŸš€ Marsæ™ºèƒ½ä½“æœåŠ¡
    â”‚       â”‚   â”œâ”€â”€ ğŸ“ mcp/              # ğŸ”Œ MCPåè®®æœåŠ¡
    â”‚       â”‚   â”œâ”€â”€ ğŸ“ mcp_agent/        # ğŸ¤– MCP AgentæœåŠ¡
    â”‚       â”‚   â”œâ”€â”€ ğŸ“ mcp_openai/       # ğŸ§  MCP OpenAIé›†æˆ
    â”‚       â”‚   â”œâ”€â”€ ğŸ“ deepsearch/       # ğŸ•µï¸ æ·±åº¦æœç´¢æœåŠ¡
    â”‚       â”‚   â”œâ”€â”€ ğŸ“ transform_paper/  # ğŸ“„ è®ºæ–‡è½¬æ¢æœåŠ¡
    â”‚       â”‚   â”œâ”€â”€ ğŸ“ autobuild/        # ğŸ—ï¸ è‡ªåŠ¨æ„å»ºæœåŠ¡
    â”‚       â”‚   â””â”€â”€ ğŸ“ rewrite/          # âœï¸ å†…å®¹é‡å†™æœåŠ¡
    â”‚       â”‚
    â”‚       â”œâ”€â”€ ğŸ“ tools/          # ğŸ› ï¸ å·¥å…·é›†æˆ
    â”‚       â”‚   â”œâ”€â”€ ğŸ“„ __init__.py # ğŸ§° å·¥å…·æ³¨å†Œå’Œç®¡ç†
    â”‚       â”‚   â”œâ”€â”€ ğŸ“ arxiv/      # ğŸ“š ArXivè®ºæ–‡å·¥å…·
    â”‚       â”‚   â”œâ”€â”€ ğŸ“ delivery/   # ğŸ“¦ å¿«é€’æŸ¥è¯¢å·¥å…·
    â”‚       â”‚   â”œâ”€â”€ ğŸ“ web_search/ # ğŸ” ç½‘ç»œæœç´¢å·¥å…·
    â”‚       â”‚   â”œâ”€â”€ ğŸ“ get_weather/     # ğŸŒ¤ï¸ å¤©æ°”æŸ¥è¯¢å·¥å…·
    â”‚       â”‚   â”œâ”€â”€ ğŸ“ send_email/      # ğŸ“§ é‚®ä»¶å‘é€å·¥å…·
    â”‚       â”‚   â”œâ”€â”€ ğŸ“ text2image/      # ğŸ¨ æ–‡æœ¬è½¬å›¾ç‰‡å·¥å…·
    â”‚       â”‚   â”œâ”€â”€ ğŸ“ image2text/      # ğŸ‘ï¸ å›¾ç‰‡è½¬æ–‡æœ¬å·¥å…·
    â”‚       â”‚   â”œâ”€â”€ ğŸ“ convert_to_pdf/  # ğŸ“„ PDFè½¬æ¢å·¥å…·
    â”‚       â”‚   â”œâ”€â”€ ğŸ“ convert_to_docx/ # ğŸ“ Wordè½¬æ¢å·¥å…·
    â”‚       â”‚   â”œâ”€â”€ ğŸ“ resume_optimizer/# ğŸ“‹ ç®€å†ä¼˜åŒ–å·¥å…·
    â”‚       â”‚   â”œâ”€â”€ ğŸ“ rag_data/        # ğŸ“Š RAGæ•°æ®å¤„ç†å·¥å…·
    â”‚       â”‚   â””â”€â”€ ğŸ“ crawl_web/       # ğŸ•·ï¸ ç½‘é¡µçˆ¬è™«å·¥å…·
    â”‚       â”‚
    â”‚       â”œâ”€â”€ ğŸ“ mcp_servers/    # ğŸ–¥ï¸ MCPæœåŠ¡å™¨é›†åˆ
    â”‚       â”œâ”€â”€ ğŸ“ prompts/        # ğŸ’¬ æç¤ºè¯æ¨¡æ¿åº“
    â”‚       â”œâ”€â”€ ğŸ“ config/         # âš™ï¸ é…ç½®æ–‡ä»¶ç›®å½•
    â”‚       â”œâ”€â”€ ğŸ“ schema/         # ğŸ“‹ æ•°æ®æ¨¡å¼å®šä¹‰
    â”‚       â”œâ”€â”€ ğŸ“ data/           # ğŸ’¾ æ•°æ®å­˜å‚¨ç›®å½•
    â”‚       â”œâ”€â”€ ğŸ“ utils/          # ğŸ§° é€šç”¨å·¥å…·å‡½æ•°
    â”‚       â””â”€â”€ ğŸ“ test/           # ğŸ§ª æµ‹è¯•ä»£ç ç›®å½•
    â”‚
    â””â”€â”€ ğŸ“ frontend/               # ğŸ¨ å‰ç«¯åº”ç”¨
        â”œâ”€â”€ ğŸ“„ package.json       # ğŸ“¦ Node.jsé¡¹ç›®é…ç½®
        â”œâ”€â”€ ğŸ“„ package-lock.json  # ğŸ”’ ä¾èµ–ç‰ˆæœ¬é”å®š
        â”œâ”€â”€ ğŸ“„ tsconfig.json      # ğŸ”§ TypeScripté…ç½®
        â”œâ”€â”€ ğŸ“„ tsconfig.app.json  # ğŸ“± åº”ç”¨TypeScripté…ç½®
        â”œâ”€â”€ ğŸ“„ tsconfig.node.json # ğŸ”§ Nodeç¯å¢ƒTypeScripté…ç½®
        â”œâ”€â”€ ğŸ“„ vite.config.ts     # âš¡ Viteæ„å»ºé…ç½®
        â”œâ”€â”€ ğŸ“„ index.html         # ğŸŒ HTMLå…¥å£æ–‡ä»¶
        â”œâ”€â”€ ğŸ“„ .gitignore         # ğŸš« å‰ç«¯Gitå¿½ç•¥é…ç½®
        â”œâ”€â”€ ğŸ“„ README.md          # ğŸ“– å‰ç«¯è¯´æ˜æ–‡æ¡£
        â”œâ”€â”€ ğŸ“„ DEBUGGING_GUIDE.md # ğŸ› è°ƒè¯•æŒ‡å—
        â”œâ”€â”€ ğŸ“„ auto-imports.d.ts  # ğŸ”„ è‡ªåŠ¨å¯¼å…¥ç±»å‹å£°æ˜
        â”œâ”€â”€ ğŸ“„ components.d.ts    # ğŸ§© ç»„ä»¶ç±»å‹å£°æ˜
        â”‚
        â”œâ”€â”€ ğŸ“ public/            # ğŸŒ é™æ€èµ„æºç›®å½•
        â”‚
        â””â”€â”€ ğŸ“ src/               # ğŸ’» å‰ç«¯æºä»£ç 
            â”œâ”€â”€ ğŸ“„ main.ts        # ğŸš€ Vueåº”ç”¨å…¥å£
            â”œâ”€â”€ ğŸ“„ App.vue        # ğŸ  æ ¹ç»„ä»¶
            â”œâ”€â”€ ğŸ“„ style.css      # ğŸ¨ å…¨å±€æ ·å¼
            â”œâ”€â”€ ğŸ“„ type.ts        # ğŸ“‹ TypeScriptç±»å‹å®šä¹‰
            â”œâ”€â”€ ğŸ“„ vite-env.d.ts  # ğŸ”§ Viteç¯å¢ƒç±»å‹å£°æ˜
            â”‚
            â”œâ”€â”€ ğŸ“ components/    # ğŸ§© å¯å¤ç”¨ç»„ä»¶åº“
            â”‚   â”œâ”€â”€ ğŸ“ agentCard/      # ğŸ¤– Agentå¡ç‰‡ç»„ä»¶
            â”‚   â”œâ”€â”€ ğŸ“ commonCard/     # ğŸƒ é€šç”¨å¡ç‰‡ç»„ä»¶
            â”‚   â”œâ”€â”€ ğŸ“ dialog/         # ğŸ’¬ å¯¹è¯æ¡†ç»„ä»¶
            â”‚   â”œâ”€â”€ ğŸ“ drawer/         # ğŸ“œ æŠ½å±‰ç»„ä»¶
            â”‚   â””â”€â”€ ğŸ“ historyCard/    # ğŸ“œ å†å²è®°å½•å¡ç‰‡
            â”‚
            â”œâ”€â”€ ğŸ“ pages/         # ğŸ“„ é¡µé¢ç»„ä»¶
            â”‚   â”œâ”€â”€ ğŸ“„ index.vue       # ğŸ  é¦–é¡µ
            â”‚   â”œâ”€â”€ ğŸ“ agent/          # ğŸ¤– Agentç®¡ç†é¡µé¢
            â”‚   â”œâ”€â”€ ğŸ“ configuration/ # âš™ï¸ é…ç½®é¡µé¢
            â”‚   â”œâ”€â”€ ğŸ“ construct/      # ğŸ—ï¸ æ„å»ºé¡µé¢
            â”‚   â”œâ”€â”€ ğŸ“ conversation/   # ğŸ’¬ å¯¹è¯é¡µé¢
            â”‚   â”œâ”€â”€ ğŸ“ homepage/       # ğŸ  ä¸»é¡µæ¨¡å—
            â”‚   â”œâ”€â”€ ğŸ“ knowledge/      # ğŸ“š çŸ¥è¯†åº“é¡µé¢
            â”‚   â”œâ”€â”€ ğŸ“ login/          # ğŸ” ç™»å½•é¡µé¢
            â”‚   â”œâ”€â”€ ğŸ“ mars/           # ğŸš€ Marså¯¹è¯é¡µé¢
            â”‚   â”œâ”€â”€ ğŸ“ mcp-server/     # ğŸ–¥ï¸ MCPæœåŠ¡å™¨é¡µé¢
            â”‚   â”œâ”€â”€ ğŸ“ model/          # ğŸ§  æ¨¡å‹ç®¡ç†é¡µé¢
            â”‚   â”œâ”€â”€ ğŸ“ notFound/       # â“ 404é¡µé¢
            â”‚   â”œâ”€â”€ ğŸ“ profile/        # ğŸ‘¤ ç”¨æˆ·èµ„æ–™é¡µé¢
            â”‚   â””â”€â”€ ğŸ“ tool/           # ğŸ› ï¸ å·¥å…·ç®¡ç†é¡µé¢
            â”‚
            â”œâ”€â”€ ğŸ“ router/        # ğŸ›£ï¸ è·¯ç”±é…ç½®
            â”œâ”€â”€ ğŸ“ store/         # ğŸ—„ï¸ çŠ¶æ€ç®¡ç†(Pinia)
            â”œâ”€â”€ ğŸ“ apis/          # ğŸŒ APIæ¥å£å®šä¹‰
            â”œâ”€â”€ ğŸ“ utils/         # ğŸ§° å·¥å…·å‡½æ•°åº“
            â””â”€â”€ ğŸ“ assets/        # ğŸ–¼ï¸ é™æ€èµ„æº(å›¾ç‰‡ã€å­—ä½“ç­‰)
```

</details>

### ğŸ“Š é¡¹ç›®ç»Ÿè®¡

<div align=""center"">

| ğŸ“‚ **ç±»åˆ«** | ğŸ“ˆ **æ•°é‡** | ğŸ“ **è¯´æ˜** |
|:---:|:---:|:---|
| **åç«¯æ¨¡å—** | 15+ | APIã€æœåŠ¡ã€å·¥å…·ã€æ•°æ®åº“ç­‰æ ¸å¿ƒæ¨¡å— |
| **å‰ç«¯é¡µé¢** | 12+ | å®Œæ•´çš„ç”¨æˆ·ç•Œé¢å’Œäº¤äº’é¡µé¢ |
| **å†…ç½®å·¥å…·** | 10+ | æ¶µç›–æœç´¢ã€æ–‡æ¡£ã€å›¾åƒã€é€šä¿¡ç­‰åŠŸèƒ½ |
| **AIæ¨¡å‹** | 5+ | æ”¯æŒä¸»æµå¤§è¯­è¨€æ¨¡å‹å’ŒåµŒå…¥æ¨¡å‹ |
| **MCPæœåŠ¡** | å¤šä¸ª | å¯æ‰©å±•çš„MCPåè®®æœåŠ¡å™¨ |

</div>

### ğŸ“Š ä»£ç é‡ç»Ÿè®¡

<div align=""center"">

*ğŸ“ åŸºäºæ–‡ä»¶æ‰©å±•åçš„è¯¦ç»†ä»£ç ç»Ÿè®¡*

| ğŸ” **æ–‡ä»¶ç±»å‹** | ğŸ“ **æ–‡ä»¶æ•°é‡** | ğŸ“„ **æ€»è¡Œæ•°** | ğŸ“‰ **æœ€å°‘è¡Œæ•°** | ğŸ“ˆ **æœ€å¤šè¡Œæ•°** | ğŸ“Š **å¹³å‡è¡Œæ•°** |
|:---:|:---:|:---:|:---:|:---:|:---:|
| **ğŸ Python** | 247 | 19,599 | 0 | 1,039 | 79 |
| **ğŸ¨ Vue** | 31 | 21,907 | 12 | 2,588 | 706 |
| **ğŸ“° Markdown** | 8 | 3,475 | 5 | 1,079 | 434 |
| **âš¡ TypeScript** | 46 | 2,103 | 1 | 212 | 45 |
| **ğŸ“‹ TXT** | 1 | 539 | 539 | 539 | 539 |
| **ğŸ“¦ JSON** | 11 | 348 | 7 | 110 | 31 |
| **âš™ï¸ TOML** | 1 | 328 | 328 | 328 | 328 |
| **ğŸ¨ CSS** | 1 | 176 | 176 | 176 | 176 |
| **ğŸ”§ YML** | 2 | 177 | 52 | 125 | 88 |
| **ğŸ“‹ YAML** | 2 | 152 | 35 | 117 | 76 |
| **âš™ï¸ CONF** | 1 | 101 | 101 | 101 | 101 |
| **ğŸš€ Shell** | 2 | 87 | 35 | 52 | 43 |
| **ğŸš¦ PROD** | 1 | 41 | 41 | 41 | 41 |
| **ğŸš« GitIgnore** | 1 | 24 | 24 | 24 | 24 |
| **ğŸŒ HTML** | 1 | 13 | 13 | 13 | 13 |
| **ğŸ³ DockerIgnore** | 1 | 10 | 10 | 10 | 10 |

**ğŸ“Š æ€»è®¡**: **356** ä¸ªæ–‡ä»¶ï¼Œ**48,560** è¡Œä»£ç 

</div>

### ğŸ† æŠ€æœ¯æ ˆå æ¯”

<div align=""center"">

| ğŸ¯ **æŠ€æœ¯æ ˆ** | ğŸ“ˆ **å æ¯”** | ğŸ”¥ **ç‰¹ç‚¹** |
|:---:|:---:|:---|
| **ğŸ¨ å‰ç«¯ (Vue+TS)** | 45.1% | ç°ä»£åŒ–å“åº”å¼ç•Œé¢ï¼ŒTypeScriptå¼ºç±»å‹æ”¯æŒ |
| **ğŸ åç«¯ (Python)** | 40.4% | é«˜æ€§èƒ½å¼‚æ­¥æœåŠ¡ï¼Œä¸°å¯Œçš„AIé›†æˆ |
| **ğŸ“š æ–‡æ¡£ (MD)** | 7.2% | å®Œæ•´çš„é¡¹ç›®æ–‡æ¡£å’ŒAPIè¯´æ˜ |
| **âš™ï¸ é…ç½® (JSON/YAML)** | 7.3% | çµæ´»çš„é…ç½®ç®¡ç†å’Œéƒ¨ç½²æ”¯æŒ |

*ğŸ’¡ é¡¹ç›®é‡‡ç”¨å‰åç«¯åˆ†ç¦»æ¶æ„ï¼Œä»£ç ç»“æ„æ¸…æ™°ï¼Œæ–‡æ¡£å®Œå–„*

</div>


---

## ğŸš€ å¿«é€Ÿå¼€å§‹

> ğŸ¯ **ä¸‰ç§éƒ¨ç½²æ–¹å¼ä»»ä½ é€‰æ‹©** - Dockerä¸€é”®éƒ¨ç½² | æœ¬åœ°å¼€å‘ | ç”Ÿäº§ç¯å¢ƒ

<div align=""center"">

### ğŸ“‹ ç³»ç»Ÿè¦æ±‚

| ğŸ› ï¸ **ç»„ä»¶** | ğŸ”¢ **ç‰ˆæœ¬è¦æ±‚** | ğŸ“ **è¯´æ˜** |
|:---:|:---:|:---|
| **Python** | 3.12+ | åç«¯è¿è¡Œç¯å¢ƒ |
| **Node.js** | 18+ | å‰ç«¯æ„å»ºç¯å¢ƒ |
| **MySQL** | 8.0+ | ä¸»æ•°æ®åº“ |
| **Redis** | 7.0+ | ç¼“å­˜å’Œä¼šè¯å­˜å‚¨ |
| **Docker** | 20.10+ | å®¹å™¨åŒ–éƒ¨ç½²ï¼ˆæ¨èï¼‰ |

</div>

### ğŸ‰ **æ–¹å¼ä¸€ï¼šDockerä¸€é”®éƒ¨ç½²ï¼ˆæ¨èï¼‰**

<details>
<summary><b>ğŸ’« ç‚¹å‡»å±•å¼€Dockeréƒ¨ç½²æ­¥éª¤</b></summary>

#### ğŸ”¥ **è¶…ç®€å•ä¸‰æ­¥éƒ¨ç½²**

```bash
# 1ï¸âƒ£ å…‹éš†é¡¹ç›®
git clone https://github.com/Shy2593666979/AgentChat.git
cd AgentChat

# 2ï¸âƒ£ é…ç½®APIå¯†é’¥
cp src/backend/agentchat/config.yaml.example src/backend/agentchat/config.yaml
# ç¼–è¾‘é…ç½®æ–‡ä»¶ï¼Œå¡«å…¥ä½ çš„APIå¯†é’¥

# 3ï¸âƒ£ ä¸€é”®å¯åŠ¨
cd docker
docker-compose up --build -d
```

#### âœ… **éªŒè¯éƒ¨ç½²**
```bash
# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
docker-compose ps

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f app
```

ğŸŠ **å®Œæˆï¼** è®¿é—® [http://localhost:8090](http://localhost:8090) å¼€å§‹ä½¿ç”¨ï¼

</details>

### ğŸ› ï¸ **æ–¹å¼äºŒï¼šæœ¬åœ°å¼€å‘ç¯å¢ƒ**

<details>
<summary><b>ğŸ‘¨â€ğŸ’» ç‚¹å‡»å±•å¼€æœ¬åœ°å¼€å‘æ­¥éª¤</b></summary>

#### ğŸ”§ **åç«¯ç¯å¢ƒæ­å»º**

```bash
# 1ï¸âƒ£ å…‹éš†é¡¹ç›®
git clone https://github.com/Shy2593666979/AgentChat.git
cd AgentChat

# ä½¿ç”¨pipå®‰è£…ä¾èµ–
pip install -r requirements.txt
```


#### âš™ï¸ **é…ç½®æ–‡ä»¶è®¾ç½®**

åˆ›å»ºå¹¶ç¼–è¾‘é…ç½®æ–‡ä»¶ `src/backend/agentchat/config.yaml`:

#### ğŸš€ **å¯åŠ¨æœåŠ¡**

```bash
# åç«¯æœåŠ¡
cd src/backend
uvicorn agentchat.main:app --port 7860 --host 0.0.0.0

# æ–°ç»ˆç«¯ - å‰ç«¯æœåŠ¡
cd src/frontend
npm install
npm run dev
```

#### ğŸŒ **è®¿é—®åœ°å€**

| ğŸ¯ **æœåŠ¡** | ğŸ”— **åœ°å€** | ğŸ“ **è¯´æ˜** |
|:---:|:---:|:---|
| **å‰ç«¯ç•Œé¢** | [localhost:8090](http://localhost:8090) | ç”¨æˆ·ç•Œé¢ |
| **åç«¯API** | [localhost:7860](http://localhost:7860) | APIæœåŠ¡ |
| **APIæ–‡æ¡£** | [localhost:7860/docs](http://localhost:7860/docs) | Swaggeræ–‡æ¡£ |

</details>


---

## ğŸ“¦ é«˜çº§éƒ¨ç½²æŒ‡å—

> ğŸ¯ **çµæ´»çš„éƒ¨ç½²é€‰æ‹©** - ä»å¼€å‘æµ‹è¯•åˆ°ç”Ÿäº§ç¯å¢ƒçš„å®Œæ•´æ–¹æ¡ˆ

### ğŸŒ **éƒ¨ç½²æ¶æ„é€‰æ‹©**

<table>
<tr>
  
<td width=""33%"">


---

## ğŸ“– æ–‡æ¡£

### ğŸ“š APIæ–‡æ¡£
- [AgentChat Document](docs/agentchat.md) - agentchatå…·ä½“æ–‡æ¡£
- [API Documentation v3.0](docs/api.md) - æœ€æ–°APIæ–‡æ¡£

### ğŸ”§ å¼€å‘æ–‡æ¡£
- **åœ¨çº¿APIæ–‡æ¡£**: å¯åŠ¨åç«¯æœåŠ¡åè®¿é—® `/docs`
- **å‰ç«¯è°ƒè¯•æŒ‡å—**: [src/frontend/DEBUGGING_GUIDE.md](src/frontend/DEBUGGING_GUIDE.md)

### ğŸ“‹ é…ç½®æŒ‡å—

#### å‘é‡æ•°æ®åº“é…ç½®
- **Milvus**: [å®‰è£…æŒ‡å—](https://milvus.io/docs/zh/install_standalone-windows.md)
- **ChromaDB**: é¡¹ç›®ä¸­å·²é›†æˆï¼Œæ— éœ€é¢å¤–é…ç½®

#### æ¨¡å‹æœåŠ¡é…ç½®
- **Rerankæ¨¡å‹**: [é˜¿é‡Œäº‘æ¨¡å‹æœåŠ¡](https://help.aliyun.com/zh/model-studio/text-rerank-api)
- **Embeddingæ¨¡å‹**: [OpenAIå…¼å®¹æ¥å£](https://help.aliyun.com/zh/model-studio/embedding-interfaces-compatible-with-openai)

#### æœç´¢å¼•æ“é…ç½®
- **Elasticsearch**: [IKåˆ†è¯å™¨](https://release.infinilabs.com/analysis-ik/stable/)

---

## ğŸ”§ å¼€å‘æŒ‡å—

### âš ï¸ é‡è¦æç¤º (ç›®å‰å·²å•ç‹¬æ”¾åˆ°ä¸agentchatå¹¶åˆ—ï¼Œä¸éœ€è¦å†é¢å¤–å¤„ç†)

ç”±äº `fastapi-jwt-auth` åº“ä½¿ç”¨è¾ƒæ—§ç‰ˆæœ¬çš„ Pydanticï¼Œè€Œé¡¹ç›®ä¸­çš„ LangChainã€MCP ç­‰ç»„ä»¶éœ€è¦ Pydantic >= 2ï¼Œéœ€è¦æ‰‹åŠ¨ä¿®æ”¹åº“æ–‡ä»¶ï¼š

æ‰¾åˆ°ä½ çš„è™šæ‹Ÿç¯å¢ƒä¸­çš„æ–‡ä»¶ï¼š
```
/path/to/your/env/lib/python3.12/site-packages/fastapi_jwt_auth/config.py
```

æ›¿æ¢ä¸ºä»¥ä¸‹å†…å®¹ï¼š

<details>
<summary>ç‚¹å‡»å±•å¼€é…ç½®ä»£ç </summary>

```python
from datetime import timedelta
from typing import Optional, Union, Sequence, List
from pydantic import (
    BaseModel,
    validator,
    StrictBool,
    StrictInt,
    StrictStr
)

class LoadConfig(BaseModel):
    authjwt_token_location: Optional[List[StrictStr]] = ['headers']
    authjwt_secret_key: Optional[StrictStr] = None
    authjwt_public_key: Optional[StrictStr] = None
    authjwt_private_key: Optional[StrictStr] = None
    authjwt_algorithm: Optional[StrictStr] = ""HS256""
    authjwt_decode_algorithms: Optional[List[StrictStr]] = None
    authjwt_decode_leeway: Optional[Union[StrictInt,timedelta]] = 0
    authjwt_encode_issuer: Optional[StrictStr] = None
    authjwt_decode_issuer: Optional[StrictStr] = None
    authjwt_decode_audience: Optional[Union[StrictStr,Sequence[StrictStr]]] = None
    authjwt_denylist_enabled: Optional[StrictBool] = False
    authjwt_denylist_token_checks: Optional[List[StrictStr]] = ['access','refresh']
    authjwt_header_name: Optional[StrictStr] = ""Authorization""
    authjwt_header_type: Optional[StrictStr] = ""Bearer""
    authjwt_access_token_expires: Optional[Union[StrictBool,StrictInt,timedelta]] = timedelta(minutes=15)
    authjwt_refresh_token_expires: Optional[Union[StrictBool,StrictInt,timedelta]] = timedelta(days=30)
    # # option for create cookies
    authjwt_access_cookie_key: Optional[StrictStr] = ""access_token_cookie""
    authjwt_refresh_cookie_key: Optional[StrictStr] = ""refresh_token_cookie""
    authjwt_access_cookie_path: Optional[StrictStr] = ""/""
    authjwt_refresh_cookie_path: Optional[StrictStr] = ""/""
    authjwt_cookie_max_age: Optional[StrictInt] = None
    authjwt_cookie_domain: Optional[StrictStr] = None
    authjwt_cookie_secure: Optional[StrictBool] = False
    authjwt_cookie_samesite: Optional[StrictStr] = None
    # # option for double submit csrf protection
    authjwt_cookie_csrf_protect: Optional[StrictBool] = True
    authjwt_access_csrf_cookie_key: Optional[StrictStr] = ""csrf_access_token""
    authjwt_refresh_csrf_cookie_key: Optional[StrictStr] = ""csrf_refresh_token""
    authjwt_access_csrf_cookie_path: Optional[StrictStr] = ""/""
    authjwt_refresh_csrf_cookie_path: Optional[StrictStr] = ""/""
    authjwt_access_csrf_header_name: Optional[StrictStr] = ""X-CSRF-Token""
    authjwt_refresh_csrf_header_name: Optional[StrictStr] = ""X-CSRF-Token""
    authjwt_csrf_methods: Optional[List[StrictStr]] = ['POST','PUT','PATCH','DELETE']

    @validator('authjwt_access_token_expires')
    def validate_access_token_expires(cls, v):
        if v is True:
            raise ValueError(""The 'authjwt_access_token_expires' only accept value False (bool)"")
        return v

    @validator('authjwt_refresh_token_expires')
    def validate_refresh_token_expires(cls, v):
        if v is True:
            raise ValueError(""The 'authjwt_refresh_token_expires' only accept value False (bool)"")
        return v

    @validator('authjwt_denylist_token_checks', each_item=True)
    def validate_denylist_token_checks(cls, v):
        if v not in ['access','refresh']:
            raise ValueError(""The 'authjwt_denylist_token_checks' must be between 'access' or 'refresh'"")
        return v

    @validator('authjwt_token_location', each_item=True)
    def validate_token_location(cls, v):
        if v not in ['headers','cookies']:
            raise ValueError(""The 'authjwt_token_location' must be between 'headers' or 'cookies'"")
        return v

    @validator('authjwt_cookie_samesite')
    def validate_cookie_samesite(cls, v):
        if v not in ['strict','lax','none']:
            raise ValueError(""The 'authjwt_cookie_samesite' must be between 'strict', 'lax', 'none'"")
        return v

    @validator('authjwt_csrf_methods', each_item=True)
    def validate_csrf_methods(cls, v):
        if v.upper() not in [""GET"", ""HEAD"", ""POST"", ""PUT"", ""DELETE"", ""PATCH""]:
            raise ValueError(""The 'authjwt_csrf_methods' must be between http request methods"")
        return v.upper()

    class Config:
        str_min_length = 1
        str_strip_whitespace = True
```

</details>

å…¶å®æ‰¾èµ·æ¥æŒºéº»çƒ¦çš„ï¼Œæ‰€ä»¥æä¾›äº†ä¸€ä¸ªç›´æ¥ä¿®æ”¹æºä»£ç çš„è„šæœ¬
```python
python scripts/fix_fastapi_jwt_auth.py # è¿›è¡Œè„šæœ¬ä¿®å¤(å‰ææ˜¯éœ€è¦å°†ä¾èµ–åŒ…å®‰è£…å®Œæ•´)
```


## ğŸ¤ è´¡çŒ®æŒ‡å—

> ğŸ’ª **å…±å»ºAIæœªæ¥** - æ¯ä¸€ä¸ªè´¡çŒ®éƒ½è®©AgentChatå˜å¾—æ›´å¥½

<div align=""center"">

### ğŸŒŸ **æˆ‘ä»¬æ¬¢è¿æ‰€æœ‰å½¢å¼çš„è´¡çŒ®ï¼**

</div>

<table>
<tr>
<td width=""25%"">

#### ğŸ› **Bugä¿®å¤**
*å‘ç°é—®é¢˜ï¼Œè§£å†³é—®é¢˜*

1. ğŸ” æœç´¢å·²æœ‰Issues
2. ğŸ“ åˆ›å»ºè¯¦ç»†BugæŠ¥å‘Š
3. ğŸ§ª æä¾›å¤ç°æ­¥éª¤
4. ğŸ’¡ æäº¤ä¿®å¤æ–¹æ¡ˆ

</td>
<td width=""25%"">

#### âœ¨ **åŠŸèƒ½å¼€å‘**
*æ–°æƒ³æ³•ï¼Œæ–°åŠŸèƒ½*

1. ğŸ’­ åˆ›å»ºFeature Request
2. ğŸ“‹ è¯¦ç»†æè¿°éœ€æ±‚åœºæ™¯
3. ğŸ¨ è®¾è®¡å®ç°æ–¹æ¡ˆ
4. ğŸš€ å¼€å‘å¹¶æµ‹è¯•

</td>
<td width=""25%"">

#### ğŸ“š **æ–‡æ¡£å®Œå–„**
*çŸ¥è¯†å…±äº«ï¼ŒåŠ©åŠ›ä»–äºº*

1. ğŸ“– è¡¥å……APIæ–‡æ¡£
2. âœï¸ ç¼–å†™ä½¿ç”¨æ•™ç¨‹
3. ğŸŒ å¤šè¯­è¨€ç¿»è¯‘
4. ğŸ¥ åˆ¶ä½œè§†é¢‘æ•™ç¨‹

</td>
<td width=""25%"">

#### ğŸ§ª **ç¤¾åŒºæ”¯æŒ**
*å¸®åŠ©ä»–äººï¼Œåˆ†äº«ç»éªŒ*

1. â“ å›ç­”ç¤¾åŒºé—®é¢˜
2. ğŸ’¬ å‚ä¸æŠ€æœ¯è®¨è®º
3. ğŸ¤ åˆ†äº«ä½¿ç”¨å¿ƒå¾—
4. ğŸ¤ æ¨å¹¿é¡¹ç›®

</td>
</tr>
</table>





## ğŸ“„ **è®¸å¯è¯**

<div align=""center"">

æœ¬é¡¹ç›®é‡‡ç”¨ **[MIT License](LICENSE)** å¼€æºè®¸å¯è¯

*è¿™æ„å‘³ç€ä½ å¯ä»¥è‡ªç”±ä½¿ç”¨ã€ä¿®æ”¹å’Œåˆ†å‘æœ¬é¡¹ç›® ğŸ‰*

</div>

---

<div align=""center"">

## ğŸŒŸ **æ„Ÿè°¢æ”¯æŒ AgentChatï¼**

### å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ª â­ï¸

*è®©æ›´å¤šçš„äººå‘ç°è¿™ä¸ªé¡¹ç›®ï¼Œä¸€èµ·æ„å»ºAIçš„æœªæ¥ï¼*

*Made with â¤ by the AgentChat Author MingGuang Tian*


<picture>
  <source
    media=""(prefers-color-scheme: dark)""
    srcset=""
      https://api.star-history.com/svg?repos=Shy2593666979/AgentChat&type=Date&theme=dark
    ""
  />
  <source
    media=""(prefers-color-scheme: light)""
    srcset=""
      https://api.star-history.com/svg?repos=Shy2593666979/AgentChat&type=Date
    ""
  />
  <img
    alt=""Star History Chart""
    src=""https://api.star-history.com/svg?repos=Shy2593666979/AgentChat&type=Date""
  />
</picture>

</div>
",94
https://mcp.so/server/AgentForgeMCP/DataBassGit,https://github.com/DataBassGit/AgentForgeMCP,AgentForgeMCP,"A template MCP server to easily deploy tools, resources, and prompts for agents. From the AgentForge team",English,developer-tools,agentforge; mcp; deployment,"what is AgentForgeMCP? AgentForgeMCP is a template server designed to facilitate the deployment of tools, resources, and prompts for agents, created by the AgentForge team. how to use AgentForgeMCP? To use AgentForgeMCP, clone the repository from GitHub, configure the server settings, and deploy the necessary tools and resources for your agents. key features of AgentForgeMCP? Easy deployment of agent tools and resources Customizable server settings Support for various agent prompts use cases of AgentForgeMCP? Setting up a development environment for AI agents. Deploying multiple tools for agent-based applications. Streamlining the process of managing agent resources. FAQ from AgentForgeMCP? What is the purpose of AgentForgeMCP? AgentForgeMCP serves as a template to simplify the deployment of agent-related tools and resources. Is AgentForgeMCP free to use? Yes! AgentForgeMCP is available under the MIT license, making it free to use and modify. Where can I find the documentation? Documentation can be found in the GitHub repository at https://github.com/DataBassGit/AgentForgeMCP .","# AgentForgeMCP
",0
https://mcp.so/server/AgentNull/jaschadub,https://github.com/jaschadub/AgentNull,ğŸ§  AgentNull: AI System Security Threat Catalog + Proof-of-Concepts,"AgentNull: AI System Security Threat Catalog + Proof-of-Concepts. Collection of PoCs for using Agents, MCP, and RAG in bad ways.",English,research-and-data,agent; research; ai; proof-of-concept; mcp; poc,"What is AgentNull? AgentNull is a repository that catalogs attack vectors targeting autonomous AI agents, providing proof-of-concepts (PoCs) for each vector, aimed at enhancing security research and threat modeling. How to use AgentNull? To use AgentNull, navigate into each pocs/<attack_name>/ folder and follow the README instructions to replicate the attack scenarios. Key features of AgentNull? Comprehensive threat catalog for AI agents Individual proof-of-concepts for various attack vectors Structured data for SOC/SIEM ingestion Use cases of AgentNull? Red team exercises to test the security of AI systems. Educational purposes for understanding AI vulnerabilities. Internal security research to develop better defenses against AI threats. FAQ from AgentNull? Is AgentNull suitable for production use? No, AgentNull is intended for educational and internal security research purposes only. Can I use the techniques in AgentNull against systems I do not own? No, you must have explicit authorization to test any systems with the techniques provided.","# ğŸ§  AgentNull: AI System Security Threat Catalog + Proof-of-Concepts

This repository contains a red team-oriented catalog of attack vectors targeting AI systems including autonomous agents (MCP, LangGraph, AutoGPT), RAG pipelines, vector databases, and embedding-based retrieval systems, along with individual proof-of-concepts (PoCs) for each.

## ğŸ“˜ Structure

- `catalog/AgentNull_Catalog.md` â€” Human-readable threat catalog
- `catalog/AgentNull_Catalog.json` â€” Structured version for SOC/SIEM ingestion
- `pocs/` â€” One directory per attack vector, each with its own README, code, and sample input/output

## âš ï¸ Disclaimer

This repository is for **educational and internal security research** purposes only. Do not deploy any techniques or code herein in production or against systems you do not own or have explicit authorization to test.

## ğŸ”§ Usage

Navigate into each `pocs/<attack_name>/` folder and follow the README to replicate the attack scenario.

### ğŸ¤– Testing with Local LLMs (Recommended)

For enhanced PoC demonstrations without API costs, use Ollama with local models:

#### Install Ollama
```bash
# Linux/macOS
curl -fsSL https://ollama.ai/install.sh | sh

# Or download from https://ollama.ai/download
```

#### Setup Local Model
```bash
# Pull a lightweight model (recommended for testing)
ollama pull gemma3

# Or use a more capable model
ollama pull deepseek-r1
ollama pull qwen3
```

#### Run PoCs with Local LLM
```bash
# Advanced Tool Poisoning with real LLM
cd pocs/AdvancedToolPoisoning
python3 advanced_tool_poisoning_agent.py local

# Other PoCs work with simulation mode
cd pocs/ContextPackingAttacks
python3 context_packing_agent.py
```

#### Ollama Configuration
- **Default endpoint**: `http://localhost:11434`
- **Model selection**: Edit the model name in PoC files if needed
- **Performance**: Llama2 (~4GB RAM), Mistral (~4GB RAM), CodeLlama (~4GB RAM)

## ğŸ§© Attack Vectors Covered

### ğŸ¤– MCP & Agent Systems
- **â­ [Full-Schema Poisoning (FSP)](pocs/FullSchemaPoisoning/)** - Exploit any field in tool schema beyond descriptions
- **â­ [Advanced Tool Poisoning Attack (ATPA)](pocs/AdvancedToolPoisoning/)** - Manipulate tool outputs to trigger secondary actions
- **â­ [MCP Rug Pull Attack](pocs/MCPRugPull/)** - Swap benign descriptions for malicious ones after approval
- **â­ [Schema Validation Bypass](pocs/SchemaValidationBypass/)** - Exploit client validation implementation differences
- **[Tool Confusion Attack](pocs/ToolConfusionAttack/)** - Trick agents into using wrong tools via naming similarity
- **[Nested Function Call Hijack](pocs/NestedFunctionHijack/)** - Use JSON-like data to trigger dangerous function calls
- **[Subprompt Extraction](pocs/SubpromptExtraction/)** - Induce agents to reveal system instructions or tools
- **[Backdoor Planning](pocs/BackdoorPlanning/)** - Inject future intent into multi-step planning for exfiltration

### ğŸ§  Memory & Context Systems
- **[Recursive Leakage](pocs/RecursiveLeakage/)** - Secrets leak through context summarization
- **[Token Gaslighting](pocs/TokenGaslighting/)** - Push safety instructions out of context via token spam
- **[Heuristic Drift Injection](pocs/HeuristicDriftInjection/)** - Poison agent logic with repeated insecure patterns
- **â­ [Context Packing Attacks](pocs/ContextPackingAttacks/)** - Overflow context windows to truncate safety instructions

### ğŸ” RAG & Vector Systems
- **â­ [Cross-Embedding Poisoning](pocs/CrossEmbeddingPoisoning/)** - Manipulate embeddings to increase malicious content retrieval
- **â­ [Index Skew Attacks](pocs/IndexSkewAttacks/)** - Bias vector indices to favor malicious content *(theoretical)*
- **â­ [Zero-Shot Vector Beaconing](pocs/ZeroShotVectorBeaconing/)** - Embed latent activation patterns for covert signaling *(theoretical)*
- **â­ [Embedding Feedback Loops](pocs/EmbeddingFeedbackLoops/)** - Poison continual learning systems *(theoretical)*

### ğŸ’» Code & File Systems
- **[Hidden File Exploitation](pocs/HiddenFileExploitation/)** - Get agents to modify `.env`, `.git`, or internal config files

### âš¡ Resource & Performance
- **[Function Flooding](pocs/FunctionFlooding/)** - Generate recursive tool calls to overwhelm budgets/APIs
- **[Semantic DoS](pocs/SemanticDoS/)** - Trigger infinite generation or open-ended tasks

## ğŸ“š Related Research & Attribution

### Novel Attack Vectors (â­)
The attack vectors marked with â­ represent novel concepts primarily developed within the AgentNull project, extending beyond existing documented attack patterns.

### Known Attack Patterns with Research Links
- **Recursive Leakage**: [Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/abs/2307.03172)
- **Heuristic Drift Injection**: [Poisoning Web-Scale Training Data is Practical](https://arxiv.org/abs/2302.10149)
- **Tool Confusion Attack**: [LLM-as-a-judge](https://arxiv.org/abs/2306.05685)
- **Token Gaslighting**: [RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture](https://arxiv.org/abs/2401.08406)
- **Function Flooding**: [Denial-of-Service Attack on Test-Time-Tuning Models](https://arxiv.org/abs/2405.02324)
- **Hidden File Exploitation**: [OWASP Top 10 for Large Language Model Applications](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
- **Backdoor Planning**: [Backdoor Attacks on Language Models](https://arxiv.org/abs/2311.09403)
- **Nested Function Call Hijack**: [OWASP Top 10 for Large Language Model Applications](https://owasp.org/www-project-top-10-for-large-language-model-applications/)

### Sponsored by [ThirdKey](https://thirdkey.ai)
",3
https://mcp.so/server/AgentWong_iac-memory-mcp-server-project/MCP-Mirror,https://github.com/MCP-Mirror/AgentWong_iac-memory-mcp-server-project,IaC Memory MCP Server,Mirror of,English,research-and-data,iac; memory; mcp; server; terraform; ansible,"What is IaC Memory MCP Server? The IaC Memory MCP Server is a Model Context Protocol (MCP) server designed to enhance Claude AI's capabilities by providing persistent memory storage for Infrastructure-as-Code (IaC) components, focusing on version tracking and relationship mapping for Terraform and Ansible resources. How to use IaC Memory MCP Server? To use the IaC Memory MCP Server, set up the server with the required environment variables, and utilize the provided prompts and tools for managing IaC components. You can integrate it with Claude Desktop for enhanced functionality. Key features of IaC Memory MCP Server? Persistent storage and version tracking for IaC components Hierarchical resource organization with URI-based access Comprehensive relationship mapping between components Version-specific documentation management Automated relationship analysis and insights Use cases of IaC Memory MCP Server? Managing Terraform and Ansible resources with version control. Analyzing relationships between different IaC components. Providing a structured approach to resource management in cloud environments. FAQ from IaC Memory MCP Server? Can the IaC Memory MCP Server manage all types of IaC components? Yes! It supports Terraform and Ansible resources. Is there a cost associated with using the IaC Memory MCP Server? The project was initially personal and is not intended for further development, but it is available for use. How can I contribute to the IaC Memory MCP Server? Contributions can be made through the GitHub repository.","# IaC Memory MCP Server

A Model Context Protocol (MCP) server that enhances Claude AI's capabilities by providing persistent memory storage for Infrastructure-as-Code (IaC) components, with a focus on version tracking and relationship mapping for Terraform and Ansible resources.

> [!NOTE]  
> This was a personal project to determine the state of AI's ability if the person using it (me)
> doesn't have subject matter expertise (lack of Python knowledge).  Since it has become rather cost
> prohibitive, I do not intend to develop or maintain this project further.

## Overview

The IaC Memory MCP Server addresses the challenge of maintaining accurate, version-aware context for IaC components by providing:

- Persistent storage and version tracking for IaC components
- Hierarchical resource organization with URI-based access
- Comprehensive relationship mapping between components
- Version-specific documentation management
- Schema validation and temporal metadata tracking
- Automated relationship analysis and insights

## Core Components

### Resource Management

The server implements a sophisticated resource management system with hierarchical URIs:

#### Resource URI Structure
```
resources://<platform>/<category>/<name>
```

Supported platforms:
- terraform
- ansible
- iac (for general infrastructure entities)

Example URIs:
```
resources://terraform/providers/aws
resources://terraform/resources/aws/s3_bucket
resources://ansible/collections/community.aws
resources://ansible/modules/community.aws/s3_bucket
```

#### Resource Templates
The server provides dynamic resource templates for standardized access patterns:
- Terraform provider information: `resources://terraform/providers/{provider_name}`
- Resource type details: `resources://terraform/resources/{provider_name}/{resource_type}`
- Ansible collection data: `resources://ansible/collections/{collection_name}`
- Module information: `resources://ansible/modules/{collection_name}/{module_name}`

### Prompts

The server implements four specialized prompts for IaC component discovery and analysis:

#### search_resources
- Purpose: Search for IaC resources
- Arguments:
  - `provider`: Provider name
  - `resource_type`: Resource type
- Returns: Information about specific resources for the given provider

#### analyze_entity
- Purpose: Analyze an entity and its relationships
- Arguments:
  - `entity_id`: Entity ID
  - `include_relationships`: Include relationships
- Returns: Detailed entity analysis including name, type, and observations

#### terraform_provider
- Purpose: Get information about a Terraform provider
- Arguments:
  - `provider_name`: Name of the Terraform provider (required)
  - `version`: Specific version to query (optional)
- Returns: Detailed provider information for the specified version

#### ansible_module
- Purpose: Get information about an Ansible module
- Arguments:
  - `collection_name`: Name of the Ansible collection (required)
  - `module_name`: Name of the module (required)
  - `version`: Specific version to query (optional)
- Returns: Detailed module information for the specified version

### Tools

The server implements comprehensive tooling for IaC component management:

#### Terraform Tools
- `get_terraform_provider_info`: Retrieve detailed provider information including version and resources
- `list_provider_resources`: List all resources available for a specific provider
- `get_terraform_resource_info`: Get detailed information about a specific resource type
- `add_terraform_provider`: Register new providers with versioning
- `add_terraform_resource`: Add resource definitions with schemas
- `update_provider_version`: Update provider versions with new documentation

#### Ansible Tools
- `get_ansible_collection_info`: Get detailed information about an Ansible collection
- `list_ansible_collections`: List all available Ansible collections
- `get_collection_version_history`: View version history of a collection
- `get_ansible_module_info`: Get detailed information about a specific module
- `list_collection_modules`: List all modules in a collection
- `get_module_version_compatibility`: Check version compatibility of modules
- `add_ansible_collection`: Register new Ansible collections
- `add_ansible_module`: Add new modules with validation and documentation

#### Entity Operations
- `create_entity`: Create new infrastructure entities
- `update_entity`: Modify existing entity configurations
- `delete_entity`: Remove entities with relationship cleanup
- `view_relationships`: Analyze entity dependencies and relationships

## Configuration

The server supports configuration through environment variables:

- `DATABASE_URL`: SQLite database location
- `MCP_DEBUG`: Enable debug logging when set
- `MCP_TEST_MODE`: Enable test mode for database resets

For development, create a `.env` file:
```bash
DATABASE_URL=sqlite:////path/to/db.sqlite
MCP_DEBUG=1
MCP_TEST_MODE=1
```

## Integration with Claude Desktop

### Development Setup
```json
""mcpServers"": {
  ""iac-memory"": {
    ""command"": ""uv"",
    ""args"": [
      ""--directory"",
      ""/path/to/iac-memory-mcp-server"",
      ""run"",
      ""iac-memory-mcp-server""
    ]
    ""env"": {
          ""DATABASE_URL"": ""sqlite:////home/herman/iac.db""
      }
  }
}
```

### Production Setup
```json
""mcpServers"": {
  ""iac-memory"": {
    ""command"": ""uvx"",
    ""args"": [
        ""--from"",
        ""git+https://github.com/AgentWong/iac-memory-mcp-server.git"",
        ""python"",
        ""-m"",
        ""iac_memory_mcp_server""
    ],
    ""env"": {
          ""DATABASE_URL"": ""sqlite:////home/herman/iac.db""
      }
  }
}
```

## Development

### Local Development
```bash
# Install dependencies
uv sync

# Run tests
uv run pytest

# Development server with MCP Inspector
npx @modelcontextprotocol/inspector uv run iac-memory-mcp-server
```

## License

This project is licensed under the MIT License - see the LICENSE file for details.",0
https://mcp.so/server/AgentWong_optimized-memory-mcp-server/MCP-Mirror,https://github.com/MCP-Mirror/AgentWong_optimized-memory-mcp-server,optimized-memory-mcp-server,Mirror of,English,research-and-data,memory; knowledge-graph; AI-assistant,"What is optimized-memory-mcp-server? The optimized-memory-mcp-server is a project designed to test and demonstrate Claude AI's coding abilities, focusing on good AI workflows and prompt design. It implements a persistent memory system using a local knowledge graph, allowing Claude to remember user information across chats. How to use optimized-memory-mcp-server? To use the server, set it up with Docker or NPX, and configure it in your claude_desktop_config.json. You can then interact with the server to create entities, relations, and observations in the knowledge graph. Key features of optimized-memory-mcp-server? Persistent memory using a local knowledge graph Ability to create and manage entities and their relationships API for adding, deleting, and searching entities and observations Integration with Claude AI for personalized interactions Use cases of optimized-memory-mcp-server? Storing user preferences and behaviors for personalized AI interactions. Managing relationships between different entities in a knowledge graph. Enhancing AI's memory capabilities for better user experience. FAQ from optimized-memory-mcp-server? What programming language is used? The server is implemented in Python. Is there a license for this project? Yes, it is licensed under the MIT License, allowing free use, modification, and distribution. How can I build the server? You can build it using Docker with the provided Dockerfile.","# optimized-memory-mcp-server

This is to test and demonstrate Claude AI's coding abilities, as well as good AI workflows and prompt design.
This is a fork of a Python Memory MCP Server (I believe the official one is in Java) which uses SQLite for a backend.

# Knowledge Graph Memory Server
A basic implementation of persistent memory using a local knowledge graph. This lets Claude remember information about the user across chats.

## Core Concepts

### Entities
Entities are the primary nodes in the knowledge graph. Each entity has:
- A unique name (identifier)
- An entity type (e.g., ""person"", ""organization"", ""event"")
- A list of observations

Example:
```json
{
  ""name"": ""John_Smith"",
  ""entityType"": ""person"",
  ""observations"": [""Speaks fluent Spanish""]
}
```

### Relations
Relations define directed connections between entities. They are always stored in active voice and describe how entities interact or relate to each other.

Example:
```json
{
  ""from"": ""John_Smith"",
  ""to"": ""Anthropic"",
  ""relationType"": ""works_at""
}
```
### Observations
Observations are discrete pieces of information about an entity. They are:

- Stored as strings
- Attached to specific entities
- Can be added or removed independently
- Should be atomic (one fact per observation)

Example:
```json
{
  ""entityName"": ""John_Smith"",
  ""observations"": [
    ""Speaks fluent Spanish"",
    ""Graduated in 2019"",
    ""Prefers morning meetings""
  ]
}
```

## API

### Tools
- **create_entities**
  - Create multiple new entities in the knowledge graph
  - Input: `entities` (array of objects)
    - Each object contains:
      - `name` (string): Entity identifier
      - `entityType` (string): Type classification
      - `observations` (string[]): Associated observations
  - Ignores entities with existing names

- **create_relations**
  - Create multiple new relations between entities
  - Input: `relations` (array of objects)
    - Each object contains:
      - `from` (string): Source entity name
      - `to` (string): Target entity name
      - `relationType` (string): Relationship type in active voice
  - Skips duplicate relations

- **add_observations**
  - Add new observations to existing entities
  - Input: `observations` (array of objects)
    - Each object contains:
      - `entityName` (string): Target entity
      - `contents` (string[]): New observations to add
  - Returns added observations per entity
  - Fails if entity doesn't exist

- **delete_entities**
  - Remove entities and their relations
  - Input: `entityNames` (string[])
  - Cascading deletion of associated relations
  - Silent operation if entity doesn't exist

- **delete_observations**
  - Remove specific observations from entities
  - Input: `deletions` (array of objects)
    - Each object contains:
      - `entityName` (string): Target entity
      - `observations` (string[]): Observations to remove
  - Silent operation if observation doesn't exist

- **delete_relations**
  - Remove specific relations from the graph
  - Input: `relations` (array of objects)
    - Each object contains:
      - `from` (string): Source entity name
      - `to` (string): Target entity name
      - `relationType` (string): Relationship type
  - Silent operation if relation doesn't exist

- **read_graph**
  - Read the entire knowledge graph
  - No input required
  - Returns complete graph structure with all entities and relations

- **search_nodes**
  - Search for nodes based on query
  - Input: `query` (string)
  - Searches across:
    - Entity names
    - Entity types
    - Observation content
  - Returns matching entities and their relations

- **open_nodes**
  - Retrieve specific nodes by name
  - Input: `names` (string[])
  - Returns:
    - Requested entities
    - Relations between requested entities
  - Silently skips non-existent nodes

# Usage with Claude Desktop

### Setup

Add this to your claude_desktop_config.json:

#### Docker

```json
{
  ""mcpServers"": {
    ""memory"": {
      ""command"": ""docker"",
      ""args"": [""run"", ""-i"", ""--rm"", ""mcp/memory""]
    }
  }
}
```

#### NPX
```json
{
  ""mcpServers"": {
    ""memory"": {
      ""command"": ""npx"",
      ""args"": [
        ""-y"",
        ""@modelcontextprotocol/server-memory""
      ]
    }
  }
}
```

### System Prompt

The prompt for utilizing memory depends on the use case. Changing the prompt will help the model determine the frequency and types of memories created.

Here is an example prompt for chat personalization. You could use this prompt in the ""Custom Instructions"" field of a [Claude.ai Project](https://www.anthropic.com/news/projects). 

```
Follow these steps for each interaction:

1. User Identification:
   - You should assume that you are interacting with default_user
   - If you have not identified default_user, proactively try to do so.

2. Memory Retrieval:
   - Always begin your chat by saying only ""Remembering..."" and retrieve all relevant information from your knowledge graph
   - Always refer to your knowledge graph as your ""memory""

3. Memory
   - While conversing with the user, be attentive to any new information that falls into these categories:
     a) Basic Identity (age, gender, location, job title, education level, etc.)
     b) Behaviors (interests, habits, etc.)
     c) Preferences (communication style, preferred language, etc.)
     d) Goals (goals, targets, aspirations, etc.)
     e) Relationships (personal and professional relationships up to 3 degrees of separation)

4. Memory Update:
   - If any new information was gathered during the interaction, update your memory as follows:
     a) Create entities for recurring organizations, people, and significant events
     b) Connect them to the current entities using relations
     b) Store facts about them as observations
```

## Building

Docker:

```sh
docker build -t mcp/memory -f src/memory/Dockerfile . 
```

## License

This MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.",0
https://mcp.so/server/AgentWong_optimized-memory-mcp-serverv2/MCP-Mirror,https://github.com/MCP-Mirror/AgentWong_optimized-memory-mcp-serverv2,optimized-memory-mcp-serverv2,Mirror of,English,developer-tools,mcp-server; AI; python,"what is optimized-memory-mcp-serverv2? This project is a Python-based Model Context Protocol (MCP) server designed to work with Claude Desktop as an MCP client, testing Claude AI's ability to self-write server code. how to use optimized-memory-mcp-serverv2? To use this project, clone the repository, set up a Python virtual environment, install the required dependencies, initialize the database, and run the server using the provided commands. key features of optimized-memory-mcp-serverv2? Python-based implementation of an MCP server. Integration with Claude Desktop as a client. Modular project structure with clear documentation and testing. use cases of optimized-memory-mcp-serverv2? Developing AI applications that require a context protocol server. Testing AI capabilities in self-writing code. Providing a backend for AI-driven applications. FAQ from optimized-memory-mcp-serverv2? What is the purpose of this project? The project aims to test Claude AI's ability to generate server code for its own use. What are the system requirements? Python 3.13.1 is required to run this project. How do I contribute to the project? Follow the conventions outlined in the documentation for best practices.","# optimized-memory-mcp-serverv2
This is a personal project to test Claude AI's ability to self-write an MCP Server code for its own use.

This is a Python-based Model Context Protocol (MCP) server implementation designed to work with Claude Desktop as an MCP client.

## Project Structure

```
mcp_server/
â”œâ”€â”€ README.md                 # Project documentation
â”œâ”€â”€ requirements.txt          # Project dependencies
â”œâ”€â”€ server.py                # Main server implementation
â”œâ”€â”€ tests/                   # Test directory
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_resources.py
â”‚   â””â”€â”€ test_tools.py
â”œâ”€â”€ docs/                    # Documentation
â”‚   â””â”€â”€ CONVENTIONS.md       # Implementation conventions
â””â”€â”€ src/                     # Source code
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ resources/          # Resource implementations
    â”‚   â””â”€â”€ __init__.py
    â”œâ”€â”€ tools/              # Tool implementations
    â”‚   â””â”€â”€ __init__.py
    â””â”€â”€ utils/              # Utility functions
        â””â”€â”€ __init__.py
```

## Getting Started

1. Ensure Python 3.13.1 is installed:
   ```bash
   python --version  # Should show 3.13.1
   ```

2. Install uvx if not already installed:
   ```bash
   pip install uvx
   ```

3. Clone this repository:
   ```bash
   git clone https://github.com/AgentWong/optimized-memory-mcp-serverv2.git
   cd optimized-memory-mcp-serverv2
   ```

4. Create and activate a virtual environment:
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # On Unix/macOS
   # or
   .venv\Scripts\activate  # On Windows
   ```

5. Install dependencies:
   ```bash
   pip install -r requirements.txt
   pip install -r requirements-dev.txt  # For development
   ```

6. Initialize the database:
   ```bash
   alembic upgrade head
   ```

7. Run the server:
   ```bash
   uvx run python -m src.main
   ```

## Development

Follow the conventions outlined in `docs/CONVENTIONS.md` for implementation details and best practices.
",0
https://mcp.so/server/AgentX/lokesMCI,https://github.com/lokesMCI/AgentX,AgentX,"The future of development is here, this repo integrates various MCP servers and gives a Agent Client implementation - called AgentX to use those MCP servers to Solve real world problems",English,research-and-data,mathgpt; math-solver; math-assistant,"what is AgentX? AgentX is a development tool that integrates various MCP servers and provides an Agent Client implementation to solve real-world problems using these servers. how to use AgentX? To use AgentX, clone the repository from GitHub, set up the necessary MCP servers, and run the Agent Client to start solving problems. key features of AgentX? Integration with multiple MCP servers Implementation of an Agent Client for real-world problem solving Open-source with MIT license use cases of AgentX? Automating tasks across different MCP servers Developing applications that require real-time data processing Solving complex problems by leveraging multiple server capabilities FAQ from AgentX? What is an MCP server? MCP servers are multi-channel processing servers that handle various tasks and data streams. Is AgentX free to use? Yes! AgentX is open-source and free to use under the MIT license. How can I contribute to AgentX? You can contribute by submitting issues, pull requests, or suggestions on the GitHub repository.","# AgentX
The future of development is here, this repo integrates various MCP servers and gives a Agent Client implementation - called AgentX to use those MCP servers to Solve real world problems
",0
https://mcp.so/server/Agentic-AI-Projects/gautamgc17,https://github.com/gautamgc17/Agentic-AI-Projects,Agentic-AI-Projects,"About This repository features a collection of agentic AI projects across various domains, showcasing practical applications of AI agents with different frameworks, techniques, and tools.",English,research-and-data,langflow; function-calling; crewai; langgraph; agentic-ai; agentic-workflows,"what is Agentic-AI-Projects? Agentic-AI-Projects is a repository that showcases a collection of agentic AI projects across various domains, highlighting practical applications of AI agents using different frameworks, techniques, and tools. how to use Agentic-AI-Projects? To use the projects in this repository, visit the GitHub page, explore the various projects listed, and follow the provided documentation for each project to implement them in your own applications. key features of Agentic-AI-Projects? A diverse collection of AI projects across multiple domains. Insights into practical applications of AI agents. Use of various frameworks and tools for agentic AI development. use cases of Agentic-AI-Projects? Developing intelligent agents for automation tasks. Implementing AI-driven workflows in business processes. Exploring the capabilities of different AI frameworks and techniques. FAQ from Agentic-AI-Projects? What types of projects are included in this repository? The repository includes a variety of projects that utilize agentic AI across different domains and tasks. How can I contribute to Agentic-AI-Projects? Contributions are welcome! You can fork the repository, add your projects, and submit a pull request. Are there any prerequisites to use the projects? Familiarity with AI concepts and the specific frameworks used in the projects is recommended.","# Agentic-AI-Projects

This repository features a diverse collection of agentic AI projects covering multiple domains, tasks, and real-world use cases. It highlights different frameworks, techniques, tools, LLMs, and agent communication protocols, offering practical insights into how AI agents can be used in real-world scenarios.
",2
https://mcp.so/server/AgenticProductSearching/Gen-AI-Developer,https://github.com/Gen-AI-Developer/AgenticProductSearching,AgenticProductSearching,Agentic Product Searching containing MCP Server / DB / Fast API / OpenAI Agent SDK,English,research-and-data,mathgpt; math-solver; math-assistant,"what is Agentic Product Searching? Agentic Product Searching is a comprehensive solution that integrates a MCP Server, Database, Fast API, and OpenAI Agent SDK to facilitate efficient product searching. how to use Agentic Product Searching? To use Agentic Product Searching, set up the MCP Server and connect it with the Fast API and OpenAI Agent SDK to start querying products effectively. key features of Agentic Product Searching? Integration of multiple components for seamless product searching Utilization of OpenAI's capabilities for enhanced search results Fast API for quick and efficient data handling use cases of Agentic Product Searching? E-commerce platforms looking to improve product search functionality. Applications requiring intelligent product recommendations. Businesses needing to analyze product data efficiently. FAQ from Agentic Product Searching? What technologies does Agentic Product Searching use? It uses MCP Server, Database, Fast API, and OpenAI Agent SDK. Is there a demo available? Currently, there is no demo available, but you can check the GitHub repository for setup instructions. Can I contribute to the project? Yes! Contributions are welcome, and you can find guidelines in the GitHub repository.",,
https://mcp.so/server/Agentic_search/Code-Trees,https://github.com/Code-Trees/Agentic_search,File Search Assistant with LLM Integration,"Learn how to: âœ… Build a file-search AI using natural language queries âœ… Create embeddings from local Linux files using Hugging Face models âœ… Integrate Gemini API (Google AI Studio) into your local apps âœ… Use MCP to control multiple agents with server-client architecture âœ… Apply cosine similarity, asyncio Python, and more!",English,research-and-data,mathgpt; math-solver; math-assistant,"what is Agentic_search? Agentic_search is a file search assistant that integrates Large Language Models (LLM) to enable intelligent file searching using natural language queries in Linux systems. how to use Agentic_search? To use Agentic_search, clone the repository, install the dependencies, configure your environment variables with the Gemini API key, and run the main application to start searching for files using natural language queries. key features of Agentic_search? Natural language file search queries Semantic search using BERT embeddings Integration with Gemini LLM for enhanced query understanding MCP server for efficient file system operations Automatic inference of file extensions use cases of Agentic_search? Finding specific files based on natural language descriptions. Searching for documents or scripts in a Linux environment. Enhancing productivity by quickly locating files without remembering exact names. FAQ from Agentic_search? What programming language is used for Agentic_search? The project is developed in Python. Do I need a GPU to run Agentic_search? A CUDA-compatible GPU is optional but recommended for faster processing. How does the semantic search work? It uses BERT embeddings to understand the context of the search queries.","# File Search Assistant with LLM Integration

This project combines semantic search capabilities with Large Language Models (LLM) to provide intelligent file search functionality in Linux systems.

## Features

- Natural language file search queries
- Semantic search using BERT embeddings
- File system integration with MCP server
- Gemini LLM integration for query understanding
- Automatic file extension inference

## Prerequisites

- Python 3.8 or higher
- CUDA-compatible GPU (optional, for faster processing)
- Linux operating system

## Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd LLm_To_agent/data_dir
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Configure environment variables:
   - Create a `.env` file in the project root
   - Add your Gemini API key:
   ```
   GEMINI_API_KEY=""your-api-key-here""
   ```

## Project Structure

```
data_dir/
â”œâ”€â”€ main.py              # Main application entry point
â”œâ”€â”€ tools/
â”‚   â””â”€â”€ file_finder.py   # File search implementation
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ doc_Search.py    # Document search utilities
â”œâ”€â”€ llm/
â”‚   â””â”€â”€ gemini_client.py # LLM integration
â”œâ”€â”€ .env                 # Environment variables
â””â”€â”€ OSData_store.pth     # Embedded data storage
```

## Usage

1. Start the application:
```bash
python main.py
```

2. Enter your search query when prompted:
```
What file are you looking for?
> ""find a python script that handles file operations""
```

3. The system will:
   - Extract the relevant filename using LLM
   - Search the file system using semantic search
   - Display matching file locations

## Features in Detail

- **LLM Integration**: Uses Google's Gemini for natural language understanding
- **Semantic Search**: Employs BERT embeddings for context-aware file matching
- **MCP Server**: Handles file system operations efficiently
- **Extensible Architecture**: Easy to add new search capabilities

## Troubleshooting

If you encounter the error ""Embeddings file not found"", run:
```bash
cd ~ && find / -type f 2>/dev/null >> Desktop/LLm_To_agent
```

## License

[Your chosen license]

## Contributing

[Your contribution guidelines]",0
https://mcp.so/server/AiSpire/CompewterTutor,https://github.com/CompewterTutor/AiSpire,AiSpire,MCP Server and Plugin for Vectric Aspire/V-Carve,English,developer-tools,,"what is AiSpire? AiSpire is an intelligent interface designed for Vectric Aspire and V-Carve CAD/CAM software, enabling AI-powered design and machining capabilities through a Lua plugin and a Python MCP server. how to use AiSpire? To use AiSpire, you will need to install the Lua Gadget plugin within the Vectric software and set up the Python MCP server. Detailed installation instructions will be provided in the future. key features of AiSpire? Execute arbitrary Lua code within Vectric software. Create and manipulate vector paths programmatically. Import and modify 3D models. Draw vector shapes and text. Optimize vector nesting for material utilization. Generate and calculate toolpaths for CNC machining. use cases of AiSpire? Automating the design process for CNC machining. Enhancing the capabilities of Vectric Aspire and V-Carve with AI. Streamlining the creation of complex vector paths and shapes. FAQ from AiSpire? What software do I need to use AiSpire? You need Vectric Aspire or V-Carve software to use AiSpire. Is AiSpire free to use? License information is yet to be determined. What programming languages are used in AiSpire? AiSpire uses Lua for the plugin and Python for the MCP server.","```
                                          
     .oo  o .oPYo.         o              
    .P 8    8                             
   .P  8 o8 `Yooo. .oPYo. o8 oPYo. .oPYo. 
  oPooo8  8     `8 8    8  8 8  `' 8oooo8 
 .P    8  8      8 8    8  8 8     8.     
.P     8  8 `YooP' 8YooP'  8 8     `Yooo' 
..:::::..:..:.....:8 ....::....:::::.....:
:::::::::::::::::::8 :::::::::::::::::::::
:::::::::::::::::::..:::::::::::::::::::::
```
# AiSpire


AiSpire is an intelligent interface for Vectric Aspire and V-Carve CAD/CAM software that enables AI-powered design and machining capabilities.

## Overview

AiSpire consists of two primary components:
1. **Lua Gadget** - A plugin for Vectric Aspire/V-Carve that runs inside the CAD/CAM environment
2. **Python MCP Server** - A server implementing the Model Context Protocol for LLM integration

```
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚             â”‚            â”‚             â”‚            â”‚         â”‚
                 â”‚    LLMs     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚ Python MCP  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚   Lua   â”‚
                 â”‚             â”‚   (MCP)    â”‚   Server    â”‚  (Socket)  â”‚ Gadget  â”‚
                 â”‚             â”‚            â”‚             â”‚            â”‚         â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                           â”‚
                                                                           â”‚
                                                                           â–¼
                                                                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                                      â”‚  Vectric    â”‚
                                                                      â”‚ Aspire/     â”‚
                                                                      â”‚  V-Carve    â”‚
                                                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Features

- Execute arbitrary Lua code inside Vectric software
- Create and manipulate vector paths programmatically
- Import and modify 3D models
- Draw vector shapes and text
- Optimize vector nesting for material utilization
- Generate and calculate toolpaths for CNC machining
- Interactive UI with log viewing and command history
- Utilize advanced Vectric SDK capabilities:
  - Matrix transformations for precise object manipulation
  - Specialized job types (two-sided, rotary)
  - Advanced toolpath strategies (V-carving, fluting, prism carving)
  - Tool database integration
  - Custom UI elements (HTML dialogs, progress bars)
  - External toolpath generation and import/export

## Project Status

AiSpire is currently in active development. Key components have been implemented:

- **Lua Socket Server**: Basic socket server with command processing pipeline
- **Command Processing**: Framework for executing Lua code and SDK functions
- **UI Manager**: Interactive user interface with log viewer and command history
- **Helper Functions**: Comprehensive library for common operations
- **Python MCP Server**: Basic MCP protocol implementation with socket client

## UI Features

The AiSpire Gadget includes a comprehensive user interface:

- **Log Viewer**: Real-time display of system messages with color-coded severity levels
- **Command History**: View and replay past commands with success/failure indicators
- **History Management**: Save and load command histories for future reference
- **Connection Status**: Visual indicators showing connection state and activity
- **Disconnect Controls**: Safely disconnect with confirmation dialog

The UI is accessible in two ways:
1. Primary gadget action starts the server with UI
2. Secondary action (""Show AiSpire Control Panel"") opens just the UI without restarting the server

## Directory Structure

- `lua_gadget/` - Lua plugin for Vectric software
  - `server.lua` - Socket server implementation
  - `ui_manager.lua` - UI implementation for the gadget
  - `json.lua` - JSON parsing implementation
  - `helpers/` - Helper functions for common operations
- `python_mcp_server/` - Python implementation of MCP server
- `tests/` - Test files for both components
- `docs/` - Project documentation
  - `vectric_sdk/` - Contains detailed Vectric SDK reference documentation
- `llm_brain/` - AI development guidelines and project memory

## Getting Started

*Coming soon - Installation and usage instructions will be provided when the first functional version is available.*

### Building and Testing

The project includes a Makefile with various commands for building and testing:

```bash
# Install development dependencies
make install-dev-deps

# Run all tests
make test

# Run specific test suites
make test-all-lua     # Run all Lua tests
make test-all-python  # Run all Python tests
make test-e2e         # Run end-to-end tests

# More granular test options
make test-lua          # Run Lua tests in tests/lua_tests/
make test-lua-helpers  # Run Lua helper tests
make test-python-core  # Run Python core tests
make test-python-mcp   # Run Python MCP server tests
make test-python-tests # Run tests in tests/python_tests

# Create Vectric gadget bundle
make bundle

# Show all available commands
make help
```

## Requirements

- Vectric Aspire or V-Carve software
- Lua 5.3+ with LuaSocket library
- Python 3.8+ with required packages (see requirements.txt once available)
- Access to an LLM supporting the Model Context Protocol

## SDK Reference

The project includes a comprehensive Vectric SDK reference (located in `docs/vectric_sdk/Vectric.lua`) that documents the available functions for interacting with Vectric Aspire/V-Carve. This reference covers:

- Job creation and management
- Vector and geometry operations
- Toolpath generation with various strategies
- Tool database access
- User interface elements
- 3D component manipulation (Aspire only)
- System information and registry access

## Planned Enhancements

Based on the detailed SDK reference, we plan to implement:

1. **Advanced Geometric Operations**: Matrix-based transformations, complex path creation, and polar coordinate functions
2. **Specialized Job Management**: Support for two-sided and rotary jobs
3. **Comprehensive Toolpath Strategies**: Profile, pocket, V-carving, fluting, prism carving, 3D roughing and finishing
4. **Tool Database Integration**: Access system tool database, create custom tools
5. **UI Integration**: Custom dialogs, progress bars, file selection interfaces
6. **System Integration**: Application detection, file location access, registry settings
7. **External Toolpath Support**: Generate, import, and export custom toolpaths

## License

*License information to be determined*

## Contact

*Contact information to be provided*

",2
https://mcp.so/server/AidenYangX_mapbox-mcp-server/MCP-Mirror,https://github.com/MCP-Mirror/AidenYangX_mapbox-mcp-server,Mapbox MCP Server,Mirror of,English,location-services,mapbox; mcp-server; navigation-tools,"what is Mapbox MCP Server? Mapbox MCP Server is a server designed to interact with the Mapbox API, providing various navigation and geocoding tools. how to use Mapbox MCP Server? To use the Mapbox MCP Server, set up the server by installing the required dependencies, configuring your Mapbox API key, and running the server with the appropriate commands. key features of Mapbox MCP Server? Navigation tools for getting directions and calculating travel times. Geocoding capabilities to convert addresses into coordinates. Comprehensive error handling for various API interactions. use cases of Mapbox MCP Server? Finding directions between two geographical coordinates. Searching for places and converting them into coordinates. Calculating travel time and distance matrices for multiple locations. FAQ from Mapbox MCP Server? What are the prerequisites for using the server? You need Node.js 16 or higher, TypeScript 4.5 or higher, and a valid Mapbox API key. How do I get a Mapbox API key? You can obtain a Mapbox API key by following the instructions on the Mapbox account access tokens page. What are the rate limits for the APIs? The Directions API allows 300 requests per minute, while the Geocoding API allows 600 requests per minute.","<!--
 * @Author: AidenYangX
 * @Email: xscs709560271@gmail.com
 * @Date: 2024-12-21 23:30:55
 * @Description: Mapbox MCP Server
-->

# Mapbox MCP Server

MCP Server for the Mapbox API.

## Features

### Navigation Tools

1. `mapbox_directions`

   - Get directions between coordinates
   - Inputs:
     - `coordinates` ({latitude: number, longitude: number}[])
     - `profile` (optional): ""driving-traffic"", ""driving"", ""walking"", ""cycling""
   - Returns: route details with steps, distance, duration

2. `mapbox_directions_by_places`

   - Get directions between places using their names
   - Inputs:
     - `places` (string[]): Array of place names
     - `profile` (optional): ""driving-traffic"", ""driving"", ""walking"", ""cycling""
     - `language` (optional): Two-letter language code (e.g., ""zh"", ""en"")
   - Returns:
     - Geocoding results for each place
     - Route details with steps, distance, duration
     - Any errors that occurred during processing

3. `mapbox_matrix`

   - Calculate travel time and distance matrices between coordinates
   - Inputs:
     - `coordinates` ({latitude: number, longitude: number}[])
     - `profile` (optional): ""driving"", ""walking"", ""cycling""
     - `annotations` (optional): ""duration"", ""distance"", ""duration,distance""
     - `sources` (optional): Indices of source coordinates
     - `destinations` (optional): Indices of destination coordinates
   - Returns: Matrix of durations and/or distances between points

4. `mapbox_matrix_by_places`
   - Calculate travel time and distance matrices between places using their names
   - Inputs:
     - `places` (string[]): Array of place names (2-25 places)
     - `profile` (optional): ""driving"", ""walking"", ""cycling""
     - `annotations` (optional): ""duration"", ""distance"", ""duration,distance""
     - `language` (optional): Two-letter language code
     - `sources` (optional): Indices of source places
     - `destinations` (optional): Indices of destination places
   - Returns:
     - Geocoding results for each place
     - Matrix of durations and/or distances
     - Any errors that occurred during processing

### Search Tools

1. `mapbox_geocoding`
   - Search for places and convert addresses into coordinates
   - Inputs:
     - `searchText` (string): The place or address to search for
     - `limit` (optional): Maximum number of results (1-10)
     - `types` (optional): Filter by place types (country, region, place, etc.)
     - `language` (optional): Two-letter language code
     - `fuzzyMatch` (optional): Enable/disable fuzzy matching
   - Returns: Detailed location information including coordinates and properties

## Claude Desktop Integration

Add this configuration to your Claude Desktop config file (typically located at `~/Library/Application Support/Claude/claude_desktop_config.json`):

```json
{
  ""mcpServers"": {
    ""mapbox-mcp-server"": {
      ""command"": ""node"",
      ""args"": [""/absolute/path/to/mapbox-mcp-server/build/index.js""],
      ""env"": {
        ""MAPBOX_ACCESS_TOKEN"": ""your-api-key""
      }
    }
  }
}
```

## Setup

### Prerequisites

- Node.js 16 or higher
- TypeScript 4.5 or higher
- A valid Mapbox API key

### API Key

Get a Mapbox API key by following the instructions [here](https://console.mapbox.com/account/access-tokens/).

Set your API key as an environment variable:

```bash
export MAPBOX_ACCESS_TOKEN=your_api_key_here
```

## Rate Limits

- Directions API: 300 requests per minute
- Matrix API:
  - 60 requests per minute for driving/walking/cycling
  - 30 requests per minute for driving-traffic
- Geocoding API: 600 requests per minute

## Deployment

### Structure

In mapbox-mcp-server, we use the following structure to manage the server's handlers:

- `src/server/handlers/base.ts`: Base class for all handlers
- `src/server/registry.ts`: Registry for all handlers
- `src/server/main.ts`: Main entry point for the server

Each feature module follows this structure:

```plaintext
src/
â”œâ”€â”€ types/          # Type definitions
â”œâ”€â”€ schemas/        # Zod schemas for validation
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ definitions/  # Tool definitions
â”‚   â””â”€â”€ handlers/     # Tool implementations
â””â”€â”€ server/
    â””â”€â”€ handlers/     # Handler classes
```

---

**Class Diagram**:
![mapbox-mcp-server-class-diagram](./assets/MapboxMCPServerClass.png)

---

**Process Diagram**:
![mapbox-mcp-server-process-diagram](./assets/MapboxMCPServerProcess.png)

## Error Handling

All tools implement comprehensive error handling:

- Input validation errors
- API request failures
- Rate limit errors
- Service-specific errors (e.g., no routes found, invalid coordinates)

## License

This MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.
",2
https://mcp.so/server/Air-Pollution/Ashwani00001,https://github.com/Ashwani00001/Air-Pollution,Air-Pollution,This project is a Model Context Protocol (MCP) Server built using Node.js + Express.js that interacts with the OpenWeather API. It allows users to fetch air pollution data based on latitude & longitude or city & country. The backend ensures secure API key management using dotenv and handles API requests efficiently.,English,research-and-data,,"what is Air-Pollution? Air-Pollution is a Model Context Protocol (MCP) Server built using Node.js and Express.js that interacts with the OpenWeather API to provide users with air pollution data based on geographical coordinates or city information. how to use Air-Pollution? To use Air-Pollution, send a request to the server with either latitude and longitude or city and country parameters to fetch the relevant air pollution data. key features of Air-Pollution? Fetch air pollution data using geographical coordinates or city names. Secure API key management using dotenv. Efficient handling of API requests. use cases of Air-Pollution? Monitoring air quality in specific locations. Researching the impact of pollution on health. Integrating air quality data into environmental applications. FAQ from Air-Pollution? What data can I retrieve using Air-Pollution? You can retrieve air pollution data based on latitude & longitude or city & country. Is the API key management secure? Yes, the project uses dotenv for secure API key management. How can I contribute to the project? You can contribute by submitting issues or pull requests on the GitHub repository.","# Air-Pollution
This project is a Model Context Protocol (MCP) Server built using Node.js + Express.js that interacts with the OpenWeather API. It allows users to fetch air pollution data based on latitude &amp; longitude or city &amp; country. The backend ensures secure API key management using dotenv and handles API requests efficiently.
",0
https://mcp.so/server/Albiemark_dbx-mcp-server/MCP-Mirror,https://github.com/MCP-Mirror/Albiemark_dbx-mcp-server,dbx-mcp-server,Mirror of,English,developer-tools,dbx-mcp-server; dropbox-integration; mcp-server,"what is dbx-mcp-server? The dbx-mcp-server is a Model Context Protocol (MCP) server that integrates with Dropbox, enabling MCP-compatible clients to interact with Dropbox through a set of powerful tools. how to use dbx-mcp-server? To use dbx-mcp-server, clone the repository, install dependencies, register a Dropbox app, and configure your MCP client to use the server. key features of dbx-mcp-server? Integration with Dropbox's public API Supports file operations like upload, download, and delete Provides account information and metadata retrieval Search functionality for files and folders use cases of dbx-mcp-server? Automating file management tasks in Dropbox. Building applications that require Dropbox integration. Facilitating secure file operations through an MCP interface. FAQ from dbx-mcp-server? Is dbx-mcp-server affiliated with Dropbox? No, it is an independent integration that works with Dropbox's public API. What programming language is used? The server is built with TypeScript. How do I authenticate with Dropbox? The server uses OAuth 2.0 with PKCE for secure authentication.","# dbx-mcp-server

A Model Context Protocol (MCP) server that provides integration with Dropbox, allowing MCP-compatible clients to interact with Dropbox through a set of powerful tools.

**Important Disclaimer:** This project is not affiliated with, endorsed by, or sponsored by Dropbox. It is an independent integration that works with Dropbox's public API.

## Table of Contents

- [Quick Start](#quick-start)
- [Installation](#installation)
- [Authentication](#authentication)
- [Available Tools](#available-tools)
- [Configuration](#configuration)
- [Usage Examples](#usage-examples)
- [Testing](#testing)
- [Development](#development)
- [License](#license)

## Quick Start

1. Clone the repository
2. Run `npm install` to install dependencies
3. Run `npm run build` to build the project
4. Register a Dropbox app at [Dropbox App Console](https://www.dropbox.com/developers/apps):
   - Choose ""Scoped access"" API
   - Choose the access type your app needs
   - Name your app and click ""Create app""
   - Under ""Permissions"", select the required permissions:
     - `files.metadata.read`
     - `files.content.read`
     - `files.content.write`
     - `sharing.write`
     - `account_info.read`
   - Add `http://localhost:3000/callback` as your redirect URI
   - Note your App key and App secret
5. Run the setup script:
   ```bash
   npm run setup
   ```
6. Configure your MCP client to use the server

## Installation

1. **Clone the repository**

   ```bash
   git clone https://github.com/your-username/dbx-mcp-server.git
   cd dbx-mcp-server
   ```

2. **Install dependencies and build**

   ```bash
   npm install
   npm run build
   ```

3. **Run the setup script**

   ```bash
   npm run setup
   ```

4. **Add to MCP settings**

   Add the following to your MCP settings file:

   ```json
   {
     ""mcpServers"": {
       ""dbx"": {
         ""command"": ""node"",
         ""args"": [""/path/to/dbx-mcp-server/build/index.js""]
       }
     }
   }
   ```

## Authentication

The server uses OAuth 2.0 with PKCE for secure authentication with Dropbox.

### Environment Variables

Required:

- `DROPBOX_APP_KEY`: Your Dropbox app's key
- `DROPBOX_APP_SECRET`: Your Dropbox app's secret
- `DROPBOX_REDIRECT_URI`: OAuth redirect URI
- `TOKEN_ENCRYPTION_KEY`: 32+ character key for token encryption

Optional:

- `TOKEN_REFRESH_THRESHOLD_MINUTES`: Minutes before expiration to refresh token (default: 5)
- `MAX_TOKEN_REFRESH_RETRIES`: Maximum number of refresh attempts (default: 3)
- `TOKEN_REFRESH_RETRY_DELAY_MS`: Delay between refresh attempts in ms (default: 1000)

## Available Tools

### File Operations

- `list_files`: List files in a directory
- `upload_file`: Upload a file
- `download_file`: Download a file
- `safe_delete_item`: Safely delete with recycle bin support
- `create_folder`: Create a new folder
- `copy_item`: Copy a file or folder
- `move_item`: Move or rename a file/folder

### Metadata and Search

- `get_file_metadata`: Get file/folder metadata
- `search_file_db`: Search files and folders
- `get_sharing_link`: Create sharing links
- `get_file_content`: Get file contents

### Account Operations

- `get_account_info`: Get account information

## Usage Examples

```typescript
// List files in root directory
await mcp.useTool(""dbx-mcp-server"", ""list_files"", { path: """" });

// Upload a file
await mcp.useTool(""dbx-mcp-server"", ""upload_file"", {
  path: ""/test.txt"",
  content: Buffer.from(""Hello World"").toString(""base64""),
});

// Search for files
await mcp.useTool(""dbx-mcp-server"", ""search_file_db"", {
  query: ""report"",
  path: ""/Documents"",
  max_results: 10,
});
```

## Testing

Run the test suite:

```bash
npm test
```

Tests verify all operations including authentication, file operations, and error handling.

### Test Structure

The test suite is organized into several modules:

- **Dropbox Operations**: Tests for basic file operations (upload, download, list, etc.)
- **Account Operations**: Tests for accessing account information
- **Search and Delete**: Tests for search functionality and safe deletion with recycle bin support
- **Resource System**: Tests for the MCP resource system integration

### Handling Test Data

The tests use dynamically generated file and folder names based on timestamps to avoid conflicts. Test data is automatically cleaned up after test execution.

### Running Specific Tests

To run a specific test file or test group:

```bash
npm test -- tests/dropbox/search-delete.test.ts  # Run specific test file
npm test -- -t ""should search for files""        # Run tests matching description
```

### Troubleshooting Tests

If tests fail with timing or authentication issues:

1. Check that the mock implementations in `tests/setup.ts` match your test expectations
2. Ensure test helpers are correctly configured
3. For Jest scope errors, avoid referencing imported variables in mock factory functions

## Development

Built with:

- TypeScript
- Model Context Protocol SDK
- Dropbox SDK v10.34.0
- Dropbox API v2

## License

MIT License

Copyright (c) 2025 MCP Server Contributors

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the ""Software""), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

[![smithery badge](https://smithery.ai/badge/@Albiemark/dbx-mcp-server)](https://smithery.ai/server/@Albiemark/dbx-mcp-server)
",0
https://mcp.so/server/Alesion30_my-mcp-server/MCP-Mirror,https://github.com/MCP-Mirror/Alesion30_my-mcp-server,My MCP Server,Mirror of,English,developer-tools,mcp; mcp-server; ai-integration,"what is My MCP Server? My MCP Server is an implementation of the Model Context Protocol (MCP), which standardizes how applications provide context to large language models (LLMs). It acts as a bridge connecting AI models with various data sources and tools in a standardized manner. how to use My MCP Server? To use My MCP Server, download the Claude Desktop application, build the custom MCP server, and register it in the Claude Desktop settings. You can then interact with the server to retrieve contextual information from various sources. key features of My MCP Server? Standardized protocol for AI context provision Integration with multiple data sources and tools Easy setup and configuration with Claude Desktop use cases of My MCP Server? Connecting AI applications to local and remote data sources. Enhancing AI responses with real-time data. Facilitating the development of AI applications that require context-aware interactions. FAQ from My MCP Server? What is the Model Context Protocol (MCP)? MCP is an open protocol that standardizes how applications provide context to LLMs, similar to a USB-C port for AI applications. How do I set up My MCP Server? Follow the instructions in the Getting Started section to download Claude Desktop, build the server, and register it. Can I use My MCP Server with any AI model? Yes, My MCP Server is designed to work with various AI models that support the MCP.","<link
  href=""https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css""
  rel=""stylesheet""
/>

# My MCP Server

## MCP(Model Context Protocol)

MCPã¯ã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒLLMã«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æä¾›ã™ã‚‹æ–¹æ³•ã‚’æ¨™æº–åŒ–ã—ãŸã‚ªãƒ¼ãƒ—ãƒ³ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã§ã‚ã‚‹ã€‚ã“ã‚Œã¯AIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®USB-Cãƒãƒ¼ãƒˆã®ã‚ˆã†ãªã‚‚ã®ã§ã€AIãƒ¢ãƒ‡ãƒ«ã¨æ§˜ã€…ãªãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚„ãƒ„ãƒ¼ãƒ«ã‚’æ¨™æº–çš„ãªæ–¹æ³•ã§æ¥ç¶šã™ã‚‹ã“ã¨ã‚’å¯èƒ½ã«ã™ã‚‹ã€‚

```mermaid
flowchart LR
  User(fa:fa-user User)
  Client[""""""
  Host with MCP Client
  (Claude, IDEs, Tools)
  """"""]

  Server1[""MCP Server A""]
  Server2[""MCP Server B""]
  Server3[""MCP Server C""]

  DB1[""Local Data Source A""]
  DB2[""Local Data Source B""]
  Services[""Remote Services""]

  subgraph Local Computer
  User <--> Client
  Client <--> |MCP Protocol| Server1
  Client <--> |MCP Protocol| Server2
  Client <--> |MCP Protocol| Server3

  Server1 <--> DB1
  Server2 <--> DB2
  end

  subgraph Internet
  Server2 <--> |Web APIs| Services
  Server3 <--> |Web APIs| Services
  end
```

å‚è€ƒ: https://modelcontextprotocol.io/introduction

### ã€é¡ä¼¼æ¦‚å¿µã€‘ Function Calling

```mermaid
flowchart LR
  User(fa:fa-user User)
  Server[""Application Server""]
  OpenAI[""OpenAI API""]

  Function1[""Function A on Server""]
  Function2[""Function B on Server""]
  Function3[""Function C on Server""]
  DB[""Local Data Source""]
  Services[""Remote Services""]

  subgraph Local Computer
  User <--> Server
  Server <--> Function1
  Server <--> Function2
  Server <--> Function3
  Function1 <--> DB
  Function2 <--> DB
  end

  subgraph ""Internet(OpenAI)""
  Server --> |prompt| OpenAI
  OpenAI --> |response with function calls| Server
  end

  subgraph Internet
  Function2 <--> |Web APIs| Services
  Function3 <--> |Web APIs| Services
  end
```

å‚è€ƒ: https://platform.openai.com/docs/guides/function-calling


## Getting Started

### 1. Claude Desktopã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹

https://claude.ai/download

### 2. Custom MCPã‚µãƒ¼ãƒãƒ¼ã‚’ãƒ“ãƒ«ãƒ‰ã™ã‚‹

```sh
$ npm install
$ npm run build

# build/main.jsã®ãƒ•ãƒ«ãƒ‘ã‚¹ã‚’è¡¨ç¤ºã™ã‚‹
$ find `pwd` -maxdepth 2 -name main.js
```

### 3. Claude Desktopã®è¨­å®šã§MCPã‚µãƒ¼ãƒãƒ¼ã‚’ç™»éŒ²ã™ã‚‹

> [!NOTE]
> ä»¥ä¸‹ã¯ã€macOSç‰ˆã§ã®ã‚„ã‚Šæ–¹ã§ã™ã€‚ãã®ä»–ã®OSã«ã¤ã„ã¦ã¯[å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://modelcontextprotocol.io/quickstart/server#:~:text=Testing%20your%20server%20with%20Claude%20for%20Desktop)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

```sh
$ vi ~/Library/Application\ Support/Claude/claude_desktop_config.json
```

```json
{
  ""mcpServers"": {
    ""myMcp"": {
      // NOTE: nodeã®ãƒ•ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®šï¼ˆ`$ which node`ï¼‰
      ""command"": ""node"",
      // NOTE: build/main.jsã®ãƒ•ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š
      ""args"": [""xxx/my-mcp-server/build/main.js""]
    }
  }
}
```

### 4. Claude Desktopã§MCPã‚µãƒ¼ãƒãƒ¼ãŒç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹

Claude Desktopã‚’å†èµ·å‹•ã—ã€MCPã‚µãƒ¼ãƒãƒ¼ãŒç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹

![](./images/mcp-server-setting.png)
![](./images/mcp-server-setting2.png)

è©¦ã—ã«ã‚µãƒ³ãƒ•ãƒ©ãƒ³ã‚·ã‚¹ã‚³ã®å¤©æ°—ã‚’èãã¨ã€MCPã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰å¤©æ°—äºˆå ±ã®æƒ…å ±ã‚’å–å¾—ã—ã€ç”ŸæˆAIã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«å¤©æ°—ã®æƒ…å ±ãŒçµ„ã¿è¾¼ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚

![](./images/get-forecast.png)

ï¼ˆå‚è€ƒï¼‰MCPã‚µãƒ¼ãƒãƒ¼ãŒãªã„å ´åˆ

![](./images/none-mcp-server.png)

## å‚è€ƒãƒªãƒ³ã‚¯

- https://www.anthropic.com/news/model-context-protocol
- https://modelcontextprotocol.io/quickstart/server
",0
https://mcp.so/server/AllAboutAI-YT_mcp-servers/MCP-Mirror,https://github.com/MCP-Mirror/AllAboutAI-YT_mcp-servers,MCP Servers - OpenAI and Flux Integration,Mirror of,English,developer-tools,mcp-servers; openai; flux-integration,"What is MCP Servers? MCP Servers is a repository that provides Model Context Protocol (MCP) servers for integrating with OpenAI's o1 model and Flux capabilities. How to use MCP Servers? To use MCP Servers, clone the repository, set up the required environment variables, and start the servers using the provided configurations. Key features of MCP Servers? Direct access to OpenAI's o1-preview model. Streaming support for real-time data processing. Control over temperature and top_p parameters for model responses. System message configuration for customized interactions. Integration with state-of-the-art image models through Flux. Use cases of MCP Servers? Building applications that require advanced AI model interactions. Streaming data processing for real-time applications. Integrating image processing capabilities into AI solutions. FAQ from MCP Servers? What is the MCP protocol? The MCP protocol is a method for interacting with AI models in a standardized way. How do I secure my API keys? Store API keys securely and use environment variables to manage sensitive data. Is there a license for using MCP Servers? Yes, MCP Servers is licensed under the MIT License.","# MCP Servers - OpenAI and Flux Integration

This repository contains MCP (Model Context Protocol) servers for integrating with OpenAI's o1 model and Flux capabilities.

## Server Configurations

### OpenAI o1 MCP Server

The o1 server enables interaction with OpenAI's o1 preview model through the MCP protocol.

```json
{
  ""mcpServers"": {
    ""openai"": {
      ""command"": ""openai-server"",
      ""env"": {
        ""OPENAI_API_KEY"": ""apikey""
      }
    }
  }
}

```

Key features:
- Direct access to o1-preview model
- Streaming support
- Temperature and top_p parameter control
- System message configuration

### Flux MCP Server

The Flux server provides integration with Flux capabilities through MCP.

```json
{
  ""mcpServers"": {
    ""flux"": {
      ""command"": ""flux-server"",
      ""env"": {
        ""REPLICATE_API_TOKEN"": ""your-replicate-token""
      }
    }
  }
}
```

Key features:
- SOTA Image Model

## Usage

1. Clone or Fork Server
```bash
git clone https://github.com/AllAboutAI-YT/mcp-servers.git
```

2. Set up environment variables in your .env file:
```env
FLUX_API_KEY=your_flux_key_here
```

3. Start the servers using the configurations above.

## Security

- Store API keys securely
- Use environment variables for sensitive data
- Follow security best practices in SECURITY.md

## License

MIT License - See LICENSE file for details.
",0
https://mcp.so/server/AlphavantageMCPServer/DonMorr,https://github.com/DonMorr/AlphavantageMCPServer,Overview,Alphavantage MCP Server,English,research-and-data,alphavantage; mcp-server; stock-market,"what is Alphavantage MCP Server? Alphavantage MCP Server is a modified version of the original Alphavantage MCP server, designed to provide access to stock market data through the Alphavantage API, fixing previous bugs for improved functionality. how to use Alphavantage MCP Server? To use the server, sign up for a free Alphavantage API key, configure your environment variables, and set up the server in your claude_desktop_config.json file with the appropriate commands and arguments. key features of Alphavantage MCP Server? Access to real-time and historical stock market data. Bug fixes from the original server for enhanced performance. Easy integration with Claude Desktop for seamless operation. use cases of Alphavantage MCP Server? Retrieving stock prices for analysis and trading. Integrating stock data into applications for financial insights. Automating stock market data retrieval for research purposes. FAQ from Alphavantage MCP Server? How do I get an API key? You can sign up for a free API key at the Alphavantage website. Is there a cost associated with using the Alphavantage API? No, the API is free to use, but there may be usage limits. What kind of data can I access with this server? You can access real-time stock prices, historical data, and various financial indicators.","# Overview

This is a fork of Cesar Alvernaz's ""Alphavantage MCP server"". It fixes a few bugs which were preventing the server from working correctly.

# Alphavantage MCP Server

A MCP server for the stock market data API, Alphavantage API.

## Configuration

### Getting an API Key
1. Sign up for a [Free Alphavantage API key](https://www.alphavantage.co/support/#api-key)
2. Add the API key to your environment variables as `ALPHAVANTAGE_API_KEY`


### Usage with Claude Desktop
Add this to your `claude_desktop_config.json`:

```
{
  ""mcpServers"": {
    ""alphavantage"": {
      ""command"": ""uv"",
      ""args"": [
        ""--directory"",
        ""<DIRECTORY>/alphavantage"",
        ""run"",
        ""alphavantage""
      ],
      ""env"": {
        ""ALPHAVANTAGE_API_KEY"": ""YOUR_API_KEY_HERE""
      }
    }
  }
}
```
",0
https://mcp.so/server/Andrew-Beniash_mcp-command-server/MCP-Mirror,https://github.com/MCP-Mirror/Andrew-Beniash_mcp-command-server,MCP Command Server,Mirror of,English,developer-tools,mcp-command-server; llm-integration; secure-command-execution,"What is MCP Command Server? MCP Command Server is a secure Model Context Protocol (MCP) server designed for executing system commands through LLM applications like Claude. How to use MCP Command Server? To use MCP Command Server, install the package using pip, configure the allowed commands, and integrate it with Claude Desktop configuration. Key features of MCP Command Server? ğŸ”’ Secure command execution with a whitelist of allowed commands. âœ… User confirmation required for all commands to enhance security. ğŸ“ Comprehensive audit logging for tracking command executions. ğŸ” Input validation and sanitization to prevent malicious inputs. ğŸ¤– Integration with Claude Desktop for seamless operation. Use cases of MCP Command Server? Executing system commands securely in LLM applications. Providing a controlled environment for command execution to prevent unauthorized access. Logging and auditing command usage for compliance and security purposes. FAQ from MCP Command Server? Is MCP Command Server secure? Yes! It features a whitelist for commands and requires user confirmation for execution. How do I install MCP Command Server? You can install it using pip with the command: uv pip install mcp-command-server . Can I customize the allowed commands? Yes! You can configure the allowed commands by setting the ALLOWED_COMMANDS environment variable.","# MCP Command Server

A secure Model Context Protocol (MCP) server for executing system commands through LLM applications like Claude.

## Quick Start

1. Install the package:

```bash
uv pip install mcp-command-server
```

2. Configure allowed commands:

```bash
export ALLOWED_COMMANDS=""ls,pwd,echo""
```

3. Add to Claude Desktop configuration:

```json
{
  ""mcpServers"": {
    ""command-server"": {
      ""command"": ""uv"",
      ""args"": [""run"", ""python"", ""-m"", ""mcp_command_server""],
      ""env"": {
        ""ALLOWED_COMMANDS"": ""ls,pwd,echo""
      }
    }
  }
}
```

## Features

- ğŸ”’ Secure command execution with whitelist
- âœ… User confirmation for all commands
- ğŸ“ Comprehensive audit logging
- ğŸ” Input validation and sanitization
- ğŸ¤– Claude Desktop integration

## Documentation

For complete documentation, see the [docs/](./docs/README.md) directory:

- [Installation Guide](./docs/installation.md)
- [Security Guidelines](./docs/security.md)
- [API Reference](./docs/api.md)
- [Usage Examples](./docs/examples.md)
- [Troubleshooting](./docs/troubleshooting.md)

## Development

### Setup

```bash
# Clone repository
git clone https://github.com/yourusername/mcp-command-server.git
cd mcp-command-server

# Create virtual environment
uv venv
source .venv/bin/activate  # On Unix/macOS
.venv\Scripts\activate     # On Windows

# Install development dependencies
uv pip install -e "".[dev]""
```

### Testing

```bash
# Run all tests
pytest

# Run specific test file
pytest tests/unit/security/test_validator.py

# Run with coverage
pytest --cov=mcp_command_server
```

### Contributing

1. Fork the repository
2. Create your feature branch
3. Run tests and linting
4. Submit a pull request

## License

MIT License - see [LICENSE](./LICENSE) for details.
",0
https://mcp.so/server/AndrewDonelson_go-mcp-server-service/MCP-Mirror,https://github.com/MCP-Mirror/AndrewDonelson_go-mcp-server-service,go-mcp-server-service,Mirror of,English,developer-tools,go-mcp-server; note-management; json-rpc,"what is go-mcp-server-service? The go-mcp-server-service is a JSON-RPC 2.0 compliant server that implements the Model Context Protocol (MCP) for managing notes. It serves as a boilerplate for developers to create their own note management systems. how to use go-mcp-server-service? To use the go-mcp-server-service, clone the repository from GitHub, build the project using the provided Makefile, and run the command-line interface or service component to manage notes. key features of go-mcp-server-service? JSON-RPC 2.0 compliant API for seamless integration. Cross-platform support for Windows, Linux, and macOS. Thread-safe note management ensuring data integrity. Custom note:// URI scheme for accessing notes. Command-line interface and service components for flexible usage. use cases of go-mcp-server-service? Building a personal note-taking application. Integrating note management into larger software systems. Developing educational tools that require note storage and retrieval. FAQ from go-mcp-server-service? Is go-mcp-server-service suitable for production use? Yes, it is designed to be robust and can be used in production environments with proper configuration. What programming language is used? The server is built using Go (Golang). How can I contribute to the project? You can contribute by submitting issues or pull requests on the GitHub repository.","# go-mcp-server-service
A JSON-RPC 2.0 compliant server implementing the Model Context Protocol (MCP) for note management (as an example)

This is an example. You can modify this and use as a Boilerplate for your own project. It supports cross-platform development and includes a command-line interface and service component for both development and release builds.

## Features

- JSON-RPC 2.0 compliant API
- Cross-platform support (Windows, Linux, macOS)
- Thread-safe note management
- Development and release build configurations
- Service and command-line interface components

## Components

### Command Line Interface (`cmd`)

The command-line interface provides direct access to the notes server functionality.

### Service (`service`)

The service component enables system-level integration and background operation.

### Resources

The server implements a note storage system with:

- Custom `note://` URI scheme for accessing individual notes
- Resource metadata including name, description, and MIME type
- Thread-safe concurrent access

### Prompts

Available prompts:

- `summarize-notes`: Creates summaries of all stored notes
  - Optional `style` argument (""brief""/""detailed"")
  - Combines all current notes with style preference
  - Thread-safe note access

### Tools

Available tools:

- `add-note`: Adds a new note to the server
  - Required arguments: `name` (string), `content` (string)
  - Thread-safe state updates
  - Returns confirmation message

## Building

### Prerequisites

- Go 1.21 or later
- GNU Make or compatible build tool
- Git (for version information)

### Build Commands

Development builds (includes debug symbols and race detection):

```bash
# Build all components for all platforms
make dev

# Build for specific platform
make dev-windows
make dev-linux
make dev-darwin

# Build specific components
make build-cmd
make build-service
```

Release builds (optimized and stripped):

```bash
# Build all components for all platforms
make release-all

# Build for specific platform
make release-windows
make release-linux
make release-darwin
```

Run locally:

```bash
# Run command-line interface
make run-cmd

# Run service
make run-service
```

View all available targets:

```bash
make help
```

### Build Output

Binaries are created in the `bin` directory:

- Development builds: `bin/dev/<platform>/`
- Release builds: `bin/release/<platform>/`

## Configuration

### Claude Desktop Integration

Configure the notes server in Claude Desktop's configuration file:

#### Location

- MacOS: `~/Library/Application\ Support/Claude/claude_desktop_config.json`
- Windows: `%APPDATA%/Claude/claude_desktop_config.json`

#### Development Configuration

```json
{
  ""mcpServers"": {
    ""notes-server"": {
      ""command"": ""./bin/dev/<platform>/notes-server"",
      ""args"": []
    }
  }
}
```

#### Release Configuration (Service Example)

```json
{
  ""mcpServers"": {
    ""notes-server"": {
      ""command"": ""./bin/release/<platform>/notes-service"",
      ""args"": []
    }
  }
}
```

## Development

### Project Structure

```
.
â”œâ”€â”€ cmd/                    # Command-line interface
â”œâ”€â”€ service/               # Service implementation
â”œâ”€â”€ internal/
â”‚   â””â”€â”€ server/           # Core server implementation
â”‚       â”œâ”€â”€ operations.go # Server operations
â”‚       â”œâ”€â”€ server.go    # Main server logic
â”‚       â””â”€â”€ types.go     # Type definitions
â”œâ”€â”€ Makefile              # Build configuration
â””â”€â”€ README.md
```

### Debugging

Since the server runs over stdio, we recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector) for debugging:

```bash
npx @modelcontextprotocol/inspector ./bin/dev/<platform>/notes-server
```

The Inspector will provide a URL for the debugging interface.

## Error Codes

The server implements standard JSON-RPC 2.0 error codes plus custom codes:

| Code   | Description           | Standard |
| ------ | --------------------- | -------- |
| -32700 | Parse error           | Yes      |
| -32600 | Invalid request       | Yes      |
| -32601 | Method not found      | Yes      |
| -32602 | Invalid params        | Yes      |
| -32603 | Internal error        | Yes      |
| -32001 | Resource not found    | No       |
| -32002 | Unsupported operation | No       |

## License

MIT License

Copyright (c) 2024 Andrew Lee Donelson",0
https://mcp.so/server/Aniket310101_MCP-Server-Couchbase/MCP-Mirror,https://github.com/MCP-Mirror/Aniket310101_MCP-Server-Couchbase,ğŸ—„ï¸ Couchbase MCP Server for LLMs,Mirror of,English,developer-tools,couchbase; mcp-server; llm-integration,"What is Couchbase MCP Server? Couchbase MCP Server is a Model Context Protocol server that allows Large Language Models (LLMs) to interact directly with Couchbase databases on Capella clusters, enabling seamless data management through natural language. How to use Couchbase MCP Server? To use the Couchbase MCP Server, install it via NPX or manually, configure it with your Couchbase credentials, and integrate it with the Claude Desktop application to start querying and managing your database. Key features of Couchbase MCP Server? Execute N1QL queries directly through natural language. Perform CRUD operations on Couchbase databases. Manage scopes and collections efficiently. List and create indexes for optimized data retrieval. Use cases of Couchbase MCP Server? Querying and managing data in Couchbase databases using natural language. Creating and managing collections and scopes for better data organization. Integrating with LLMs for enhanced data interaction and retrieval. FAQ from Couchbase MCP Server? Can I use Couchbase MCP Server with any version of Couchbase? It is recommended to use it with a running Couchbase instance on Capella. Is there a recommended way to install the Couchbase MCP Server? Yes, using NPX is the quickest way to get started. What security measures should I take when using Couchbase MCP Server? Always use environment variables for sensitive credentials and consider running the server behind a reverse proxy.","# ğŸ—„ï¸ Couchbase MCP Server for LLMs

A Model Context Protocol (MCP) server that enables LLMs to interact directly with Couchbase databases on Capella clusters. Query buckets, perform CRUD operations, execute N1QL queries, and manage data seamlessly through natural language.

## ğŸš€ Quick Start

1. **Prerequisites**

   - Node.js 16 or higher
   - A running Couchbase instance on Capella
   - Claude Desktop application

2. **Installation**

   Couchbase MCP Server can be installed in two ways:

   ### Option 1: Using NPX (Recommended)

   The quickest way to get started is using NPX:

   ```bash
   npx -y @couchbasedatabase/couchbase-mcp
   ```

   ### Option 2: Manual Installation

   If you prefer to clone and run the project manually:

   ```bash
   # Clone the repository
   git clone https://github.com/Aniket310101/MCP-Server-Couchbase.git
   cd MCP-Server-Couchbase

   # Install dependencies
   npm install

   # Build the project
   npm run build
   ```

3. **Claude Desktop Integration**

   Add this configuration to your Claude Desktop config file:

   **Windows**: `%APPDATA%/Claude/claude_desktop_config.json`  
   **MacOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`

   ### Option 1: With Package Installation

   ```json
   {
     ""mcpServers"": {
       ""couchbase"": {
         ""command"": ""npx"",
         ""args"": [""-y"", ""@couchbasedatabase/couchbase-mcp""],
         ""env"": {
           ""COUCHBASE_URL"": ""<COUCHBASE CONNECTION STRING>"",
           ""COUCHBASE_BUCKET"": ""<BUCKET NAME>"",
           ""COUCHBASE_USERNAME"": ""<COUCHBASE USERNAME>"",
           ""COUCHBASE_PASSWORD"": ""<COUCHBASE PASSWORD>""
         }
       }
     }
   }
   ```

   ### Option 2: With Manual Installation

   ```json
   {
     ""mcpServers"": {
       ""couchbase"": {
         ""command"": ""node"",
         ""args"": [""path/to/MCP-Server-Couchbase/dist/index.js""],
         ""env"": {
           ""COUCHBASE_URL"": ""<COUCHBASE CONNECTION STRING>"",
           ""COUCHBASE_BUCKET"": ""<BUCKET NAME>"",
           ""COUCHBASE_USERNAME"": ""<COUCHBASE USERNAME>"",
           ""COUCHBASE_PASSWORD"": ""<COUCHBASE PASSWORD>""
         }
       }
     }
   }
   ```

4. **Verify Connection**

   - Restart Claude Desktop
   - The Couchbase MCP server tools should now be available in your conversations

## ğŸ“ Available Tools

### Basic Operations

- `query`: Execute N1QL queries
- `listBuckets`: List available buckets

### Scope Management

- `createScope`: Create a new scope in a bucket
- `deleteScope`: Delete an existing scope
- `listScopes`: List all scopes in a bucket

### Collection Management

- `createCollection`: Create a new collection in a scope
- `dropCollection`: Delete a collection from a scope

### Document Operations

- `createDocument`: Create a new document
- `getDocument`: Retrieve a document by ID
- `updateDocument`: Update an existing document
- `deleteDocument`: Delete a document by ID
- `bulkCreateDocuments`: Create multiple documents at once

### Index Management

- `createIndex`: Create a new index on specified fields
- `createPrimaryIndex`: Create a primary index on a collection
- `listIndexes`: List all indexes in a bucket
- `dropIndex`: Drop an existing index

Each tool supports optional `collection` and `scope` parameters for targeting specific data containers.

## ğŸ”’ Security Considerations

- Always use environment variables for sensitive credentials
- Consider running the server behind a reverse proxy for production use
- Implement appropriate access controls and authentication as needed

## ğŸ“š Examples

Here are some example interactions with Claude using the MCP server:

1. List all buckets:

   ```
   Could you show me all available buckets in the database?
   ```

2. Create a scope and collection:

   ```
   Create a new scope called ""users"" and a collection called ""profiles"" in it
   ```

3. Query documents:

   ```
   Find all users who signed up in the last 30 days
   ```

4. Create a document:
   ```
   Create a new user document with name ""John Doe"" and email ""john@example.com""
   ```

## ğŸ¤ Contribution

Contributions are welcome! Please feel free to submit a Pull Request.
",0
https://mcp.so/server/Anki-MCP-Server/ethangillani,https://github.com/ethangillani/Anki-MCP-Server,Anki MCP Server,Anki MCP Server to allow LLMs to create and manage Anki decks via Anki Connect,English,developer-tools,,"what is Anki MCP Server? Anki MCP Server is a Model Context Protocol (MCP) server that allows Large Language Models (LLMs) to create and manage Anki flashcard decks through the AnkiConnect API. how to use Anki MCP Server? To use Anki MCP Server, ensure you have Anki and the AnkiConnect add-on installed, then start the server and connect your MCP client to it. key features of Anki MCP Server? Create new decks in Anki Add notes to existing decks List available decks and note models Search for notes using Anki's search syntax Get detailed information about note models and their fields Bulk add multiple notes at once use cases of Anki MCP Server? Automating the creation of flashcard decks for study purposes. Integrating LLMs to enhance learning experiences with personalized flashcards. Managing large sets of notes efficiently through programmatic access. FAQ from Anki MCP Server? What is required to run Anki MCP Server? You need Node.js, Anki with the AnkiConnect add-on, and a compatible MCP client. How do I install AnkiConnect? Install it by going to Tools > Add-ons > Get Add-ons in Anki and entering the code: 2055492159 . Can I use Anki MCP Server with any LLM? Yes, as long as the LLM supports the Model Context Protocol.","# Anki MCP Server

A Model Context Protocol (MCP) server that enables LLMs to interact with Anki flashcard software through the AnkiConnect API.

## Features

- Create new decks in Anki
- Add notes to existing decks
- List available decks and note models
- Search for notes using Anki's search syntax
- Get detailed information about note models and their fields
- Bulk add multiple notes at once

## Prerequisites

- [Node.js](https://nodejs.org/) (v14 or later)
- [Anki](https://apps.ankiweb.net/) with the [AnkiConnect](https://ankiweb.net/shared/info/2055492159) add-on installed
- A Model Context Protocol compatible client (such as Claude with Anthropic MCP support)

## Installation

1. Make sure you have Anki installed with the AnkiConnect add-on
   - Install AnkiConnect by going to Tools > Add-ons > Get Add-ons and entering code: `2055492159`

2. Clone this repository:
   ```
   git clone https://github.com/yourusername/anki-mcp-server.git
   cd anki-mcp-server
   ```

3. Install dependencies:
   ```
   npm install
   ```

4. Build the project:
   ```
   npm run build
   ```

## Usage

1. Make sure Anki is running on your computer with AnkiConnect enabled

2. Start the MCP server:
   ```
   npm start
   ```

3. Connect your MCP client (e.g., Claude) to this server

## Available Tools

The server provides the following tools to MCP clients:

- **listDecks**: Get a list of all decks in Anki
- **listModels**: Get a list of all note models/types in Anki
- **createDeck**: Create a new deck in Anki
- **getModel**: Get details about a specific note model/type
- **addNote**: Add a single note to a deck
- **addNotes**: Add multiple notes at once
- **searchNotes**: Search for notes using Anki's search syntax

## Examples

### Create a New Deck

```json
{
  ""name"": ""createDeck"",
  ""arguments"": {
    ""name"": ""My New Deck""
  }
}
```

### Add a Note

```json
{
  ""name"": ""addNote"",
  ""arguments"": {
    ""deckName"": ""My New Deck"",
    ""modelName"": ""Basic"",
    ""fields"": {
      ""Front"": ""What is the capital of France?"",
      ""Back"": ""Paris""
    },
    ""tags"": [""geography"", ""europe""]
  }
}
```

### Search Notes

```json
{
  ""name"": ""searchNotes"",
  ""arguments"": {
    ""query"": ""deck:\""My New Deck\"" tag:geography""
  }
}
```

## Configuration

The server configuration is in the `config` object in `src/index.ts`. You can modify:

- `ankiConnectUrl`: URL for the AnkiConnect API (default: `http://localhost:8765`)
- `apiVersion`: API version for AnkiConnect (default: `6`)
- `defaultDeckName`: Default deck to use if none specified (default: `Default`)

## Troubleshooting

1. **Cannot connect to Anki**
   - Make sure Anki is running
   - Check that AnkiConnect add-on is installed and working
   - Verify that the AnkiConnect URL is correct (default: http://localhost:8765)

2. **Permission issues with AnkiConnect**
   - AnkiConnect may prompt for permission when the server tries to add cards. Look for a popup in Anki.

## License

MIT License",1
https://mcp.so/server/Anthropic-MCP-Server/CoderSoumya007,https://github.com/CoderSoumya007/Anthropic-MCP-Server,Anthropic MCP Server,Server for Posting Tweets from X (Twitter) Using a Google Sheet as a source,English,cloud-platforms,,"what is Anthropic MCP Server? Anthropic MCP Server is a server application designed to post tweets from X (formerly known as Twitter) using data provided in a Google Sheet as a source. how to use Anthropic MCP Server? To use the Anthropic MCP Server, set up your Google Sheet with your desired tweets, configure the server settings, and then run the server to automate the posting of tweets. key features of Anthropic MCP Server? Integration with Google Sheets for tweet content management. Automated posting of tweets on specified schedules. User-friendly configuration for seamless operation. use cases of Anthropic MCP Server? Scheduling promotional tweets for social media marketing. Automating updates for events or announcements. Managing a feed of tweets for content creators efficiently. FAQ from Anthropic MCP Server? Can I customize the content of the tweets? Yes! You can edit the Google Sheet to change the tweet content. Is there a limit to how many tweets can be posted? There are rate limits imposed by the Twitter API, so you'll need to follow those guidelines. Do I need programming knowledge to set up Anthropic MCP Server? Basic understanding of Google Sheets and following the provided documentation is sufficient.","# Anthropic MCP Server
 Server for Posting Tweets from X (Twitter) Using a Google Sheet as a source
",0
https://mcp.so/server/AnuragRai017_python-docs-server-MCP-Server/MCP-Mirror,https://github.com/MCP-Mirror/AnuragRai017_python-docs-server-MCP-Server,python-docs-server MCP Server,Mirror of,English,developer-tools,python-docs-server; MCP-server; documentation-fetcher,"what is python-docs-server MCP Server? The python-docs-server MCP Server is a TypeScript-based Model Context Protocol server that allows users to fetch Python documentation using the Brave Search API. how to use python-docs-server MCP Server? To use the server, install the necessary dependencies, build the server, and configure it with Claude Desktop by adding the server configuration to the appropriate JSON file based on your operating system. key features of python-docs-server MCP Server? Fetches Python documentation for specific queries using the Brave Search API. Provides a command-line interface for easy interaction. Supports debugging through the MCP Inspector tool. use cases of python-docs-server MCP Server? Quickly retrieving Python documentation for programming queries. Assisting developers in finding relevant documentation links while coding. Integrating with other applications that require Python documentation access. FAQ from python-docs-server MCP Server? What programming language is used for the server? The server is built using TypeScript. How do I install the server? You can install it by running npm install in the project directory. Is there a way to debug the server? Yes! You can use the MCP Inspector for debugging, which provides a URL to access debugging tools in your browser.","# python-docs-server MCP Server

A Model Context Protocol server

This is a TypeScript-based MCP server that provides tools to fetch Python documentation using the Brave Search API.

## Features

### Tools
- `get_python_docs` - Get Python documentation for a given query
  - Takes a search query as a required parameter
  - Uses the Brave Search API to fetch relevant documentation links

## Development

Install dependencies:
```bash
npm install
```

Build the server:
```bash
npm run build
```

For development with auto-rebuild:
```bash
npm run watch
```

## Installation

To use with Claude Desktop, add the server config:

On MacOS: `~/Library/Application Support/Claude/claude_desktop_config.json`
On Windows: `%APPDATA%/Claude/claude_desktop_config.json`

```json
{
  ""mcpServers"": {
    ""python-docs-server"": {
      ""command"": ""/path/to/python-docs-server/build/index.js""
    }
  }
}
```

### Debugging

Since MCP servers communicate over stdio, debugging can be challenging. We recommend using the [MCP Inspector](https://github.com/modelcontextprotocol/inspector), which is available as a package script:

```bash
npm run inspector
```

The Inspector will provide a URL to access debugging tools in your browser.
",0
https://mcp.so/server/Apple-Shortcuts-MCP/as2811-project,https://github.com/as2811-project/Apple-Shortcuts-MCP,Apple Shortcuts MCP,Extend Claude's capabilities using MCP and MacOS Shortcuts,English,developer-tools,apple; mcp; gemini; shortcuts-app; mcp-server,"what is Apple Shortcuts MCP? Apple Shortcuts MCP is a project that extends the capabilities of Claude using MacOS Shortcuts, allowing users to automate tasks such as extracting ingredients from recipes, summarizing conversations, and creating calendar events. how to use Apple Shortcuts MCP? To use Apple Shortcuts MCP, set up the required Shortcuts on your Mac, clone the repository, install dependencies, and configure the server with your Gemini API key. After setup, you can interact with Claude to perform various tasks. key features of Apple Shortcuts MCP? Extracts ingredients from recipes using the Gemini Video Understanding API. Adds extracted ingredients to a Reminders list. Summarizes chat conversations and creates notes. Creates calendar events based on user input. use cases of Apple Shortcuts MCP? Automating grocery list creation from YouTube recipes. Summarizing important chat discussions for easy reference. Scheduling events in the calendar with minimal input. FAQ from Apple Shortcuts MCP? What do I need to set up Apple Shortcuts MCP? You need MacOS, Python, and the required Shortcuts set up in your Reminders app. Can I use it for any recipe? Yes, as long as you provide a URL to a recipe video, it can extract ingredients. Is there any cost associated with using this project? No, it is free to use.","## Claude + MacOS Shortcuts

This is a simple MCP server that leverages the MacOS Shortcuts functionality to perform specific tasks:

1. Hit Gemini Video Understanding API to extract ingredients from a recipe (this is why I began building this in the first place)
2. Add the extracted ingredients to a list in the Reminders app (this assumes you have a ""Groceries"" list in your Reminders app)
3. Summarise a chat conversation and add the summary to a new note.
4. Create a calendar event

All the above functionalities require the following Shortcuts to be setup on your Mac:

1. Add to Groceries list: https://www.icloud.com/shortcuts/6f696d784722468097df8ed238f8eaf2
2. Summarise and create note: https://www.icloud.com/shortcuts/bb0d13d3790c48e5b78eb26dbc7e3aca
3. Calendar: https://www.icloud.com/shortcuts/c01969b7e92343dfa3f38caad3a08f40

After adding the shorts mentioned above, follow these steps to setup the MCP server:

1. Clone the repo
2. Install uv (brew install uv)
3. cd into the directory of this repo, run `uv sync` followed by `uv pip install google-genai`
4. Copy the Claude Config JSON specified below with your Gemini API key and update your claude_desktop_config.json file
5. Open/Reopen Claude, you should be able to see the tools now.

Config:

```
{
  ""mcpServers"": {
    ""Shortcuts-MCP"": {
      ""command"": ""uv"",
      ""args"": [
        ""--directory"",
        ""ABSOLUTE/PATH/TO/THIS/DIRECTORY"",
        ""run"",
        ""main.py""
      ],
      ""env"": {
        ""GEMINI_API_KEY"": ""your_gemini_api_key""
      }
    }
  }
}
```

#### Usage

1. Extract ingredients and create shopping list: Simply send the URL to a recipe on YouTube and ask Claude to add the ingredients to your list.
2. Calendar: Provide some context on what the event is about, and the start and end datetime values.

You can daisy chain the usage of these tools, for example: You can ask it to simply get the ingredients first, then separately ask it to create the list and add a calendar event.

<img width=""872"" alt=""image"" src=""https://github.com/user-attachments/assets/884b361c-542d-4dc4-817f-739d69e91fe1"" />


#### Troubleshooting

Ensure you have Python, uv and other dependencies installed. On your Mac, create a ""Groceries"" list in your reminders app for the Shortcut itself to work properly.

#### Contributing

If you have other interesting Mac Shortcuts that can be triggered via Claude/MCP, feel free to add your tool here.
",2
https://mcp.so/server/Appwrite/appwrite,https://github.com/appwrite/mcp,Appwrite,"The Appwrite MCP server allows you to connect LLMs and tools like Claude Desktop, Cursor, Windsurf etc. to your Appwrite project and interact with the API using natural language.",English,developer-tools,appwrite; mcp-server; api-integration,"what is Appwrite MCP Server? Appwrite MCP Server is a Model Context Protocol server that allows developers to connect various tools and LLMs to their Appwrite projects, enabling interaction with the Appwrite API using natural language. how to use Appwrite MCP Server? To use the Appwrite MCP Server, configure your environment by creating a .env file with your API key and project ID, then run the server using either uv or pip commands as specified in the installation guide. key features of Appwrite MCP Server? Connects multiple tools and LLMs to Appwrite projects. Supports natural language interaction with the Appwrite API. Provides management capabilities for databases, users, and functions. use cases of Appwrite MCP Server? Integrating AI tools like Claude Desktop and Cursor with Appwrite projects. Managing user data and databases through natural language commands. Facilitating local development and debugging of Appwrite applications. FAQ from Appwrite MCP Server? What tools can I connect with Appwrite MCP Server? Currently, it supports tools like Claude Desktop, Zed, and Cursor. Is there a specific installation method recommended? Yes, using uv is recommended for running the server without specific installation requirements. Can I debug the server? Yes, you can use the MCP inspector for debugging.","# Appwrite MCP server

mcp-name: io.github.appwrite/mcp-for-api

[![Install MCP Server](https://cursor.com/deeplink/mcp-install-light.svg)](https://cursor.com/install-mcp?name=appwrite&config=eyJjb21tYW5kIjoidXZ4IG1jcC1zZXJ2ZXItYXBwd3JpdGUgLS11c2VycyIsImVudiI6eyJBUFBXUklURV9BUElfS0VZIjoiPHlvdXItYXBpLWtleT4iLCJBUFBXUklURV9QUk9KRUNUX0lEIjoiPHlvdXItcHJvamVjdC1pZD4iLCJBUFBXUklURV9FTkRQT0lOVCI6Imh0dHBzOi8vPFJFR0lPTj4uY2xvdWQuYXBwd3JpdGUuaW8vdjEifX0%3D)

## Overview

A Model Context Protocol server for interacting with Appwrite's API. This server provides tools to manage databases, users, functions, teams, and more within your Appwrite project.

## Quick Links
- [Configuration](#configuration)
- [Installation](#installation)
- IDE Integration:
  - [Claude Desktop](#usage-with-claude-desktop)
  - [Cursor](#usage-with-cursor)
  - [Windsurf Editor](#usage-with-windsurf-editor)
  - [VS Code](#usage-with-vs-code)
- [Local Development](#local-development)
- [Debugging](#debugging)

## Configuration

> Before launching the MCP server, you must setup an [Appwrite project](https://cloud.appwrite.io/) and create an API key with the necessary scopes enabled.

Create a `.env` file in your working directory and add the following:

```env
APPWRITE_PROJECT_ID=your-project-id
APPWRITE_API_KEY=your-api-key
APPWRITE_ENDPOINT=https://<REGION>.cloud.appwrite.io/v1
```

Then, open your terminal and run the following command

### Linux and MacOS

```sh
source .env
```

### Windows

#### Command Prompt

```cmd
for /f ""tokens=1,2 delims=="" %A in (.env) do set %A=%B
```

#### PowerShell

```powershell
Get-Content .\.env | ForEach-Object {
  if ($_ -match '^(.*?)=(.*)$') {
    [System.Environment]::SetEnvironmentVariable($matches[1], $matches[2], ""Process"")
  }
}
```

## Installation

### Using uv (recommended)
When using [`uv`](https://docs.astral.sh/uv/) no specific installation is needed. We will
use [`uvx`](https://docs.astral.sh/uv/guides/tools/) to directly run *mcp-server-appwrite*.

```bash
uvx mcp-server-appwrite [args]
```

### Using pip

```bash
pip install mcp-server-appwrite
```
Then run the server using 

```bash
python -m mcp_server_appwrite [args]
```

### Command-line arguments

Both the `uv` and `pip` setup processes require certain arguments to enable MCP tools for various Appwrite APIs.

> When an MCP tool is enabled, the tool's definition is passed to the LLM, using up tokens from the model's available context window. As a result, the effective context window is reduced.  
>  
> The default Appwrite MCP server ships with only the Databases tools (our most commonly used API) enabled to stay within these limits. Additional tools can be enabled by using the flags below.

| Argument | Description |
| --- | --- |
| `--tables-db` | Enables the TablesDB API |
| `--users` | Enables the Users API |
| `--teams` | Enables the Teams API |
| `--storage` | Enables the Storage API |
| `--functions` | Enables the Functions API |
| `--messaging` | Enables the Messaging API |
| `--locale` | Enables the Locale API |
| `--avatars` | Enables the Avatars API |
| `--sites` | Enables the Sites API |
| `--all` | Enables all Appwrite APIs |
| `--databases` | Enables the Legacy Databases API |

## Usage with Claude Desktop

In the Claude Desktop app, open the app's **Settings** page (press `CTRL + ,` on Windows or `CMD + ,` on MacOS) and head to the **Developer** tab. Clicking on the **Edit Config** button will take you to the `claude_desktop_config.json` file, where you must add the following:

```json
{
  ""mcpServers"": {
    ""appwrite"": {
      ""command"": ""uvx"",
      ""args"": [
        ""mcp-server-appwrite""
      ],
      ""env"": {
        ""APPWRITE_PROJECT_ID"": ""<YOUR_PROJECT_ID>"",
        ""APPWRITE_API_KEY"": ""<YOUR_API_KEY>"",
        ""APPWRITE_ENDPOINT"": ""https://<REGION>.cloud.appwrite.io/v1"" // Optional
      }
    }
  }
}

```

> Note: In case you see a `uvx ENOENT` error, ensure that you either add `uvx` to the `PATH` environment variable on your system or use the full path to your `uvx` installation in the config file.

Upon successful configuration, you should be able to see the server in the list of available servers in Claude Desktop.

![Claude Desktop Config](images/claude-desktop-integration.png)

## Usage with [Cursor](https://www.cursor.com/)

Head to Cursor `Settings > MCP` and click on **Add new MCP server**. Choose the type as `Command` and add the command below to the **Command** field.

- **MacOS**

```bash
env APPWRITE_API_KEY=your-api-key env APPWRITE_PROJECT_ID=your-project-id uvx mcp-server-appwrite
```

- **Windows**

```cmd
cmd /c SET APPWRITE_PROJECT_ID=your-project-id && SET APPWRITE_API_KEY=your-api-key && uvx mcp-server-appwrite
```

![Cursor Settings](./images/cursor-integration.png)

## Usage with [Windsurf Editor](https://codeium.com/windsurf)

Head to Windsurf `Settings > Cascade > Model Context Protocol (MCP) Servers` and click on **View raw config**. Update the `mcp_config.json` file to include the following:

```json
{
  ""mcpServers"": {
    ""appwrite"": {
      ""command"": ""uvx"",
      ""args"": [
        ""mcp-server-appwrite""
      ],
      ""env"": {
        ""APPWRITE_PROJECT_ID"": ""<YOUR_PROJECT_ID>"",
        ""APPWRITE_API_KEY"": ""<YOUR_API_KEY>"",
        ""APPWRITE_ENDPOINT"": ""https://<REGION>.cloud.appwrite.io/v1"" // Optional
      }
    }
  }
}
```

![Windsurf Settings](./images/windsurf-integration.png)

## Usage with [VS Code](https://code.visualstudio.com/)

### Configuration

1. **Update the MCP configuration file**: Open the Command Palette (`Ctrl+Shift+P` or `Cmd+Shift+P`) and run `MCP: Open User Configuration`. It should open the `mcp.json` file in your user settings.

2. **Add the Appwrite MCP server configuration**: Add the following to the `mcp.json` file:

```json
{
  ""servers"": {
    ""appwrite"": {
      ""command"": ""uvx"",
      ""args"": [""mcp-server-appwrite"", ""--users""],
      ""env"": {
        ""APPWRITE_PROJECT_ID"": ""<YOUR_PROJECT_ID>"",
        ""APPWRITE_API_KEY"": ""<YOUR_API_KEY>"",
        ""APPWRITE_ENDPOINT"": ""https://<REGION>.cloud.appwrite.io/v1""
      }
    }
  }
}
```

3. **Start the MCP server**: Open the Command Palette (`Ctrl+Shift+P` or `Cmd+Shift+P`) and run `MCP: List Servers`. In the dropdown, select `appwrite` and click on the **Start Server** button.

4. **Use in Copilot Chat**: Open Copilot Chat and switch to **Agent Mode** to access the Appwrite tools.

![VS Code Settings](./images/vs-code-integration.png)

## Local Development

### Clone the repository

```bash
git clone https://github.com/appwrite/mcp.git
```

### Install `uv`

- Linux or MacOS

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

- Windows (PowerShell)

```powershell
powershell -ExecutionPolicy ByPass -c ""irm https://astral.sh/uv/install.ps1 | iex""
```

### Prepare virtual environment

First, create a virtual environment.

```bash
uv venv
```

Next, activate the virtual environment.

- Linux or MacOS

```bash
source .venv/bin/activate
```

- Windows

```powershell
.venv\Scripts\activate
```

### Run the server

```bash
uv run -v --directory ./ mcp-server-appwrite
```

## Debugging

You can use the MCP inspector to debug the server. 

```bash
npx @modelcontextprotocol/inspector \
  uv \
  --directory . \
  run mcp-server-appwrite
```

Make sure your `.env` file is properly configured before running the inspector. You can then access the inspector at `http://localhost:5173`.

## License

This MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.
",60
https://mcp.so/server/Archive-Agent/shredEngineer,https://github.com/shredEngineer/Archive-Agent,ğŸ§  Archive Agent,Archive Agent is an open-source semantic file tracker with OCR + AI search.,English,research-and-data,python; ocr; openai; rag; ollama; lm-studio,"What is Archive Agent? Archive Agent is an open-source semantic file tracker that utilizes OCR and AI search capabilities to manage and retrieve information from various file types. How to use Archive Agent? To use Archive Agent, install it via GitHub, set up the required AI provider (OpenAI or Ollama), and run the command-line interface (CLI) or graphical user interface (GUI) to track and search your files. Key features of Archive Agent? Smart indexing with a Retrieval-Augmented Generation (RAG) engine. Supports multiple AI providers for enhanced performance and privacy. Command-line and graphical user interfaces for user-friendly interaction. Fast semantic chunking and local storage using Qdrant vector database. Use cases of Archive Agent? Tracking changes in documents and images over time. Searching for specific content within a large collection of files. Automating file management tasks through integration with IDEs. FAQ from Archive Agent? What file types does Archive Agent support? Archive Agent supports various file types including text files, PDFs, and images. Is Archive Agent free to use? Yes! Archive Agent is open-source and free for everyone. How does the OCR feature work? Archive Agent uses different OCR strategies to extract text from images and PDFs, allowing for flexible and accurate data retrieval.","![Archive Agent Logo](archive_agent/assets/Archive-Agent-800x300.png)

---

# Archive Agent

*An intelligent file indexer with powerful AI search (RAG engine), automatic OCR, and a seamless MCP interface.*

![GitHub Release](https://img.shields.io/github/v/release/shredEngineer/Archive-Agent)
![GitHub License](https://img.shields.io/github/license/shredEngineer/Archive-Agent)
[![Listed on RAGHub](https://img.shields.io/badge/RAGHub-listed-green)](https://github.com/Andrew-Jang/RAGHub?tab=readme-ov-file#rag-projects)
[![Verified on MCPHub](https://img.shields.io/badge/MCPHub-verified-green)](https://mcphub.com/mcp-servers/shredEngineer/Archive-Agent)
[![Listed on MCP.so](https://img.shields.io/badge/MCP.so-listed-green)](https://mcp.so/server/Archive-Agent/shredEngineer)
[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/499d8d83-02c8-4c9b-9e4f-8e8391395482)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/shredEngineer/Archive-Agent)

**Archive Agent** brings RAG to your command line and connects to your tools via MCP â€” it's *not* a chatbot.

---

## Find what you need with natural language

- **Unlock your documents with semantic AI search & query**
- Files are split using [semantic chunking with context headers](#how-smart-chunking-works) and committed to a local database.
- [RAG engine](#how-chunks-are-retrieved)**Â¹** uses [reranking and expanding](#how-chunks-are-reranked-and-expanded) of retrieved chunks
 
**Â¹** *[Retrieval Augmented Generation](https://en.wikipedia.org/wiki/Retrieval-augmented_generation) is the method of matching pre-made snippets of information to a query.*

---

## Natively index your documents on-device

- **Includes local AI file system indexer**
- Natively ingests [PDFs, images, Markdown, plaintext, and moreâ€¦](#which-files-are-processed)
- [Selects and tracks files using patterns](#how-files-are-selected-for-tracking) like `~/Documents/*.pdf` 
- Transcribes images using [automatic OCR](#ocr-strategies) (experimental) and entity extraction
- Changes are automatically synced to a local [Qdrant](https://qdrant.tech/) vector database.

---

## Your AI, Your Choice

- **Supports many AI providers and MCP** 
- [OpenAI](https://platform.openai.com/docs/overview) or compatible API Â¹ for best performance
- [Ollama](https://ollama.com/) and [LM Studio](https://lmstudio.ai/) for best privacy (local LLM)
- Integrates with your workflow** via a built-in [MCP](https://modelcontextprotocol.io/introduction) server.

<small>**Â¹** Includes [xAI / Grok](https://x.ai/api) and [Claude](https://docs.anthropic.com/en/api/openai-sdk) OpenAI compatible APIs.
Simply adjust the URL [settings](#archive-agent-settings) and overwrite `OPENAI_API_KEY`.</small>

---

## Scalable Performance

- **Fully resumable parallel processing**
- Processes multiple files at once using optimized multi-threading.  
- Uses AI cache and generous request retry logic for all network requests.
- Leverages AI structured output with high-quality prompts and schemas.

---

## Architecture

([If you can't see the diagram below, view it on Mermaid.live](https://mermaid.live/edit#pako:eNqNk29v2jAQxr_KyVLfEZaE8C-qkGhKKS1sLXSbtNEXJrkGi8SJHKeFon732U5gLZu0vUHxPXe_u8c-9iTMIiQ-iQXN1zCdL_mSA5ydwYTHWEiWcbhjOSaMoxaKclVlHmUdBRj-PF8NHgQNN0B5BEGWpkzCFUuwOP-0GpyvxGBR5nkmZAF3InvSwiNY1gAudGVF-5g_LGWWUslC-BLMYSEFlRjvtDJJaYww4pLJHYy2Sgn1JI_VKBcGG2jsAlPKNSFYl3zDeHxgvzC5VkNyiVsJ10gjFIWOT5XNAnIUcJFk4cYHx7Z1_HsmoipuSD64VXymLi_xIc6l5TUdy7XdtmV7luPVswRmlks9yyhdYVSVHy3W5XoKC7UeqRmtlpVQEaNO-IahzAQs2Cv60LK7bs29NNyR8agS8IQ7zUKawH0klHuIqKQrWqApRR4dX_i-RLH7--saqWp1pZsMi42OmQfXTaqnG__29V78p7PaxNhArjVkjlIwfEb4jFToTfjoZxFqjzPGfbCbjo5UOszo1gfProGjClgdrs1hUtHVRWxgtYM5JvhMeYgH8ntO67-edGK4N8b6Nq-W3ezRCXJOI1YWalPquhtTd6vrxshRbzMMefGC4uTO_uysxQdMc11UCrUJTtOpqbeGOq2o8gTIOATTSQPGX9XPLLg7bABpqL87i4gvRYkNkqJIqT6SvU5YErnGFJfEV58RPtEykUuy5G-qTPn9kWXpoVJkZbwm_hNNCnUqc7VqeMmo2qH0GBWqIYogK7kkvuN4XUMh_p5siW85_VbTsdudXrfnui3X7rgNsiN-32t22nbLc9uO2-7Z_d5bg7yaxsq6q9JaTrvf99yO2337BQ1sfns))

```mermaid
graph LR

  %% Ingestion Pipeline
  subgraph Ingestion
    A[<b>Track and Commit Files</b><br>Supports Profiles] --> B[<b>Ingest Files</b><br>Automatic OCR Strategy<br>Image Entity Extraction]
    B --> C[<b>Semantic Chunking</b><br>with Context Headers<br>Lines per Block: 100<br>Words per Chunk: 200]
    C --> D[<b>Embed Chunks</b><br>Model: text-embedding-3-large<br>Vector Size: 3072]
    D --> E[<b>Store Chunks</b><br>Local Qdrant database]
  end

  %% Query Pipeline
  subgraph Query
    F[<b>Ask Question</b>] --> G[<b>Embed Question</b><br>Model: text-embedding-3-large]
    G --> H[<b>Retrieve Nearest Chunks</b><br>Score Min: 0.1<br>Chunks Max: 40]
    E --> H
    H --> I[<b>Rerank by Relevance</b><br>Chunks Max: 30]
    I --> J[<b>Expand Context</b><br>Chunks Radius: 2]
    J --> K[<b>Generate Answer</b>]
    K --> L[<b>Get Answer</b><br>in CLI, GUI, MCP]
  end
```

---

## Just getting started?

- ğŸ‘‰ [Install Archive Agent on Linux](#install-archive-agent)
- ğŸ‘‰ [Run Archive Agent](#run-archive-agent)
- ğŸ‘‰ [MCP Tools](#mcp-tools)
- ğŸ‘‰ [Update Archive Agent](#update-archive-agent)

---

## Documentation

<!-- TOC -->
* [Archive Agent](#archive-agent)
  * [Find what you need with natural language](#find-what-you-need-with-natural-language)
  * [Natively index your documents on-device](#natively-index-your-documents-on-device)
  * [Your AI, Your Choice](#your-ai-your-choice)
  * [Scalable Performance](#scalable-performance)
  * [Architecture](#architecture)
  * [Just getting started?](#just-getting-started)
  * [Documentation](#documentation)
  * [Supported OS](#supported-os)
  * [Install Archive Agent](#install-archive-agent)
    * [Ubuntu / Linux Mint](#ubuntu--linux-mint)
  * [AI provider setup](#ai-provider-setup)
    * [OpenAI provider setup](#openai-provider-setup)
    * [Ollama provider setup](#ollama-provider-setup)
    * [LM Studio provider setup](#lm-studio-provider-setup)
  * [Which files are processed](#which-files-are-processed)
  * [How files are processed](#how-files-are-processed)
  * [OCR strategies](#ocr-strategies)
  * [How smart chunking works](#how-smart-chunking-works)
  * [How chunk references work](#how-chunk-references-work)
  * [How chunks are retrieved](#how-chunks-are-retrieved)
  * [How chunks are reranked and expanded](#how-chunks-are-reranked-and-expanded)
  * [How answers are generated](#how-answers-are-generated)
  * [How files are selected for tracking](#how-files-are-selected-for-tracking)
  * [Run Archive Agent](#run-archive-agent)
  * [Quickstart on the command line (CLI)](#quickstart-on-the-command-line-cli)
  * [CLI command reference](#cli-command-reference)
    * [See list of commands](#see-list-of-commands)
    * [Create or switch profile](#create-or-switch-profile)
    * [Open current profile config in nano](#open-current-profile-config-in-nano)
    * [Add included patterns](#add-included-patterns)
    * [Add excluded patterns](#add-excluded-patterns)
    * [Remove included / excluded patterns](#remove-included--excluded-patterns)
    * [List included / excluded patterns](#list-included--excluded-patterns)
    * [Resolve patterns and track files](#resolve-patterns-and-track-files)
    * [List tracked files](#list-tracked-files)
    * [List changed files](#list-changed-files)
    * [Commit changed files to database](#commit-changed-files-to-database)
    * [Combined track and commit](#combined-track-and-commit)
    * [Search your files](#search-your-files)
    * [Query your files](#query-your-files)
    * [Launch Archive Agent GUI](#launch-archive-agent-gui)
    * [Start MCP Server](#start-mcp-server)
  * [MCP Tools](#mcp-tools)
  * [Update Archive Agent](#update-archive-agent)
    * [Archive Agent settings](#archive-agent-settings)
      * [Profile configuration](#profile-configuration)
    * [Watchlist](#watchlist)
    * [AI cache](#ai-cache)
  * [Qdrant database](#qdrant-database)
  * [Developer's guide](#developers-guide)
    * [Important modules](#important-modules)
    * [Network and Retry Handling](#network-and-retry-handling)
    * [Code testing and analysis](#code-testing-and-analysis)
    * [Run Qdrant with in-memory storage](#run-qdrant-with-in-memory-storage)
  * [Tools](#tools)
    * [Rename file paths in chunk metadata](#rename-file-paths-in-chunk-metadata)
    * [Remove file paths from context headers](#remove-file-paths-from-context-headers)
  * [Known issues](#known-issues)
  * [Licensed under GNU GPL v3.0](#licensed-under-gnu-gpl-v30)
  * [Collaborators welcome](#collaborators-welcome)
  * [Learn about Archive Agent](#learn-about-archive-agent)
<!-- TOC -->

---

## Supported OS

**Archive Agent** has been tested with these configurations:

- **Ubuntu 24.04** (PC x64)
- **Ubuntu 22.04** (PC x64)

If you've successfully installed and tested **Archive Agent** with a different setup, please let me know and I'll add it here! 

---

## Install Archive Agent

Please install these requirements before proceeding:

- [Docker](https://docs.docker.com/engine/install/) *(for running Qdrant server)*
- [Python](https://www.python.org/downloads/) **>= 3.10** *(core runtime)* (usually already installed)

### Ubuntu / Linux Mint

This installation method should work on any Linux distribution derived from Ubuntu (e.g. Linux Mint). 

To install **Archive Agent** in the current directory of your choice, run this once:

```bash
git clone https://github.com/shredEngineer/Archive-Agent
cd Archive-Agent
chmod +x install.sh
./install.sh
```

The `install.sh` script will execute the following steps:
- Download and install `uv` (used for Python environment management)
- Install the custom Python environment
- Install the `spaCy` model for natural language processing (pre-chunking)
- Install `pandoc` (used for document parsing)
- Download and install the Qdrant docker image with persistent storage and auto-restart
- Install a global `archive-agent` command for the current user

**Archive Agent is now installed!**

ğŸ‘‰ **Please complete the [AI provider setup](#ai-provider-setup) next.**  
(Afterward, you'll be ready to [Run Archive Agent](#run-archive-agent)!)

---

## AI provider setup

**Archive Agent** lets you choose between different AI providers:

- Remote APIs *(higher performance and cost, less privacy)*:
  - **OpenAI**: Requires an OpenAI API key.


- Local APIs *(lower performance and cost, best privacy)*:
  - **Ollama**: Requires Ollama running locally.
  - **LM Studio**: Requires LM Studio running locally.

ğŸ’¡ **Good to know:** You will be prompted to choose an AI provider at startup; see: [Run Archive Agent](#run-archive-agent).

ğŸ“Œ **Note:** You *can* customize the specific **models** used by the AI provider in the [Archive Agent settings](#archive-agent-settings). However, you *cannot* change the AI provider of an *existing* profile, as the embeddings will be incompatible; to choose a different AI provider, create a new profile instead.

### OpenAI provider setup

If the OpenAI provider is selected, **Archive Agent** requires the OpenAI API key.

To export your [OpenAI API key](https://platform.openai.com/api-keys), replace `sk-...` with your actual key and run this once:

```bash
echo ""export OPENAI_API_KEY='sk-...'"" >> ~/.bashrc && source ~/.bashrc
```

This will persist the export for the current user.

ğŸ’¡ **Good to know:** [OpenAI won't use your data for training.](https://platform.openai.com/docs/guides/your-data)

### Ollama provider setup

If the Ollama provider is selected, **Archive Agent** requires Ollama running at `http://localhost:11434`.

- [How to install Ollama.](https://ollama.com/download)

With the default [Archive Agent Settings](#archive-agent-settings), these Ollama models are expected to be installed: 

```bash
ollama pull llama3.1:8b             # for chunk/rerank/query
ollama pull llava:7b-v1.6           # for vision
ollama pull nomic-embed-text:v1.5   # for embed
```

ğŸ’¡ **Good to know:** Ollama also works without a GPU.
At least 32 GiB RAM is recommended for smooth performance.

### LM Studio provider setup

If the LM Studio provider is selected, **Archive Agent** requires LM Studio running at `http://localhost:1234`.

- [How to install LM Studio.](https://lmstudio.ai/download)

With the default [Archive Agent Settings](#archive-agent-settings), these LM Studio models are expected to be installed: 

```bash
meta-llama-3.1-8b-instruct              # for chunk/rerank/query
llava-v1.5-7b                           # for vision
text-embedding-nomic-embed-text-v1.5    # for embed
```

ğŸ’¡ **Good to know:** LM Studio also works without a GPU.
At least 32 GiB RAM is recommended for smooth performance.

---

## Which files are processed

**Archive Agent** currently supports these file types:
- Text:
  - Plaintext: `.txt`, `.md`, `.markdown`
  - Documents:
    - ASCII documents: `.html`, `.htm` (images not supported)
    - Binary documents: `.odt`, `.docx` (including images)
  - PDF documents: `.pdf` (including images; see [OCR strategies](#ocr-strategies))
- Images: `.jpg`, `.jpeg`, `.png`, `.gif`, `.webp`, `.bmp`

ğŸ“Œ **Note:** Images in HTML documents are currently not supported.

ğŸ“Œ **Note:** Legacy `.doc` files are currently not supported.

ğŸ“Œ **Note:** Unsupported files are tracked but not processed.

---

## How files are processed

Ultimately, **Archive Agent** decodes everything to text like this:
- Plaintext files are decoded to UTF-8.
- Documents are converted to plaintext, images are extracted.
- PDF documents are decoded according to the OCR strategy.
- Images are decoded to text using AI vision.
  - Uses OCR, entity extraction, or both combined (default).
  - The vision model will reject unintelligible images.
  - *Entity extraction* extracts structured information from images.
  - Structured information is formatted as image description.

See [Archive Agent settings](#archive-agent-settings): `image_ocr`, `image_entity_extract`

**Archive Agent** processes files with optimized performance:
- **Surgical Synchronization**:
  - PDF analyzing phase is serialized (due to PyMuPDF threading limitations).
  - All other phases (vision, chunking, embedding) run in parallel for maximum performance.
- **Vision operations** are parallelized across images and pages within and across files.
- **Embedding operations** are parallelized across text chunks and files.
- **Smart chunking** uses sequential processing due to carry mechanism dependencies.

See [Archive Agent settings](#archive-agent-settings): `max_workers_ingest`, `max_workers_vision`, `max_workers_embed`

---

## OCR strategies

For PDF documents, there are different OCR strategies supported by **Archive Agent**:

- `strict` OCR strategy (**recommended**):
  - PDF OCR text layer is *ignored*.
  - PDF pages are treated as images and processed with OCR only.
  - **Expensive and slow, but more accurate.**

- `relaxed` OCR strategy:
  - PDF OCR text layer is extracted.
  - PDF foreground images are decoded with OCR, but background images are *ignored*.
  - **Cheap and fast, but less accurate.**

- `auto` OCR strategy:
  - Attempts to select the best OCR strategy for each page, based on the number of characters extracted from the PDF OCR text layer, if any.
  - Decides based on `ocr_auto_threshold`, the minimum number of characters for `auto` OCR strategy to resolve to `relaxed` instead of `strict`.
  - **Trade-off between cost, speed, and accuracy.**

âš ï¸ **Warning:** The `auto` OCR strategy is still experimental.
PDF documents often contain small/scattered images related to page style/layout which cause overhead while contributing little information or even cluttering the result.

ğŸ’¡ **Good to know:** You will be prompted to choose an OCR strategy at startup (see [Run Archive Agent](#run-archive-agent)).

---

## How smart chunking works

**Archive Agent** processes decoded text like this:
- Decoded text is sanitized and split into sentences.
- Sentences are grouped into reasonably-sized blocks.
- **Each block is split into smaller chunks using an AI model.**
  - Block boundaries are handled gracefully (last chunk carries over).
- Each chunk is prefixed with a *context header* (improves search).
- Each chunk is turned into a vector using AI embeddings.
- Each vector is turned into a *point* with file metadata.
- Each *point* is stored in the Qdrant database.

See [Archive Agent settings](#archive-agent-settings): `chunk_lines_block`, `chunk_words_target`

ğŸ’¡ **Good to know:** This **smart chunking** improves the accuracy and effectiveness of the retrieval.

ğŸ“Œ **Note:** In rare cases where a chunk exceeds the embedding model's token limit (typically 8192 tokens), **Archive Agent** automatically truncates it as a last resort with progressive 10% reductions (up to 10 attempts) until it fits. 

ğŸ“Œ **Note:** Splitting into sentences may take some time for huge documents.
There is currently no possibility to show the progress of this step.

---

## How chunk references work

To ensure that every chunk can be traced back to its origin, **Archive Agent** maps the text contents of each chunk to the corresponding line numbers or page numbers of the source file.

- Line-based files (e.g., `.txt`) use the range of line numbers as reference.
- Page-based files (e.g., `.pdf`) use the range of page numbers as reference.

ğŸ“Œ **Note:** References are only *approximate* due to paragraph/sentence splitting/joining in the chunking process.

---

## How chunks are retrieved

**Archive Agent** retrieves chunks related to your question like this:
- The question is turned into a vector using AI embeddings.
- Points with similar vectors are retrieved from the Qdrant database.
- Only chunks of points with sufficient score are kept.

See [Archive Agent settings](#archive-agent-settings): `retrieve_score_min`, `retrieve_chunks_max`

---

## How chunks are reranked and expanded

**Archive Agent** filters the retrieved chunks .

- Retrieved chunks are reranked by relevance to your question.
- Only the top relevant chunks are kept (the other chunks are discarded).
- Each selected chunk is expanded to get a larger context from the relevant documents.

See [Archive Agent settings](#archive-agent-settings): `rerank_chunks_max`, `expand_chunks_radius`

---

## How answers are generated

**Archive Agent** answers your question using the reranked and expanded chunks like this:
- The LLM receives the chunks as context to the question.
- LLM's answer is returned as structured output and formatted.

ğŸ’¡ **Good to know:** **Archive Agent** uses an answer template that aims to be universally helpful.

---

## How files are selected for tracking

**Archive Agent** uses *patterns* to select your files:

- Patterns can be actual file paths.
- Patterns can be paths containing wildcards that resolve to actual file paths.


- ğŸ’¡ **Patterns must be specified as (or resolve to) *absolute* paths, e.g. `/home/user/Documents/*.txt` (or `~/Documents/*.txt`).**


- ğŸ’¡ **Use the wildcard `*` to match any file in the given directory.**


- ğŸ’¡ **Use the wildcard `**` to match any files and zero or more directories, subdirectories, and symbolic links to directories.**

There are *included patterns* and *excluded patterns*:

- The set of resolved excluded files is removed from the set of resolved included files.
- Only the remaining set of files (included but not excluded) is tracked by **Archive Agent**. 
- Hidden files are always ignored!

This approach gives you the best control over the specific files or file types to track.

---

## Run Archive Agent

ğŸ’¡ **Good to know:** At startup, you will be prompted to choose the following:
- **Profile name**
- **AI provider** (see [AI Provider Setup](#ai-provider-setup))
- **OCR strategy** (see [OCR strategies](#ocr-strategies))

Screenshot of **command-line** interface (CLI):

![](archive_agent/assets/Screenshot-CLI.png)

---

## Quickstart on the command line (CLI)

For example, to [track](#how-files-are-selected-for-tracking) your documents and images, run this:

```bash
archive-agent include ""~/Documents/**"" ""~/Images/**""
archive-agent update
```

To start the GUI, run this:

```bash
archive-agent 
```

Or, to ask questions from the command line:

```bash
archive-agent query ""Which files mention donuts?""
```

---

## CLI command reference

### See list of commands

To see the list of supported commands, run this:

```bash
archive-agent
```

### Create or switch profile

To switch to a new or existing profile, run this:

```bash
archive-agent switch ""My Other Profile""
```

ğŸ“Œ **Note:** **Always use quotes** for the profile name argument,
**or skip it** to get an interactive prompt.

ğŸ’¡ **Good to know:** Profiles are useful to manage *independent* Qdrant collections (see [Qdrant database](#qdrant-database)) and [Archive Agent settings](#archive-agent-settings).

### Open current profile config in nano

To open the current profile's config (JSON) in the `nano` editor, run this:

```bash
archive-agent config
```

See [Archive Agent settings](#archive-agent-settings) for details.

### Add included patterns

To add one or more included [patterns](#how-files-are-selected-for-tracking), run this:

```bash
archive-agent include ""~/Documents/*.txt""
```

ğŸ“Œ **Note:** **Always use quotes** for the pattern argument (to prevent your shell's wildcard expansion),
**or skip it** to get an interactive prompt.

### Add excluded patterns

To add one or more excluded [patterns](#how-files-are-selected-for-tracking), run this:

```bash
archive-agent exclude ""~/Documents/*.txt""
```

ğŸ“Œ **Note:** **Always use quotes** for the pattern argument (to prevent your shell's wildcard expansion),
**or skip it** to get an interactive prompt.

### Remove included / excluded patterns

To remove one or more previously included / excluded patterns, run this:

```bash
archive-agent remove ""~/Documents/*.txt""
```

ğŸ“Œ **Note:** **Always use quotes** for the pattern argument (to prevent your shell's wildcard expansion),
**or skip it** to get an interactive prompt.

### List included / excluded patterns

To see the list of included / excluded patterns, run this: 

```bash
archive-agent patterns
```

### Resolve patterns and track files

To resolve all patterns and track changes to your files, run this:

```bash
archive-agent track
```

### List tracked files

To see the list of tracked files, run this: 

```bash
archive-agent list
```

ğŸ“Œ **Note:** Don't forget to `track` your files first.

### List changed files

To see the list of changed files, run this: 

```bash
archive-agent diff
```

ğŸ“Œ **Note:** Don't forget to `track` your files first.

### Commit changed files to database

To sync changes to your files with the Qdrant database, run this:

```bash
archive-agent commit
```

To see additional information (vision, chunking, embedding), pass the `--verbose` option.

To bypass the [AI cache](#ai-cache) (vision, chunking, embedding) for this commit, pass the `--nocache` option.

To automatically confirm deleting untracked files from the database, pass the `--confirm-delete` option.

ğŸ’¡ **Good to know:** Changes are triggered by:
- File added
- File removed
- File changed:
  - Different file size
  - Different modification date

The Qdrant database is updated after all files have been ingested. 

ğŸ“Œ **Note:** Don't forget to `track` your files first.

### Combined track and commit

To `track` and then `commit` in one go, run this:

```bash
archive-agent update
```

To see additional information (vision, chunking, embedding), pass the `--verbose` option.

To bypass the [AI cache](#ai-cache) (vision, chunking, embedding) for this commit, pass the `--nocache` option.

To automatically confirm deleting untracked files from the database, pass the `--confirm-delete` option.

### Search your files

```bash
archive-agent search ""Which files mention donuts?""
```

Lists files relevant to the question.

ğŸ“Œ **Note:** **Always use quotes** for the question argument, **or skip it** to get an interactive prompt.

To see additional information (embedding, retrieval, reranking), pass the `--verbose` option.

To bypass the [AI cache](#ai-cache) (embedding, reranking) for this search, pass the `--nocache` option.

### Query your files

```bash
archive-agent query ""Which files mention donuts?""
```

Answers your question using RAG.

ğŸ“Œ **Note:** **Always use quotes** for the question argument, **or skip it** to get an interactive prompt.

To see additional information (embedding, retrieval, reranking, querying), pass the `--verbose` option.

To bypass the [AI cache](#ai-cache) (embedding, reranking) for this query, pass the `--nocache` option.

To save the query results to a JSON file, run either:

- `--to-json` with a specific filename:
  ```bash
  archive-agent query ""Which files mention donuts?"" --to-json answer.json
  ```

- `--to-json-auto [DIR]` to auto-generate a clean filename from the question
  (max 160 chars, truncated with `[...]` if needed)
  and write to directory `DIR` if provided (defaults to current directory `.`; creates directories in path if not existing):
  ```bash
  archive-agent query ""Which files mention donuts?"" --to-json-auto Output/
  # Creates: Output/Which_files_mention_donuts_.json
  ```

ğŸ“Œ **Note:** As of **Archive Agent** v12.2.0, a corresponding Markdown file (`.md`) containing the answer is also created when using the `--to-json` or `--to-json-auto` options. (There is currently no way to opt out of this.)   

### Launch Archive Agent GUI

To launch the **Archive Agent** GUI in your browser, run this:

```bash
archive-agent gui
```

To see additional information (embedding, retrieval, reranking, querying), pass the `--verbose` option.

To bypass the [AI cache](#ai-cache) (embedding, reranking) for this query, pass the `--nocache` option.

To save the query results to JSON files, run this:

- `--to-json-auto [DIR]` to auto-generate clean filenames from the questions
  (max 160 chars, truncated with `[...]` if needed)
  and write to directory `DIR` if provided (defaults to current directory `.`; creates directories in path if not existing):
  ```bash
  archive-agent gui --to-json-auto Output/
  ```

ğŸ“Œ **Note:** As of **Archive Agent** v12.2.0, corresponding Markdown files (`.md`) containing the answers are also created when using the `--to-json-auto` option. (There is currently no way to opt out of this.)   

ğŸ“Œ **Note:** Press `CTRL+C` in the console to close the GUI server.

### Start MCP Server

To start the **Archive Agent** MCP server, run this:

```bash
archive-agent mcp
```

To see additional information (embedding, retrieval, reranking, querying), pass the `--verbose` option.

To bypass the [AI cache](#ai-cache) (embedding, reranking) for this query, pass the `--nocache` option.

To save the query results to JSON files, run this:

- `--to-json-auto [DIR]` to auto-generate clean filenames from the questions
  (max 160 chars, truncated with `[...]` if needed)
  and write to directory `DIR` if provided (defaults to current directory `.`; creates directories in path if not existing):
  ```bash
  archive-agent mcp --to-json-auto Output/
  ```

ğŸ“Œ **Note:** As of **Archive Agent** v12.2.0, corresponding Markdown files (`.md`) containing the answers are also created when using the `--to-json-auto` option. (There is currently no way to opt out of this.)

ğŸ“Œ **Note:** Press `CTRL+C` in the console to close the MCP server.

ğŸ’¡ **Good to know:** Use these MCP configurations to let your IDE or AI extension automate **Archive Agent**:

- [`.vscode/mcp.json`](.vscode/mcp.json) for [GitHub Copilot agent mode (VS Code)](https://code.visualstudio.com/blogs/2025/02/24/introducing-copilot-agent-mode): 
- [`.roo/mcp.json`](.roo/mcp.json) for [Roo Code (VS Code extension)](https://marketplace.visualstudio.com/items?itemName=RooVeterinaryInc.roo-cline)

---

## MCP Tools

**Archive Agent** exposes these tools via MCP:

| MCP tool            | Equivalent CLI command(s) | Argument(s) | Implementation | Description                                     |
|---------------------|---------------------------|-------------|----------------|-------------------------------------------------|
| `get_patterns`      | `patterns`                | None        | Synchronous    | Get the list of included / excluded patterns.   |
| `get_files_tracked` | `track` and then `list`   | None        | Synchronous    | Get the list of tracked files.                  |
| `get_files_changed` | `track` and then `diff`   | None        | Synchronous    | Get the list of changed files.                  |
| `get_search_result` | `search`                  | `question`  | Asynchronous   | Get the list of files relevant to the question. |
| `get_answer_rag`    | `query`                   | `question`  | Asynchronous   | Get answer to question using RAG.               |

ğŸ“Œ **Note:** These commands are **read-only**, preventing the AI from changing your Qdrant database.

ğŸ’¡ **Good to know:** Just type `#get_answer_rag` (e.g.) in your IDE or AI extension to call the tool directly.

ğŸ’¡ **Good to know:** The `#get_answer_rag` output follows the `QuerySchema` format defined in [`AiQuery.py`](archive_agent/ai/query/AiQuery.py).

---

## Update Archive Agent

This step is not immediately needed if you just installed Archive Agent.
However, to get the latest features, you should update your installation regularly.

To update your **Archive Agent** installation, run this in the installation directory:

```bash
./update.sh
```

ğŸ“Œ **Note:** If updating doesn't work, try removing the installation directory and then [Install Archive Agent](#install-archive-agent) again.
Your config and data are safely stored in another place;
see [Archive Agent settings](#archive-agent-settings) and [Qdrant database](#qdrant-database) for details.

ğŸ’¡ **Good to know:** To also update the Qdrant docker image, run this:

```bash
sudo ./manage-qdrant.sh update
```

---

### Archive Agent settings

**Archive Agent** settings are organized as profile folders in `~/.archive-agent-settings/`.

E.g., the `default` profile is located in `~/.archive-agent-settings/default/`.

The currently used profile is stored in `~/.archive-agent-settings/profile.json`.

ğŸ“Œ **Note:** To delete a profile, simply delete the profile folder.
This will not delete the Qdrant collection (see [Qdrant database](#qdrant-database)).

#### Profile configuration

The profile configuration is contained in the profile folder as `config.json`.

ğŸ’¡ **Good to know:** Use the `config` CLI command to open the current profile's config (JSON) in the `nano` editor (see [Open current profile config in nano](#open-current-profile-config-in-nano)).

ğŸ’¡ **Good to know:** Use the `switch` CLI command to switch to a new or existing profile (see [Create or switch profile](#create-or-switch-profile)).

| Key                    | Description                                                                                      |
|------------------------|--------------------------------------------------------------------------------------------------|
| `config_version`       | Config version                                                                                   |
| `mcp_server_host`      | MCP server host (default `http://127.0.0.1`; set to `http://0.0.0.0` to expose in LAN)           |
| `mcp_server_port`      | MCP server port (default `8008`)                                                                 |
| `ocr_strategy`         | OCR strategy in [`DecoderSettings.py`](archive_agent/config/DecoderSettings.py)                  |
| `ocr_auto_threshold`   | Minimum number of characters for `auto` OCR strategy to resolve to `relaxed` instead of `strict` |
| `image_ocr`            | Image handling: `true` enables OCR, `false` disables it.                                         |
| `image_entity_extract` | Image handling: `true` enables entity extraction, `false` disables it.                           |
| `chunk_lines_block`    | Number of lines per block for chunking                                                           |
| `chunk_words_target`   | Target number of words per chunk                                                                 |
| `qdrant_server_url`    | URL of the Qdrant server                                                                         |
| `qdrant_collection`    | Name of the Qdrant collection                                                                    |
| `retrieve_score_min`   | Minimum similarity score of retrieved chunks (`0`...`1`)                                         |
| `retrieve_chunks_max`  | Maximum number of retrieved chunks                                                               |
| `rerank_chunks_max`    | Number of top chunks to keep after reranking                                                     |
| `expand_chunks_radius` | Number of preceding and following chunks to prepend and append to each reranked chunk            |
| `max_workers_ingest`   | Maximum number of files to process in parallel, creating one thread for each file                |
| `max_workers_vision`   | Maxmimum number of parallel vision requests **per file**, creating one thread per request        |
| `max_workers_embed`    | Maxmimum number of parallel embedding requests **per file**, creating one thread per request     |
| `ai_provider`          | AI provider in [`ai_provider_registry.py`](archive_agent/ai_provider/ai_provider_registry.py)    |
| `ai_server_url`        | AI server URL                                                                                    |
| `ai_model_chunk`       | AI model used for chunking                                                                       |
| `ai_model_embed`       | AI model used for embedding                                                                      |
| `ai_model_rerank`      | AI model used for reranking                                                                      |
| `ai_model_query`       | AI model used for queries                                                                        |
| `ai_model_vision`      | AI model used for vision (`""""` disables vision)                                                  |
| `ai_vector_size`       | Vector size of embeddings (used for Qdrant collection)                                           |
| `ai_temperature_query` | Temperature of the query model (ignored for GPT-5)                                               |

ğŸ“Œ **Note:** When using GPT-5 (default as of **Archive Agent** v14.0.0), `ai_temperature_query` is ignored.
GPT-5 reasoning effort and verbosity are currently not available in the configuration,
but may be customized directly inside `OpenAiProvider.py`.

ğŸ“Œ **Note:** Since `max_workers_vision` and `max_workers_embed` requests are processed in parallel **per file**,
and `max_workers_ingest` files are processed in parallel, the total number of requests multiplies quickly.
Adjust according to your system resources and in alignment with your AI provider's rate limits.

### Watchlist

The profile watchlist is contained in the profile folder as `watchlist.json`.

The watchlist is managed by these commands only:

- `include` / `exclude` / `remove`
- `track` / `commit` / `update`

### AI cache

Each profile folder also contains an `ai_cache` folder.

The AI cache ensures that, in a given profile:
- The same image is only OCR-ed once.
- The same text is only chunked once.
- The same text is only embedded once.
- The same combination of chunks is only reranked once.

This way, **Archive Agent** can quickly resume where it left off if a commit was interrupted.

To bypass the AI cache for a single commit, pass the `--nocache` option to the `commit` or `update` command
(see [Commit changed files to database](#commit-changed-files-to-database) and [Combined track and commit](#combined-track-and-commit)).

ğŸ’¡ **Good to know:** Queries are never cached, so you always get a fresh answer. 

ğŸ“Œ **Note:** To clear the entire AI cache, simply delete the profile's cache folder.

ğŸ“Œ **Technical Note:** **Archive Agent** keys the cache using a composite hash made from the text/image bytes, and of the AI model names for chunking, embedding, reranking, and vision.
Cache keys are deterministic and change generated whenever you change the *chunking*, *embedding* or *vision* AI model names.
Since cache entries are retained forever, switching back to a prior combination of AI model names will again access the ""old"" keys.  

---

## Qdrant database

The [Qdrant](https://qdrant.tech/) database is stored in `~/.archive-agent-qdrant-storage/`.

ğŸ“Œ **Note:** This folder is created by the Qdrant Docker image running as root.

ğŸ’¡ **Good to know:** Visit your [Qdrant dashboard](http://localhost:6333/dashboard#/collections) to manage collections and snapshots.

---

## Developer's guide

**Archive Agent** was written from scratch for educational purposes (on either end of the software).

ğŸ’¡ **Good to know:** Tracking the `test_data/` gets you started with *some* kind of test data. 

### Important modules

To get started, check out these epic modules:

- Files are processed in [`archive_agent/data/FileData.py`](archive_agent/data/FileData.py)
- The app context is initialized in [`archive_agent/core/ContextManager.py`](archive_agent/core/ContextManager.py)
- The default config is defined in [`archive_agent/config/ConfigManager.py`](archive_agent/config/ConfigManager.py)  
- The CLI commands are defined in [`archive_agent/__main__.py`](archive_agent/__main__.py)
- The commit logic is implemented in [`archive_agent/core/CommitManager.py`](archive_agent/core/CommitManager.py)
- The CLI verbosity is handled in [`archive_agent/util/CliManager.py`](archive_agent/core/CliManager.py)
- The GUI is implemented in [`archive_agent/core/GuiManager.py`](archive_agent/core/GuiManager.py)
- The AI API prompts for chunking, embedding, vision, and querying are defined in [`archive_agent/ai/AiManager.py`](archive_agent/ai/AiManager.py) 
- The AI provider registry is located in [`archive_agent/ai_provider/ai_provider_registry.py`](archive_agent/ai_provider/ai_provider_registry.py)

If you miss something or spot bad patterns, feel free to contribute and refactor!

### Network and Retry Handling

Archive Agent implements comprehensive retry logic with exponential backoff to handle transient failures:

- **AI Provider Operations**: 10 retries with exponential backoff (max 60s delay) for network timeouts and API errors.
- **Database Operations**: 10 retries with exponential backoff (max 10s delay) for Qdrant connection issues  .
- **Schema Validation**: Additional 10 linear retry attempts for AI response parsing failures with cache invalidation.
- **Dual-Layer Strategy**: Network-level retries handle infrastructure failures, while schema-level retries handle AI response quality issues.

This robust retry system ensures reliable operation even with unstable network conditions or intermittent service issues.

### Code testing and analysis

To run unit tests, check types, and check style, run this:

```bash
./audit.sh
```

### Run Qdrant with in-memory storage

To run Qdrant with in-memory storage (e.g., in OpenAI Codex environment where Docker is not available),
export this environment variable before running `install.sh` and `archive-agent`:

```bash
export ARCHIVE_AGENT_QDRANT_IN_MEMORY=1
``` 

- The environment variable is checked by `install.sh` to skip `manage-qdrant.sh`.
- The environment variable is checked by `QdrantManager.py` to ignore server URL and use in-memory storage instead.

ğŸ“Œ **Note:** Qdrant in-memory storage is volatile (not persisted to disk).

---

## Tools

### Rename file paths in chunk metadata

To bulk-rename file paths in chunk metadata in the currently active Qdrant collection, run this:

```bash
cd tools/

./qdrant-rename-paths-in-chunk-metadata.py
# OR
uv run python qdrant-rename-paths-in-chunk-metadata.py
```

Useful after moving files or renaming folders when you don't want to run the `update` command again.

ğŸ“Œ **Note:**
- This tool modifies the Qdrant database directly â€” **ensure you have backups if working with critical data**.
- This tool will **not** update the tracked files. You need to update your watchlist (see [Archive Agent settings](#archive-agent-settings)) using manual search and replace.

### Remove file paths from context headers

**Archive Agent** < v11.0.0 included file paths in the chunk context headers; this was a bad design decision that led to skewed retrieval.

To bulk-remove all file paths in context headers in the currently active Qdrant collection, run this:

```bash
cd tools/

./qdrant-remove-paths-from-chunk-headers.py
# OR
uv run python qdrant-remove-paths-from-chunk-headers.py
```

ğŸ“Œ **Note:**
- This tool modifies the Qdrant database directly â€” **ensure you have backups if working with critical data**.

---

## Known issues

- [ ] While `track` initially reports a file as *added*, subsequent `track` calls report it as *changed*. 


- [ ] Removing and restoring a tracked file in the tracking phase is currently not handled properly:
  - Removing a tracked file sets `{size=0, mtime=0, diff=removed}`.
  - Restoring a tracked file sets `{size=X, mtime=Y, diff=added}`.
  - Because `size` and `mtime` were cleared, we lost the information to detect a restored file.


- [ ] Unprocessable files are tracked in the watchlist (and attempted to be deleted from the Qdrant database when untracked again)


- [ ] Unprocessable files are shown in the final statistics as being ""updated in Qdrant database""


- [ ] AI vision is employed on empty images as well, even though they could be easily detected locally and skipped. 


- [ ] PDF vector images may not convert as expected, due to missing tests. (Using `strict` OCR strategy would certainly help in the meantime.) 


- [ ] Binary document page numbers (e.g., `.docx`) are not supported yet; Microsoft Word document support is experimental.


- [ ] References are only *approximate* due to paragraph/sentence splitting/joining in the chunking process.


- [ ] AI cache does not handle `AiResult` schema migration yet. (If you encounter errors, passing the `--nocache` flag or deleting all AI cache folders would certainly help in the meantime.)


- [ ] Rejected images (e.g., due to OpenAI content filter policy violation) from PDF pages in `strict` OCR mode are currently left empty instead of resorting to text extracted from PDF OCR layer (if any).


- [ ] The spaCy model `en_core_web_md` used for sentence splitting is only suitable for English source text. Multilingual support is missing at the moment.


- [ ] HTML document images are not supported.


- [ ] The behaviour of handling unprocessable files is not customizable yet. Should the user be prompted? Should the entire file be rejected? **Unprocessable images are currently tolerated and replaced by `[Unprocessable image]`.** 


- [ ] GPT-5 reasoning effort and verbosity are currently not available in the configuration.


- [ ] GUI sometimes doesn't react on first button click, needs a second one. (Should migrate to use NiceGUI instead of Streamlit.)

---

## Licensed under GNU GPL v3.0

Copyright Â© 2025 Dr.-Ing. Paul Wilhelm <[paul@wilhelm.dev](mailto:paul@wilhelm.dev)>

```
This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.
```

See [LICENSE](LICENSE) for details.

---

## Collaborators welcome
You are invited to contribute to this open source project! Feel free to [file issues](https://github.com/shredEngineer/Archive-Agent/issues) and [submit pull requests](https://github.com/shredEngineer/Archive-Agent/pulls) anytime.

---

## Learn about Archive Agent

([Tap to open external link to YouTube playlist](https://youtube.com/playlist?list=PLK0uOAnKcdRQSYnV32GHqV8ZAnKstJ-GU&si=QH4MfEmreVEXoa-q))  
[![YouTube: Archive Agent (Playlist)](archive_agent/assets/Thumbnail-GOCCWwI25EI.jpg)](https://youtube.com/playlist?list=PLK0uOAnKcdRQSYnV32GHqV8ZAnKstJ-GU&si=QH4MfEmreVEXoa-q)  
",58
https://mcp.so/server/Arrodes-2.0/aldindugolli,,404,,,,,,,
https://mcp.so/server/Arxivloader-MCP-Server-and-Client/alihassanml,https://github.com/alihassanml/Arxivloader-MCP-Server-and-Client,Arxivloader MCP Server and Client,"This project provides an MCP (Microservice Communication Protocol) server and client setup for fetching research papers from the arXiv. The MCP server processes queries and fetches relevant research papers, while the client communicates with the server using the MCP protocol.",English,research-and-data,arxiv; mcp; research-papers,"what is Arxivloader MCP Server and Client? Arxivloader MCP Server and Client is a project that provides a Microservice Communication Protocol (MCP) server and client setup for fetching research papers from the arXiv repository. The server processes queries and retrieves relevant research papers, while the client communicates with the server using the MCP protocol. how to use Arxivloader MCP Server and Client? To use the project, clone the repository, install the required dependencies, start the MCP server, and run the Streamlit client interface to input queries for research papers. key features of Arxivloader MCP Server and Client? Efficient MCP communication between server and client. Retrieval of research papers from arXiv based on user queries. User-friendly Streamlit UI for inputting queries and displaying results. Integration with tools like Groq and LangChain for enhanced query handling. use cases of Arxivloader MCP Server and Client? Fetching research papers on specific topics or queries. Assisting researchers in finding relevant literature from arXiv. Providing a simple interface for users to explore research papers. FAQ from Arxivloader MCP Server and Client? What programming language is used for this project? The project is developed in Python. Is there a user interface for this project? Yes, it uses Streamlit to provide a web interface for users. How can I contribute to this project? You can fork the repository and submit pull requests for any improvements or bug fixes.","# Arxivloader MCP Server and Client

This project provides an MCP (Microservice Communication Protocol) server and client setup for fetching research papers from the [arXiv](https://arxiv.org/). The MCP server processes queries and fetches relevant research papers, while the client communicates with the server using the MCP protocol. This project uses `Streamlit` for a simple UI and integrates tools such as `LangChain`, `Groq`, and `MCP` for communication.

## Features

- **MCP Communication**: Efficient communication between server and client using MCP protocol.
- **Research Paper Retrieval**: Fetches research papers from arXiv based on user queries.
- **Streamlit UI**: A simple web interface to input queries and display research paper results.
- **Tool Integration**: Uses Groq and LangChain for enhanced query handling and processing.

## Requirements

- Python 3.8 or higher
- `streamlit`
- `mcp`
- `langchain`
- `langchain_groq`
- `dotenv`
- `asyncio`

You can install the required dependencies using the following:

```bash
pip install -r requirements.txt
```

## Installation

1. Clone this repository:

```bash
git clone https://github.com/alihassanml/Arxivloader-MCP-Server-and-Client.git
cd Arxivloader-MCP-Server-and-Client
```

2. Install the necessary dependencies:

```bash
pip install -r requirements.txt
```

3. Set up the `.env` file with any required configuration for Groq and MCP settings.

## Usage

1. **Start the MCP Server**:

   Run the server script to start the MCP server:

   ```bash
   python arxivloader.py
   ```

2. **Run the Client with Streamlit**:

   Start the Streamlit client interface to interact with the server:

   ```bash
   streamlit run client.py
   ```

3. **Query Research Papers**:

   Once the client interface is running, you can enter the name of a research paper or a query in the input field. The server will fetch relevant papers and display them.

## Example

Here is an example of how the system works:

1. The user enters a query like ""Medical Claim Processing OR Health Insurance Billing"" into the Streamlit interface.
2. The MCP server processes the query and fetches relevant research paper data from arXiv.
3. The client displays the fetched paper details, such as title, authors, and publication date.

## Project Structure

```
.
â”œâ”€â”€ arxivloader.py          # MCP server code for handling queries and fetching research papers
â”œâ”€â”€ client.py               # Streamlit client UI to interact with the server
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ .env                    # Environment variables for configuration
â””â”€â”€ README.md               # Project documentation
```

## Contributing

Feel free to fork this repository and contribute to the project. If you find any bugs or have feature requests, please open an issue.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
",1
https://mcp.so/server/AshDevFr_discourse-mcp-server/MCP-Mirror,https://github.com/MCP-Mirror/AshDevFr_discourse-mcp-server,Discourse MCP Server,Mirror of,English,developer-tools,,"what is Discourse MCP Server? Discourse MCP Server is a Node.js server that implements the Model Context Protocol (MCP) for searching posts on a Discourse forum. how to use Discourse MCP Server? To use the Discourse MCP Server, you can run it using Docker or NPX by configuring the necessary API parameters in your setup. key features of Discourse MCP Server? Enables searching of posts on a Discourse forum using the MCP protocol. Provides an API tool for searching posts with a simple query input. use cases of Discourse MCP Server? Integrating Discourse forum search functionality into applications. Enhancing user experience by allowing quick access to forum posts. Automating the retrieval of relevant discussions from Discourse forums. FAQ from Discourse MCP Server? What is the MCP protocol? The Model Context Protocol (MCP) is a protocol designed for efficient search operations in forums like Discourse. How do I set up the server? You can set up the server using Docker or NPX by following the provided configuration examples. Is there a license for this project? Yes, the Discourse MCP Server is licensed under the MIT license.","# Discourse MCP Server

Node.js server implementing Model Context Protocol (MCP) for Discourse search operation.

## Features

- Search Posts on a Discourse forum using MCP protocol.

## API

### Tools

- **search_posts**
    - Search posts on a Discourse forum
    - Input: `query` (string)
    - Returns an array of post objects

## Usage with Claude Desktop
Add this to your `claude_desktop_config.json`:

### Docker

```json
{
  ""mcpServers"": {
    ""discourse"": {
      ""command"": ""docker"",
      ""args"": [
        ""run"",
        ""-i"",
        ""--rm"",
        ""-e"", ""DISCOURSE_API_URL=https://try.discourse.org"",
        ""-e"", ""DISCOURSE_API_KEY=1234"",
        ""-e"", ""DISCOURSE_API_USERNAME=ash"",
        ""ashdev/discourse-mcp-server""
      ]
    }
  }
}
```

### NPX

```json
{
  ""mcpServers"": {
    ""discourse"": {
      ""command"": ""npx"",
      ""args"": [
        ""-y"",
        ""@ashdev/discourse-mcp-server""
      ],
      ""env"": {
        ""DISCOURSE_API_URL"": ""https://try.discourse.org"",
        ""DISCOURSE_API_KEY"": ""1234"",
        ""DISCOURSE_API_USERNAME"": ""ash"" 
      }
    }
  }
}
```

## Build

Docker build:

```bash
docker build -t ashdev/discourse-mcp-server .
```
",0
https://mcp.so/server/AssetPriceMCP/mk965,,,Home Servers Clients Categories Tags Feed,English,,,,,
https://mcp.so/server/Atla/atla-ai,https://github.com/atla-ai/atla-mcp-server,Atla MCP Server,An MCP server implementation providing a standardized interface for LLMs to interact with the Atla API for state-of-the-art LLMJ evaluation.,English,,atla; atla-ai; llmj; evals; python,The Atla MCP Server provides a standardized interface for LLMs to interact with the Atla API for state-of-the-art LLMJ evaluation. Learn more about Atla here !,"# Atla MCP Server

> [!CAUTION]
> This repository was archived on July 21, 2025. The Atla API is no longer active.

An MCP server implementation providing a standardized interface for LLMs to interact with the Atla API for state-of-the-art LLMJ evaluation.

> Learn more about Atla [here](https://docs.atla-ai.com). Learn more about the Model Context Protocol [here](https://modelcontextprotocol.io).

<a href=""https://glama.ai/mcp/servers/@atla-ai/atla-mcp-server"">
  <img width=""380"" height=""200"" src=""https://glama.ai/mcp/servers/@atla-ai/atla-mcp-server/badge"" alt=""Atla MCP server"" />
</a>

## Available Tools

- `evaluate_llm_response`: Evaluate an LLM's response to a prompt using a given evaluation criteria. This function uses an Atla evaluation model under the hood to return a dictionary containing a score for the model's response and a textual critique containing feedback on the model's response.
- `evaluate_llm_response_on_multiple_criteria`: Evaluate an LLM's response to a prompt across _multiple_ evaluation criteria. This function uses an Atla evaluation model under the hood to return a list of dictionaries, each containing an evaluation score and critique for a given criteria.

## Usage

> To use the MCP server, you will need an Atla API key. You can find your existing API key [here](https://www.atla-ai.com/sign-in) or create a new one [here](https://www.atla-ai.com/sign-up).

### Installation

> We recommend using `uv` to manage the Python environment. See [here](https://docs.astral.sh/uv/getting-started/installation/) for installation instructions.

### Manually running the server

Once you have `uv` installed and have your Atla API key, you can manually run the MCP server using `uvx` (which is provided by `uv`):

```bash
ATLA_API_KEY=<your-api-key> uvx atla-mcp-server
```

### Connecting to the server

> Having issues or need help connecting to another client? Feel free to open an issue or [contact us](mailto:support@atla-ai.com)!

#### OpenAI Agents SDK

> For more details on using the OpenAI Agents SDK with MCP servers, refer to the [official documentation](https://openai.github.io/openai-agents-python/).

1. Install the OpenAI Agents SDK:

```shell
pip install openai-agents
```

2. Use the OpenAI Agents SDK to connect to the server:

```python
import os

from agents import Agent
from agents.mcp import MCPServerStdio

async with MCPServerStdio(
        params={
            ""command"": ""uvx"",
            ""args"": [""atla-mcp-server""],
            ""env"": {""ATLA_API_KEY"": os.environ.get(""ATLA_API_KEY"")}
        }
    ) as atla_mcp_server:
    ...
```

#### Claude Desktop

> For more details on configuring MCP servers in Claude Desktop, refer to the [official MCP quickstart guide](https://modelcontextprotocol.io/quickstart/user).

1. Add the following to your `claude_desktop_config.json` file:

```json
{
  ""mcpServers"": {
    ""atla-mcp-server"": {
      ""command"": ""uvx"",
      ""args"": [""atla-mcp-server""],
      ""env"": {
        ""ATLA_API_KEY"": ""<your-atla-api-key>""
      }
    }
  }
}
```

2. **Restart Claude Desktop** to apply the changes.

You should now see options from `atla-mcp-server` in the list of available MCP tools.

#### Cursor

> For more details on configuring MCP servers in Cursor, refer to the [official documentation](https://docs.cursor.com/context/model-context-protocol).

1. Add the following to your `.cursor/mcp.json` file:

```json
{
  ""mcpServers"": {
    ""atla-mcp-server"": {
      ""command"": ""uvx"",
      ""args"": [""atla-mcp-server""],
      ""env"": {
        ""ATLA_API_KEY"": ""<your-atla-api-key>""
      }
    }
  }
}
```

You should now see `atla-mcp-server` in the list of available MCP servers.

## Contributing

Contributions are welcome! Please see the [CONTRIBUTING.md](CONTRIBUTING.md) file for details.

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.
",17
https://mcp.so/server/Atomzzm_mcp-mysql-server/MCP-Mirror,https://github.com/MCP-Mirror/Atomzzm_mcp-mysql-server,@f4ww4z/mcp-mysql-server,Mirror of,English,research-and-data,mcp-mysql-server; database; AI-integration,"what is @f4ww4z/mcp-mysql-server? @f4ww4z/mcp-mysql-server is a Model Context Protocol server that facilitates MySQL database operations, allowing AI models to interact with MySQL databases through a standardized interface. how to use @f4ww4z/mcp-mysql-server? To use the server, you can install it via Smithery or manually using the provided commands. Configuration requires setting specific environment variables in your MCP settings. key features of @f4ww4z/mcp-mysql-server? Secure connection handling with automatic cleanup Prepared statement support for executing queries Comprehensive error handling and validation TypeScript support for developers Automatic connection management use cases of @f4ww4z/mcp-mysql-server? Connecting AI models to MySQL databases for data retrieval and manipulation. Executing complex SQL queries through a standardized interface. Integrating database operations into AI-driven applications. FAQ from @f4ww4z/mcp-mysql-server? Is this server secure? Yes! It uses prepared statements to prevent SQL injection and supports secure password handling. Can I contribute to this project? Absolutely! Contributions are welcome, and you can submit a Pull Request on GitHub. What programming languages does it support? The server is designed with TypeScript support, making it suitable for JavaScript developers.","# @f4ww4z/mcp-mysql-server
[![smithery badge](https://smithery.ai/badge/@f4ww4z/mcp-mysql-server)](https://smithery.ai/server/@f4ww4z/mcp-mysql-server)

A Model Context Protocol server that provides MySQL database operations. This server enables AI models to interact with MySQL databases through a standardized interface.

<a href=""https://glama.ai/mcp/servers/qma33al6ie""><img width=""380"" height=""200"" src=""https://glama.ai/mcp/servers/qma33al6ie/badge"" alt=""mcp-mysql-server MCP server"" /></a>

## Installation

### Installing via Smithery

To install MySQL Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@f4ww4z/mcp-mysql-server):

```bash
npx -y @smithery/cli install @f4ww4z/mcp-mysql-server --client claude
```

### Manual Installation
```bash
npx @f4ww4z/mcp-mysql-server
```

## Configuration

The server requires the following environment variables to be set in your MCP settings configuration file:

```json
{
  ""mcpServers"": {
    ""mysql"": {
      ""command"": ""npx"",
      ""args"": [""-y"", ""@f4ww4z/mcp-mysql-server""],
      ""env"": {
        ""MYSQL_HOST"": ""your_host"",
        ""MYSQL_USER"": ""your_user"",
        ""MYSQL_PASSWORD"": ""your_password"",
        ""MYSQL_DATABASE"": ""your_database""
      }
    }
  }
}
```

## Available Tools

### 1. connect_db
Establish connection to MySQL database using provided credentials.

```typescript
use_mcp_tool({
  server_name: ""mysql"",
  tool_name: ""connect_db"",
  arguments: {
    host: ""localhost"",
    user: ""your_user"",
    password: ""your_password"",
    database: ""your_database""
  }
});
```

### 2. query
Execute SELECT queries with optional prepared statement parameters.

```typescript
use_mcp_tool({
  server_name: ""mysql"",
  tool_name: ""query"",
  arguments: {
    sql: ""SELECT * FROM users WHERE id = ?"",
    params: [1]
  }
});
```

### 3. execute
Execute INSERT, UPDATE, or DELETE queries with optional prepared statement parameters.

```typescript
use_mcp_tool({
  server_name: ""mysql"",
  tool_name: ""execute"",
  arguments: {
    sql: ""INSERT INTO users (name, email) VALUES (?, ?)"",
    params: [""John Doe"", ""john@example.com""]
  }
});
```

### 4. list_tables
List all tables in the connected database.

```typescript
use_mcp_tool({
  server_name: ""mysql"",
  tool_name: ""list_tables"",
  arguments: {}
});
```

### 5. describe_table
Get the structure of a specific table.

```typescript
use_mcp_tool({
  server_name: ""mysql"",
  tool_name: ""describe_table"",
  arguments: {
    table: ""users""
  }
});
```

## Features

- Secure connection handling with automatic cleanup
- Prepared statement support for query parameters
- Comprehensive error handling and validation
- TypeScript support
- Automatic connection management

## Security

- Uses prepared statements to prevent SQL injection
- Supports secure password handling through environment variables
- Validates queries before execution
- Automatically closes connections when done

## Error Handling

The server provides detailed error messages for common issues:
- Connection failures
- Invalid queries
- Missing parameters
- Database errors

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request to https://github.com/f4ww4z/mcp-mysql-server

## License

MIT
",0
https://mcp.so/server/Audio-MCP-Server/GongRzhe,https://github.com/GongRzhe/Audio-MCP-Server,Audio MCP Server,,,developer-tools,audio-mcp; audio-server; ai-assistant,"what is Audio MCP Server? Audio MCP Server is a Model Context Protocol server that provides audio input/output capabilities for AI assistants like Claude, enabling interaction with your computer's audio system. how to use Audio MCP Server? To use the Audio MCP Server, clone the repository, set up a virtual environment, install dependencies, and configure it with Claude Desktop. After setup, you can interact with the server through Claude by asking it to list audio devices, record audio, or play audio files. key features of Audio MCP Server? List available audio devices on your system Record audio from microphones with customizable settings Playback of recent recordings and audio files Future implementation of text-to-speech functionality use cases of Audio MCP Server? Recording audio for transcription or analysis Playing back audio for review or testing Integrating audio capabilities into AI assistant workflows FAQ from Audio MCP Server? What are the system requirements? Python 3.8 or higher and audio input/output devices are required. How do I configure the server with Claude Desktop? You need to add specific configuration settings to the Claude Desktop configuration file based on your operating system. What should I do if no audio devices are found? Ensure your devices are connected, recognized by the OS, and that you have the necessary permissions.","# Audio MCP Server
[![smithery badge](https://smithery.ai/badge/@GongRzhe/Audio-MCP-Server)](https://smithery.ai/server/@GongRzhe/Audio-MCP-Server)

An MCP (Model Context Protocol) server that provides audio input/output capabilities for AI assistants like Claude. This server enables Claude to interact with your computer's audio system, including recording from microphones and playing audio through speakers.



## Features

- **List Audio Devices**: View all available microphones and speakers on your system
- **Record Audio**: Capture audio from any microphone with customizable duration and quality
- **Playback Recordings**: Play back your most recent recording
- **Audio File Playback**: Play audio files through your speakers
- **Text-to-Speech**: (Placeholder for future implementation)

## Requirements

- Python 3.8 or higher
- Audio input/output devices on your system

## Installation

### Installing via Smithery

To install Audio Interface Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@GongRzhe/Audio-MCP-Server):

```bash
npx -y @smithery/cli install @GongRzhe/Audio-MCP-Server --client claude
```

### Manual Installation
1. Clone this repository or download the files to your computer:

```bash
git clone https://github.com/GongRzhe/Audio-MCP-Server.git
cd Audio-MCP-Server
```

2. Create a virtual environment and install dependencies:

```bash
# Windows
python -m venv .venv
.venv\Scripts\activate
pip install -r requirements.txt

# macOS/Linux
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

3. Or use the included setup script to automate installation:

```bash
python setup_mcp.py
```

## Configuration

### Claude Desktop Configuration

To use this server with Claude Desktop, add the following to your Claude Desktop configuration file:

- **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`
- **Windows**: `%APPDATA%\Claude\claude_desktop_config.json`

```json
{
  ""mcpServers"": {
    ""audio-interface"": {
      ""command"": ""/path/to/your/.venv/bin/python"",
      ""args"": [
        ""/path/to/your/audio_server.py""
      ],
      ""env"": {
        ""PYTHONPATH"": ""/path/to/your/audio-mcp-server""
      }
    }
  }
}
```

Replace the paths with the actual paths on your system. The setup script will generate this configuration for you.

## Usage

After setting up the server, restart Claude Desktop. You should see a hammer icon in the input box, indicating that tools are available.

Try asking Claude:

- ""What microphones and speakers are available on my system?""
- ""Record 5 seconds of audio from my microphone.""
- ""Play back the audio recording.""
- ""Play an audio file from my computer.""

## Available Tools

### list_audio_devices

Lists all available audio input and output devices on your system.

### record_audio

Records audio from your microphone.

Parameters:
- `duration`: Recording duration in seconds (default: 5)
- `sample_rate`: Sample rate in Hz (default: 44100)
- `channels`: Number of audio channels (default: 1)
- `device_index`: Specific input device index to use (default: system default)

### play_latest_recording

Plays back the most recently recorded audio.

### play_audio

Placeholder for text-to-speech functionality.

Parameters:
- `text`: The text to convert to speech
- `voice`: The voice to use (default: ""default"")

### play_audio_file

Plays an audio file through your speakers.

Parameters:
- `file_path`: Path to the audio file
- `device_index`: Specific output device index to use (default: system default)

## Troubleshooting

### No devices found

If no audio devices are found, check:
- Your microphone and speakers are properly connected
- Your operating system recognizes the devices
- You have the necessary permissions to access audio devices

### Playback issues

If audio playback isn't working:
- Check your volume settings
- Ensure the correct output device is selected
- Try restarting the Claude Desktop application

### Server connectivity

If Claude can't connect to the server:
- Verify your configuration paths are correct
- Ensure Python and all dependencies are installed
- Check Claude's logs for error messages

## License

MIT

## Acknowledgments

- Built using the [Model Context Protocol](https://modelcontextprotocol.io/)
- Uses [sounddevice](https://python-sounddevice.readthedocs.io/) and [soundfile](https://pysoundfile.readthedocs.io/) for audio processing

---

*Note: This server provides tools that can access your microphone and speakers. Always review and approve tool actions before they execute.*
",5
https://mcp.so/server/Augmented-Nature-OpenTargets-MCP-Server/Augmented-Nature,https://github.com/Augmented-Nature/Augmented-Nature-OpenTargets-MCP-Server,Unofficial Open Targets MCP Server ğŸ§¬,Unofficial Model Context Protocol server for accessing Open Targets platform data for gene-drug-disease associations research.,English,research-and-data,gene-drug-disease; open-targets; mcp-server,"What is the Unofficial Open Targets MCP Server? The Unofficial Open Targets MCP Server is a Model Context Protocol server designed to access Open Targets platform data, specifically for research on gene-drug-disease associations. How to use the Unofficial Open Targets MCP Server? To use the server, install the necessary packages, build the project, and run the server using Node.js. Configuration for various MCP clients is also provided. Key features of the Unofficial Open Targets MCP Server? Target Search : Search for gene symbols, names, and descriptions. Disease Search : Find diseases by name, synonym, or description. Target-Disease Associations : Access evidence scores from over 20 databases. Disease Target Summaries : Get prioritized therapeutic targets for diseases. Comprehensive Details : Retrieve detailed information about genes and diseases. Use cases of the Unofficial Open Targets MCP Server? Cancer research workflows to identify therapeutic targets. Drug discovery pipelines to find associations between diseases and potential treatments. Academic research for understanding gene-disease relationships. FAQ from the Unofficial Open Targets MCP Server? What data sources does the server integrate? The server integrates data from over 20 databases including ChEMBL, Ensembl, and ClinVar. Is authentication required to use the API? No, authentication is not required for accessing the API. What programming language is the server implemented in? The server is implemented in TypeScript.","![Logo](logo.png)

# Unofficial Open Targets MCP Server ğŸ§¬

Unofficial Model Context Protocol server for accessing Open Targets platform data for gene-drug-disease associations research.

[![API](https://img.shields.io/badge/Open%20Targets-v25.0.1-blue)](https://platform.opentargets.org/) 

Developed by [Augmented Nature](https://augmentednature.ai) 

## âœ… **Verified Features**

**All 6 tools working with live Open Targets API data:**

- ğŸ¯ **Target Search** - Gene symbols, names, descriptions (BRCA1, TP53, etc.)
- ğŸ¦  **Disease Search** - Names, synonyms, descriptions (cancer, diabetes, etc.)
- ğŸ”— **Target-Disease Associations** - Evidence scores from 20+ databases
- ğŸ“Š **Disease Target Summaries** - Prioritized therapeutic targets
- ğŸ§¬ **Target Details** - Comprehensive gene/protein information
- ğŸ­ **Disease Details** - Complete disease profiles with ontologies

## ğŸš€ **Quick Start**

```bash
# Install and build
npm install
npm run build

# Run the server
node build/index.js
```

## ğŸ“‹ **MCP Client Configuration**

### Claude Desktop

```json
{
  ""mcpServers"": {
    ""opentargets-server"": {
      ""command"": ""node"",
      ""args"": [""/path/to/opentargets-server/build/index.js""]
    }
  }
}
```

### Other MCP Clients

```bash
node /path/to/opentargets-server/build/index.js
```

## ğŸ› ï¸ **Available Tools**

### ğŸ¯ `search_targets`

**Search therapeutic targets by gene symbol, name, or description**

```json
{
  ""name"": ""search_targets"",
  ""arguments"": {
    ""query"": ""BRCA1"", // Gene symbol, name, or description
    ""size"": 10 // Optional: 1-500 results (default: 25)
  }
}
```

**Example Results:**

- **BRCA1** (ENSG00000012048) - BRCA1 DNA repair associated
- **BRCA2** (ENSG00000139618) - BRCA2 DNA repair associated
- **BRIP1** (ENSG00000136492) - BRCA1 interacting DNA helicase 1

### ğŸ¦  `search_diseases`

**Search diseases by name, synonym, or description**

```json
{
  ""name"": ""search_diseases"",
  ""arguments"": {
    ""query"": ""breast cancer"", // Disease name, synonym, or description
    ""size"": 10 // Optional: 1-500 results (default: 25)
  }
}
```

### ğŸ”— `get_target_disease_associations`

**Get target-disease associations with evidence scores**

```json
{
  ""name"": ""get_target_disease_associations"",
  ""arguments"": {
    ""targetId"": ""ENSG00000012048"", // Target Ensembl ID
    ""size"": 10 // Optional: 1-500 results
  }
}
```

**OR**

```json
{
  ""name"": ""get_target_disease_associations"",
  ""arguments"": {
    ""diseaseId"": ""EFO_0000305"", // Disease EFO ID
    ""size"": 10 // Optional: 1-500 results
  }
}
```

### ğŸ“Š `get_disease_targets_summary`

**Get prioritized targets associated with a disease**

```json
{
  ""name"": ""get_disease_targets_summary"",
  ""arguments"": {
    ""diseaseId"": ""EFO_0000305"", // Disease EFO ID (required)
    ""size"": 20 // Optional: 1-500 targets (default: 50)
  }
}
```

### ğŸ§¬ `get_target_details`

**Get comprehensive target information**

```json
{
  ""name"": ""get_target_details"",
  ""arguments"": {
    ""id"": ""ENSG00000012048"" // Target Ensembl gene ID
  }
}
```

### ğŸ­ `get_disease_details`

**Get comprehensive disease information**

```json
{
  ""name"": ""get_disease_details"",
  ""arguments"": {
    ""id"": ""EFO_0000305"" // Disease EFO ID
  }
}
```

## ğŸ“š **Resource Templates**

Access Open Targets data through standardized URIs:

- `opentargets://target/{ensemblId}` - Complete target information
- `opentargets://disease/{efoId}` - Complete disease information
- `opentargets://drug/{chemblId}` - Drug information
- `opentargets://association/{targetId}/{diseaseId}` - Association evidence
- `opentargets://search/{query}` - Search results

## ğŸ§ª **Real-World Examples**

### Cancer Research Workflow

```bash
# 1. Search for cancer-related targets
{""name"": ""search_targets"", ""arguments"": {""query"": ""oncogene"", ""size"": 10}}

# 2. Get detailed info for specific target
{""name"": ""get_target_details"", ""arguments"": {""id"": ""ENSG00000012048""}}

# 3. Find all diseases associated with BRCA1
{""name"": ""get_target_disease_associations"", ""arguments"": {""targetId"": ""ENSG00000012048""}}

# 4. Get top targets for breast cancer
{""name"": ""get_disease_targets_summary"", ""arguments"": {""diseaseId"": ""EFO_0000305"", ""size"": 20}}
```

### Drug Discovery Pipeline

```bash
# 1. Search for Alzheimer's disease
{""name"": ""search_diseases"", ""arguments"": {""query"": ""Alzheimer"", ""size"": 5}}

# 2. Get disease details
{""name"": ""get_disease_details"", ""arguments"": {""id"": ""EFO_0000249""}}

# 3. Find prioritized therapeutic targets
{""name"": ""get_disease_targets_summary"", ""arguments"": {""diseaseId"": ""EFO_0000249"", ""size"": 30}}
```

## ğŸ”¬ **Data Sources & Standards**

**Open Targets integrates 20+ databases:**

- **ChEMBL** - Drug & compound data
- **Ensembl** - Gene & protein annotations
- **EFO** - Experimental Factor Ontology
- **ClinVar** - Clinical variant data
- **GWAS Catalog** - Genome-wide association studies
- **UniProt** - Protein sequences & functions
- **Reactome** - Biological pathways
- **And many more...**

**Standardized Identifiers:**

- **Targets**: Ensembl gene IDs (e.g., ENSG00000012048)
- **Diseases**: EFO IDs (e.g., EFO_0000305)
- **Drugs**: ChEMBL IDs (e.g., CHEMBL1234)

## ğŸ—ï¸ **Architecture**

- **TypeScript** implementation with robust type safety
- **GraphQL** queries for efficient data retrieval
- **MCP Protocol** compliant JSON-RPC communication
- **Error Handling** with comprehensive validation
- **Production Ready** with 30s timeouts and proper logging

## ğŸ“Š **API Information**

- **Base URL**: `https://api.platform.opentargets.org/api/v4/graphql`
- **Version**: Open Targets v25.0.1
- **Rate Limits**: Generous for research use
- **Authentication**: None required
- **Format**: GraphQL queries, JSON responses

## ğŸ¤ **Contributing**

This server is developed and maintained by [Augmented Nature](https://augmentednature.ai). For enhancements:

1. Fork the repository
2. Make your changes
3. Submit a pull request

## **Support**

For issues with:

- **MCP Server**: Check the server logs and error outputs
- **Open Targets API**: Visit [platform.opentargets.org](https://platform.opentargets.org/)
- **GraphQL Queries**: Use the [Open Targets GraphQL browser](https://api.platform.opentargets.org/api/v4/graphql/browser)

## Citation
If you use this project in your research or publications, please cite it as follows:

```bibtex @misc{opentargetsmcp2025, 
author = {Moudather Chelbi},
title = {OpenTargets MCP Server},
year = {2025},
howpublished = {https://github.com/Augmented-Nature/OpenTargets-MCP-Server},
note = {Accessed: 2025-06-29}
",8
https://mcp.so/server/Augmented-Nature-UniProt-MCP-Server/Augmented-Nature,https://github.com/Augmented-Nature/Augmented-Nature-UniProt-MCP-Server,Unofficial UniProt MCP Server,A comprehensive Model Context Protocol (MCP) server providing advanced access to the UniProt protein database.,English,research-and-data,bioinformatics; proteomics; drug-discovery,"What is the Unofficial UniProt MCP Server? The Unofficial UniProt MCP Server is a comprehensive Model Context Protocol (MCP) server that provides advanced access to the UniProt protein database, enabling sophisticated protein research and analysis. How to use the Unofficial UniProt MCP Server? To use the server, clone the repository, install dependencies, and run the server. You can also integrate it with MCP clients for seamless access to protein data. Key features of the Unofficial UniProt MCP Server? 26 specialized bioinformatics tools for protein analysis, comparative genomics, and structural biology. Advanced search capabilities including batch processing and complex queries. Integration with external databases and literature references. Use cases of the Unofficial UniProt MCP Server? Conducting detailed protein analysis for research. Performing comparative genomics and evolutionary studies. Analyzing protein structures and functions for drug discovery. FAQ from the Unofficial UniProt MCP Server? Can I access all protein data through this server? Yes, the server provides comprehensive access to the UniProt protein database. Is there a cost to use the Unofficial UniProt MCP Server? No, it is free to use. What programming languages are supported? The server is developed using JavaScript and requires Node.js.","
# Unofficial UniProt MCP Server

A comprehensive Model Context Protocol (MCP) server providing advanced access to the UniProt protein database. This server offers 26 specialized bioinformatics tools enabling AI assistants and MCP clients to perform sophisticated protein research, comparative genomics, structural biology analysis, and systems biology investigations directly through UniProt's REST API.

**Developed by [Augmented Nature](https://augmentednature.ai)**

## Features

### **Core Protein Analysis (5 tools)**

- **Protein Search**: Search the UniProt database by protein name, keywords, or organism
- **Detailed Protein Info**: Retrieve comprehensive protein information including function, structure, and annotations
- **Gene-based Search**: Find proteins by gene name or symbol
- **Sequence Retrieval**: Get amino acid sequences in FASTA or JSON format
- **Feature Analysis**: Access functional domains, active sites, binding sites, and other protein features

### **Comparative & Evolutionary Analysis (4 tools)**

- **Protein Comparison**: Side-by-side comparison of multiple proteins with sequence and feature analysis
- **Homolog Discovery**: Find homologous proteins across different species
- **Ortholog Identification**: Identify orthologous proteins for evolutionary studies
- **Phylogenetic Analysis**: Retrieve evolutionary relationships and phylogenetic data

### **Structure & Function Analysis (4 tools)**

- **3D Structure Information**: Access PDB references and structural data
- **Advanced Domain Analysis**: Enhanced domain analysis with InterPro, Pfam, and SMART annotations
- **Variant Analysis**: Disease-associated variants and mutations
- **Sequence Composition**: Amino acid composition, hydrophobicity, and other sequence properties

### **Biological Context Analysis (4 tools)**

- **Pathway Integration**: Associated biological pathways from KEGG and Reactome
- **Protein Interactions**: Protein-protein interaction networks
- **Functional Classification**: Search by GO terms or functional annotations
- **Subcellular Localization**: Find proteins by subcellular localization

### **Batch Processing & Advanced Search (3 tools)**

- **Batch Processing**: Efficiently process multiple protein accessions
- **Advanced Search**: Complex queries with multiple filters (length, mass, organism, function)
- **Taxonomic Classification**: Search by detailed taxonomic classification

### **Literature & Cross-References (3 tools)**

- **External Database Links**: Links to PDB, EMBL, RefSeq, Ensembl, and other databases
- **Literature References**: Associated publications and citations
- **Annotation Quality**: Quality scores and confidence levels for different annotations

### **Data Export & Utilities (3 tools)**

- **Specialized Export**: Export data in GFF, GenBank, EMBL, and XML formats
- **Accession Validation**: Verify UniProt accession number validity
- **Taxonomic Information**: Detailed taxonomic classification and lineage data

### **Resource Templates**

- Direct access to protein data via URI templates for seamless integration

## Installation

### Prerequisites

- Node.js (v16 or higher)
- npm or yarn

### Setup

1. Clone the repository:

```bash
git clone <repository-url>
cd uniprot-server
```

2. Install dependencies:

```bash
npm install
```

3. Build the project:

```bash
npm run build
```

## Docker

### Building the Docker Image

Build the Docker image:

```bash
docker build -t uniprot-mcp-server .
```

### Running with Docker

Run the container:

```bash
docker run -i uniprot-mcp-server
```

For MCP client integration, you can use the container directly:

```json
{
  ""mcpServers"": {
    ""uniprot"": {
      ""command"": ""docker"",
      ""args"": [""run"", ""-i"", ""uniprot-mcp-server""],
      ""env"": {}
    }
  }
}
```

### Docker Compose (Optional)

Create a `docker-compose.yml` for easier management:

```yaml
version: ""3.8""
services:
  uniprot-mcp:
    build: .
    image: uniprot-mcp-server
    stdin_open: true
    tty: true
```

Run with:

```bash
docker-compose up
```

## Usage

### As an MCP Server

The server is designed to run as an MCP server that communicates via stdio:

```bash
npm start
```

### Adding to MCP Client Configuration

Add the server to your MCP client configuration (e.g., Claude Desktop):

```json
{
  ""mcpServers"": {
    ""uniprot"": {
      ""command"": ""node"",
      ""args"": [""/path/to/uniprot-server/build/index.js""],
      ""env"": {}
    }
  }
}
```

## Available Tools

### 1. search_proteins

Search the UniProt database for proteins by name, keyword, or organism.

**Parameters:**

- `query` (required): Search query (protein name, keyword, or complex search)
- `organism` (optional): Organism name or taxonomy ID to filter results
- `size` (optional): Number of results to return (1-500, default: 25)
- `format` (optional): Output format - json, tsv, fasta, xml (default: json)

**Example:**

```javascript
{
  ""query"": ""insulin"",
  ""organism"": ""human"",
  ""size"": 5
}
```

### 2. get_protein_info

Get detailed information for a specific protein by UniProt accession.

**Parameters:**

- `accession` (required): UniProt accession number (e.g., P04637)
- `format` (optional): Output format - json, tsv, fasta, xml (default: json)

**Example:**

```javascript
{
  ""accession"": ""P01308"",
  ""format"": ""json""
}
```

### 3. search_by_gene

Search for proteins by gene name or symbol.

**Parameters:**

- `gene` (required): Gene name or symbol (e.g., BRCA1, INS)
- `organism` (optional): Organism name or taxonomy ID to filter results
- `size` (optional): Number of results to return (1-500, default: 25)

**Example:**

```javascript
{
  ""gene"": ""BRCA1"",
  ""organism"": ""human""
}
```

### 4. get_protein_sequence

Get the amino acid sequence for a protein.

**Parameters:**

- `accession` (required): UniProt accession number
- `format` (optional): Output format - fasta, json (default: fasta)

**Example:**

```javascript
{
  ""accession"": ""P01308"",
  ""format"": ""fasta""
}
```

### 5. get_protein_features

Get functional features and domains for a protein.

**Parameters:**

- `accession` (required): UniProt accession number

**Example:**

```javascript
{
  ""accession"": ""P01308""
}
```

## Resource Templates

The server provides direct access to UniProt data through URI templates:

### 1. Protein Information

- **URI**: `uniprot://protein/{accession}`
- **Description**: Complete protein information for a UniProt accession
- **Example**: `uniprot://protein/P01308`

### 2. Protein Sequence

- **URI**: `uniprot://sequence/{accession}`
- **Description**: FASTA format protein sequence
- **Example**: `uniprot://sequence/P01308`

### 3. Search Results

- **URI**: `uniprot://search/{query}`
- **Description**: Search results for proteins matching the query
- **Example**: `uniprot://search/insulin`

## Examples

### Basic Protein Search

Search for insulin proteins in humans:

```javascript
// Tool call
{
  ""tool"": ""search_proteins"",
  ""arguments"": {
    ""query"": ""insulin"",
    ""organism"": ""human"",
    ""size"": 10
  }
}
```

### Get Detailed Protein Information

Retrieve comprehensive information about human insulin:

```javascript
// Tool call
{
  ""tool"": ""get_protein_info"",
  ""arguments"": {
    ""accession"": ""P01308""
  }
}
```

### Gene-based Search

Find proteins associated with the BRCA1 gene:

```javascript
// Tool call
{
  ""tool"": ""search_by_gene"",
  ""arguments"": {
    ""gene"": ""BRCA1"",
    ""organism"": ""human""
  }
}
```

### Retrieve Protein Sequence

Get the amino acid sequence for human insulin:

```javascript
// Tool call
{
  ""tool"": ""get_protein_sequence"",
  ""arguments"": {
    ""accession"": ""P01308"",
    ""format"": ""fasta""
  }
}
```

### Analyze Protein Features

Get functional domains and features for human insulin:

```javascript
// Tool call
{
  ""tool"": ""get_protein_features"",
  ""arguments"": {
    ""accession"": ""P01308""
  }
}
```

## API Integration

This server integrates with the UniProt REST API for programmatic access to protein data. For more information about UniProt:

- **UniProt Website**: https://www.uniprot.org/
- **API Documentation**: https://www.uniprot.org/help/api
- **REST API Guide**: https://www.uniprot.org/help/api_queries

All API requests include:

- **User-Agent**: `UniProt-MCP-Server/1.0.0`
- **Timeout**: 30 seconds
- **Base URL**: `https://rest.uniprot.org` (programmatic access only)

## Error Handling

The server includes comprehensive error handling:

- **Input Validation**: All parameters are validated using type guards
- **API Errors**: Network and API errors are caught and returned with descriptive messages
- **Timeout Handling**: Requests timeout after 30 seconds
- **Graceful Degradation**: Partial failures are handled appropriately

## Development

### Build the Project

```bash
npm run build
```

### Development Mode

Run TypeScript compiler in watch mode:

```bash
npm run dev
```

### Project Structure

```
uniprot-server/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ index.ts          # Main server implementation
â”œâ”€â”€ build/                # Compiled JavaScript output
â”œâ”€â”€ package.json          # Node.js dependencies and scripts
â”œâ”€â”€ tsconfig.json         # TypeScript configuration
â””â”€â”€ README.md            # This file
```

## Dependencies

- **@modelcontextprotocol/sdk**: Core MCP SDK for server implementation
- **axios**: HTTP client for UniProt API requests
- **typescript**: TypeScript compiler for development

## License

MIT License

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## Support

For issues and questions:

1. Check the [UniProt API documentation](https://www.uniprot.org/help/api)
2. Review the [Model Context Protocol specification](https://modelcontextprotocol.io/)
3. Open an issue on the repository

## About Augmented Nature

This comprehensive UniProt MCP Server is developed by **[Augmented Nature](https://augmentednature.ai)**, a leading innovator in AI-powered bioinformatics and computational biology solutions. Augmented Nature specializes in creating advanced tools that bridge the gap between artificial intelligence and biological research, enabling researchers to unlock deeper insights from biological data.

## Complete Tool Reference

### **Core Protein Analysis Tools**

1. `search_proteins` - Search UniProt database by name, keyword, or organism
2. `get_protein_info` - Get detailed protein information by accession
3. `search_by_gene` - Find proteins by gene name or symbol
4. `get_protein_sequence` - Retrieve amino acid sequences
5. `get_protein_features` - Access functional features and domains

### **Comparative & Evolutionary Analysis Tools**

6. `compare_proteins` - Compare multiple proteins side-by-side
7. `get_protein_homologs` - Find homologous proteins across species
8. `get_protein_orthologs` - Identify orthologous proteins
9. `get_phylogenetic_info` - Retrieve evolutionary relationships

### **Structure & Function Analysis Tools**

10. `get_protein_structure` - Access 3D structure information from PDB
11. `get_protein_domains_detailed` - Enhanced domain analysis (InterPro, Pfam, SMART)
12. `get_protein_variants` - Disease-associated variants and mutations
13. `analyze_sequence_composition` - Amino acid composition analysis

### **Biological Context Tools**

14. `get_protein_pathways` - Associated biological pathways (KEGG, Reactome)
15. `get_protein_interactions` - Protein-protein interaction networks
16. `search_by_function` - Search by GO terms or functional annotations
17. `search_by_localization` - Find proteins by subcellular localization

### **Batch Processing & Advanced Search Tools**

18. `batch_protein_lookup` - Process multiple accessions efficiently
19. `advanced_search` - Complex queries with multiple filters
20. `search_by_taxonomy` - Search by taxonomic classification

### **Literature & Cross-Reference Tools**

21. `get_external_references` - Links to other databases (PDB, EMBL, RefSeq, etc.)
22. `get_literature_references` - Associated publications and citations
23. `get_annotation_confidence` - Quality scores for annotations

### **Data Export & Utility Tools**

24. `export_protein_data` - Export in specialized formats (GFF, GenBank, EMBL, XML)
25. `validate_accession` - Check accession number validity
26. `get_taxonomy_info` - Detailed taxonomic information

## Changelog

### v1.0.0 - Comprehensive Bioinformatics Platform

- **Major expansion**: Added 21 new specialized tools (total: 26 tools)
- **Comparative Analysis**: Protein comparison, homolog/ortholog identification, phylogenetic analysis
- **Structural Biology**: 3D structure integration, detailed domain analysis, variant analysis
- **Systems Biology**: Pathway integration, protein interactions, functional classification
- **Advanced Search**: Batch processing, complex filtering, taxonomic search
- **Literature Integration**: External database links, citations, annotation confidence
- **Data Export**: Multiple specialized formats (GFF, GenBank, EMBL, XML)
- **Enhanced Docker Support**: Multi-stage builds with security best practices
- **Comprehensive Documentation**: Complete tool reference and examples
- **Developed by Augmented Nature**: Professional bioinformatics platform

## Citation
If you use this project in your research or publications, please cite it as follows:

```bibtex @misc{uniprotmcp2025, 
author = {Moudather Chelbi},
title = {UniProt MCP Server},
year = {2025},
howpublished = {https://github.com/Augmented-Nature/Augmented-Nature-UniProt-MCP-Server/},
note = {Accessed: 2025-06-29}
",16
https://mcp.so/server/Augmented-Nature-UniProt-MCP-Server/Thiagomenezes12,https://github.com/Thiagomenezes12/Augmented-Nature-UniProt-MCP-Server,Augmented Nature: UniProt MCP Server ğŸŒ¿ğŸ”¬,"Explore the Unofficial UniProt MCP Server, your gateway to advanced protein analysis with 26 specialized tools. Join the community and enhance your research capabilities today! ğŸ™ğŸŒŸ",English,research-and-data,bioinformatics; phylogenetics; drug-discovery; proteomics; drug-design; uniprot,"What is Augmented Nature: UniProt MCP Server? The Augmented Nature UniProt MCP Server is a comprehensive tool designed for advanced protein analysis, providing access to the UniProt protein database through a user-friendly interface and specialized tools. How to use the Augmented Nature UniProt MCP Server? To use the server, clone the repository from GitHub, install the necessary dependencies, and run the server. Access it via your web browser at http://localhost:5000 to search for proteins and retrieve detailed information. Key features of the Augmented Nature UniProt MCP Server? Advanced access to UniProt data. Utilization of the Model Context Protocol for enhanced data interaction. Integration with AI tools for data analysis. Support for various topics including bioinformatics, drug design, and phylogenetics. Use cases of the Augmented Nature UniProt MCP Server? Conducting protein research in bioinformatics. Supporting drug discovery projects. Analyzing protein functions and relationships in phylogenetics. FAQ from Augmented Nature UniProt MCP Server? What is UniProt? UniProt is a comprehensive protein sequence and functional information database essential for biological research. How do I install the server? Clone the repository, install dependencies using pip, and run the server with Python. Can I access the server programmatically? Yes, the server includes a RESTful API for programmatic access to protein data.",,
https://mcp.so/server/AustinKelsay_nostr-mcp-server/MCP-Mirror,https://github.com/MCP-Mirror/AustinKelsay_nostr-mcp-server,Nostr MCP Server,Mirror of,English,developer-tools,nostr; mcp-server; llm-integration,"What is Nostr MCP Server? Nostr MCP Server is a Model Context Protocol (MCP) server that provides Nostr capabilities to large language models (LLMs) like Claude, enabling them to interact with the Nostr network. How to use Nostr MCP Server? To use the Nostr MCP Server, clone the repository, install dependencies, and configure it with applications like Claude for Desktop or Cursor. After configuration, you can make requests to interact with the Nostr network. Key features of Nostr MCP Server? Implements seven tools for Nostr network interaction, including fetching user profiles, notes, and zaps. Supports both hex public keys and npub format for user identifiers. User-friendly display of results with automatic format conversion. Use cases of Nostr MCP Server? Fetching user profile information from the Nostr network. Retrieving text and long-form notes authored by users. Accessing detailed payment information for zaps sent and received. Searching through Nostr Implementation Possibilities (NIPs). FAQ from Nostr MCP Server? Can I use Nostr MCP Server with any LLM? Yes! It is designed to work with LLMs like Claude and Cursor. Is there a limit on the number of queries? Yes, the server has a default 8-second timeout for queries to prevent hanging. What formats are supported for user identifiers? Both hex public keys and npub format are supported.","# Nostr MCP Server

A Model Context Protocol (MCP) server that provides Nostr capabilities to LLMs like Claude.

https://github.com/user-attachments/assets/1d2d47d0-c61b-44e2-85be-5985d2a81c64

## Features

This server implements seven tools for interacting with the Nostr network:

1. `getProfile`: Fetches a user's profile information by public key
2. `getKind1Notes`: Fetches text notes (kind 1) authored by a user
3. `getLongFormNotes`: Fetches long-form content (kind 30023) authored by a user
4. `getReceivedZaps`: Fetches zaps received by a user, including detailed payment information
5. `getSentZaps`: Fetches zaps sent by a user, including detailed payment information
6. `getAllZaps`: Fetches both sent and received zaps for a user, clearly labeled with direction and totals
7. `searchNips`: Search through Nostr Implementation Possibilities (NIPs) with relevance scoring

All tools fully support both hex public keys and npub format, with user-friendly display of Nostr identifiers.

## Installation

```bash
# Clone the repository
git clone https://github.com/austinkelsay/nostr-mcp-server.git
cd nostr-mcp-server

# Install dependencies
npm install

# Build the project
npm run build
```

## Connecting to Claude for Desktop

1. Make sure you have [Claude for Desktop](https://claude.ai/desktop) installed and updated to the latest version.

2. Configure Claude for Desktop by editing or creating the configuration file:

   For macOS:
   ```bash
   vim ~/Library/Application\ Support/Claude/claude_desktop_config.json
   ```

   For Windows:
   ```bash
   notepad %AppData%\Claude\claude_desktop_config.json
   ```

3. Add the Nostr server to your configuration:

   ```json
   {
       ""mcpServers"": {
           ""nostr"": {
               ""command"": ""node"",
               ""args"": [
                   ""/ABSOLUTE/PATH/TO/nostr-mcp-server/build/index.js""
               ]
           }
       }
   }
   ```

   Be sure to replace `/ABSOLUTE/PATH/TO/` with the actual path to your project.

4. Restart Claude for Desktop.

## Connecting to Cursor

1. Make sure you have [Cursor](https://cursor.sh/) installed and updated to the latest version.

2. Configure Cursor by creating or editing the configuration file:

   For macOS:
   ```bash
   vim ~/.cursor/config.json
   ```

   For Windows:
   ```bash
   notepad %USERPROFILE%\.cursor\config.json
   ```

3. Add the Nostr server to your configuration:

   ```json
   {
       ""mcpServers"": {
           ""nostr"": {
               ""command"": ""node"",
               ""args"": [
                   ""/ABSOLUTE/PATH/TO/nostr-mcp-server/build/index.js""
               ]
           }
       }
   }
   ```

   Be sure to replace `/ABSOLUTE/PATH/TO/` with the actual path to your project.

4. Restart Cursor.

## Usage in Claude

Once configured, you can ask Claude to use the Nostr tools by making requests like:

- ""Show me the profile information for npub1qny3tkh0acurzla8x3zy4nhrjz5zd8ne6dvrjehx9n9hr3lnj08qwuzwc8""
- ""What are the recent posts from npub1qny3tkh0acurzla8x3zy4nhrjz5zd8ne6dvrjehx9n9hr3lnj08qwuzwc8?""
- ""Show me the long-form articles from npub1qny3tkh0acurzla8x3zy4nhrjz5zd8ne6dvrjehx9n9hr3lnj08qwuzwc8""
- ""How many zaps has npub1qny3tkh0acurzla8x3zy4nhrjz5zd8ne6dvrjehx9n9hr3lnj08qwuzwc8 received?""
- ""Show me the zaps sent by npub1qny3tkh0acurzla8x3zy4nhrjz5zd8ne6dvrjehx9n9hr3lnj08qwuzwc8""
- ""Show me all zaps (both sent and received) for npub1qny3tkh0acurzla8x3zy4nhrjz5zd8ne6dvrjehx9n9hr3lnj08qwuzwc8""
- ""Search for NIPs about zaps""
- ""What NIPs are related to long-form content?""
- ""Show me NIP-23 with full content""

The server automatically handles conversion between npub and hex formats, so you can use either format in your queries. Results are displayed with user-friendly npub identifiers.

## Advanced Usage

You can specify custom relays for any query:

- ""Show me the profile for npub1qny3tkh0acurzla8x3zy4nhrjz5zd8ne6dvrjehx9n9hr3lnj08qwuzwc8 using relay wss://relay.damus.io""

You can also specify the number of notes or zaps to fetch:

- ""Show me the latest 20 notes from npub1qny3tkh0acurzla8x3zy4nhrjz5zd8ne6dvrjehx9n9hr3lnj08qwuzwc8""

For zap queries, you can enable extra validation and debugging:

- ""Show me all zaps for npub1qny3tkh0acurzla8x3zy4nhrjz5zd8ne6dvrjehx9n9hr3lnj08qwuzwc8 with validation and debug enabled""

For NIP searches, you can control the number of results and include full content:

- ""Search for NIPs about zaps with full content""
- ""Show me the top 5 NIPs about relays""
- ""What NIPs are related to encryption? Show me 15 results""

## Limitations

- The server has a default 8-second timeout for queries to prevent hanging
- Only public keys in hex format or npub format are supported
- Only a subset of relays is used by default

## Implementation Details

- Native support for npub format using NIP-19 encoding/decoding
- NIP-57 compliant zap receipt detection with direction-awareness (sent/received/self)
- Advanced bolt11 invoice parsing with payment amount extraction
- Smart caching system for improved performance with large volumes of zaps
- Total sats calculations for sent/received/self zaps with net balance
- Optional NIP-57 validation for ensuring zap receipt integrity
- Each tool call creates a fresh connection to the relays, ensuring reliable data retrieval

## Troubleshooting

- If queries time out, try increasing the `QUERY_TIMEOUT` value in the source code (currently 8 seconds)
- If no data is found, try specifying different relays that might have the data
- Check Claude's MCP logs for detailed error information

## Default Relays

The server uses the following relays by default:
- wss://relay.damus.io
- wss://relay.nostr.band
- wss://relay.primal.net
- wss://nos.lol
- wss://relay.current.fyi
- wss://nostr.bitcoiner.social

## Development

To modify or extend this server:

1. Edit the relevant file in the project root:
   - `index.ts`: Main server and tool registration
   - `zap-tools.ts`: Zap-related functionality (getSentZaps, getReceivedZaps, getAllZaps)
   - `nips-tools.ts`: NIPs search functionality

2. Run `npm run build` to compile

3. Restart Claude for Desktop or Cursor to pick up your changes

The codebase is organized into modules:
- Core server setup and tools for profiles and notes are in `index.ts`
- Zap functionality is encapsulated in `zap-tools.ts`
- NIPs search is implemented in `nips-tools.ts`

This separation makes the codebase more maintainable and easier to extend with new features.
",0
https://mcp.so/server/Auth0MCPServer/auth0,,,Home Servers Clients Categories Tags Feed,English,,,,,
https://mcp.so/server/AutoMaid/Buidl-Land,https://github.com/Buidl-Land/AutoMaid,AgenticMaid Project,AgenticMaid is a Python library designed to interact with one or more Multi-Capability Protocol (MCP) servers.,English,developer-tools,agenticmaid; mcp; python-library,"What is AgenticMaid? AgenticMaid is a Python library designed to interact with one or more Multi-Capability Protocol (MCP) servers, enabling dynamic fetching and utilization of tools provided by these servers. How to use AgenticMaid? To use AgenticMaid, install the required dependencies, configure it using a Python dictionary, JSON file, or .env file, and initialize the client. You can then interact with MCP servers and manage AI services. Key features of AgenticMaid? Multi-server MCP interaction for tool utilization. Dynamic tool fetching at runtime. Flexible configuration options. Management of various AI/LLM services. Scheduled task execution. Integration with chat services. Creation of reactive agents using langgraph. Use cases of AgenticMaid? Building AI-powered applications that require multiple tools from different servers. Automating tasks based on schedules. Managing interactions with chat services. Creating reactive agents that leverage AI models. FAQ from AgenticMaid? What is MCP? MCP stands for Multi-Capability Protocol, which allows for the integration of various tools and services. Is AgenticMaid free to use? Yes! AgenticMaid is open-source and free to use. What are the prerequisites for using AgenticMaid? You need Python 3.8 or higher and the required dependencies installed.","# AgenticMaid Project

## Overview

AgenticMaid is a Python library designed to interact with one or more Multi-Capability Protocol (MCP) servers. It allows for dynamic fetching and utilization of tools (capabilities) provided by these servers. The client can also manage configurations for various AI/LLM services, schedule automated tasks, and handle chat service interactions, making it a versatile component for building AI-powered applications.

It leverages `langchain-mcp-adapters` for communication with MCP servers and `langgraph` for creating reactive agents that can use the fetched MCP tools.

## Features

*   **Multi-Server MCP Interaction:** Connects to and utilizes tools from multiple MCP servers.
*   **Dynamic Tool Fetching:** Retrieves available tools from MCP servers at runtime.
*   **Flexible Configuration:** Supports configuration via Python dictionaries, JSON files, and `.env` files for sensitive data.
*   **AI Service Management:** Configures and utilizes various AI/LLM services (e.g., OpenAI, Anthropic, Azure OpenAI, local models).
*   **Scheduled Tasks:** Allows defining and running tasks based on cron-like schedules.
*   **Chat Service Integration:** Provides a framework for handling interactions with defined chat services.
*   **Agent Creation:** Uses `langgraph` to create ReAct agents that can leverage MCP tools and configured LLMs.
*   **Environment Variable Support:** Loads default configurations and sensitive keys (like API keys) from an `.env` file.

## Installation

1.  **Prerequisites:**
    *   Python 3.8+

2.  **Clone the repository (if applicable) or add `AgenticMaid` to your project.**

3.  **Install Dependencies:**
    The client relies on several libraries. Ensure you have a `requirements.txt` file in your project or install them directly. Key dependencies include:
    ```bash
    pip install python-dotenv langchain-mcp-adapters langgraph schedule langchain-core langchain-openai langchain-anthropic fastapi pydantic ""uvicorn[standard]""
    ```
    The command above includes core dependencies and those required for the FastAPI service and CLI tool. The file [`AgenticMaid/requirements.txt`](AgenticMaid/requirements.txt) lists dependencies primarily for the API and CLI features.

## Configuration

The `AgenticMaid` can be configured in multiple ways:

1.  **Python Dictionary:** Pass a Python dictionary directly to the `AgenticMaid` constructor.
2.  **JSON File:** Provide a path to a JSON configuration file to the constructor.
3.  **`.env` File:** For default values and sensitive information like API keys, create a `.env` file in the `AgenticMaid/` directory (i.e., alongside [`client.py`](./client.py:1)). Values from the `.env` file can be overridden by the main JSON/dictionary configuration.

### Configuration Structure

The main configuration (Python dictionary or JSON) generally includes the following sections:

*   `model` (optional): Global default settings for AI models.
*   `ai_services`: Definitions for various AI/LLM providers and models.
*   `mcp_servers`: Configuration for the MCP servers the client will connect to.
*   `scheduled_tasks`: An array of tasks to be run on a schedule.
*   `chat_services`: Definitions for chat services the client can interact with.
*   `agents` (optional): Pre-defined agent configurations.
*   `default_llm_service_name` (optional): A global default LLM service to use if not specified elsewhere.

See the [`AgenticMaid/config.example.json`](./config.example.json) file for a detailed example with comments explaining each field.

### 1. Using `.env` File

Create a file named `.env` in the `AgenticMaid` directory (e.g., `AgenticMaid/.env`). This file is used for API keys and other default settings. Values from here serve as defaults and can be overridden by the main configuration file or dictionary.

**Example `AgenticMaid/.env`:**
```env
# AgenticMaid/.env example

# Default API key if not specified per service in main config
# DEFAULT_API_KEY=your_default_api_key_here

# Default model name if not specified per service in main config
# DEFAULT_MODEL_NAME=gpt-3.5-turbo-default-from-env

# Provider-specific defaults
OPENAI_API_KEY=your_openai_api_key_from_env
OPENAI_DEFAULT_MODEL=gpt-3.5-turbo-openai-from-env

ANTHROPIC_API_KEY=your_anthropic_api_key_from_env
ANTHROPIC_DEFAULT_MODEL=claude-2-from-env

# For Azure OpenAI
# AZURE_OPENAI_API_KEY=your_azure_openai_key
# AZURE_OPENAI_ENDPOINT=your_azure_endpoint
# AZURE_OPENAI_DEPLOYMENT_NAME=your_deployment_name

# For local OpenAI-compatible servers (e.g., LM Studio)
# LOCAL_OPENAI_BASE_URL=http://localhost:1234/v1
```

### 2. Main Configuration (JSON or Python Dict)

This configuration defines the specifics of your MCP servers, AI services, tasks, and chat interfaces.

#### a. `ai_services`

Define each AI service you intend to use. The key is a custom name for the service.
Refer to [`AgenticMaid/config.example.json`](./config.example.json) for fields like `provider`, `model`, `api_key`, `base_url`, etc. The `api_key` can be sourced from the `.env` file if not provided here.

**Example Snippet (from `config.example.json`):**
```json
{
  ""ai_services"": {
    ""openai_gemini_pro"": {
      ""provider"": ""Google"",
      ""model"": ""gemini-2.5-pro"",
      ""api_key"": ""your_google_api_key_here_or_leave_blank_to_use_env""
    },
    ""anthropic_claude_opus"": {
      ""provider"": ""Anthropic"",
      ""model"": ""claude-4-opus""
    }
  }
}
```

#### b. `mcp_servers`

Define the MCP servers the client should connect to. The `MultiServerAgenticMaid` (or relevant class) will use these configurations.
Refer to [`AgenticMaid/config.example.json`](./config.example.json) for fields like `adapter_type`, `base_url` (for FastAPI), `command_template` (for CLI), `name`, and `description`.

**Example Snippet (from `config.example.json`):**
```json
{
  ""mcp_servers"": {
    ""server_1_local_fastapi"": {
      ""adapter_type"": ""fastapi"",
      ""base_url"": ""http://localhost:8001/mcp/v1"",
      ""name"": ""Local FastAPI MCP Server""
    }
  }
}
```

#### c. `scheduled_tasks`

Define tasks that should run on a schedule. Each task object includes:
*   `name`: A descriptive name for the task.
*   `cron_expression`: A cron-like expression (currently supports simple forms like ""daily at HH:MM"" or ""0 * * * *"" for hourly via the `schedule` library's interpretation, which might require custom parsing in `_schedule_tasks` for full cron).
*   `prompt`: The instruction/prompt for the agent.
*   `agent_id` (optional): Reference to a pre-defined agent in the `agents` section.
*   `model_config_name`: The name of the AI service (from `ai_services`) to use for this task's agent.
*   `enabled`: Boolean, `true` to enable the task, `false` to disable.

**Example Snippet (from `config.example.json`):**
```json
{
  ""scheduled_tasks"": [
    {
      ""name"": ""Hourly Summary Bot"",
      ""cron_expression"": ""0 * * * *"", // Placeholder, actual parsing depends on _schedule_tasks
      ""prompt"": ""Generate a brief summary of activities from the last hour."",
      ""model_config_name"": ""openai_gemini_pro"",
      ""enabled"": true
    }
  ]
}
}
```
The `cron_expression` interpretation is handled by the `schedule` library. For more complex cron strings, the parsing logic in `_schedule_tasks` within [`AgenticMaid/client.py`](./client.py:237) might need adjustments.
#### d. `chat_services`

Define configurations for different chat interfaces. Each chat service object includes:
*   `service_id`: A unique identifier for the chat service.
*   `llm_service_name`: The name of the AI service (from `ai_services`) to power this chat.
*   `streaming_api_endpoint` (conceptual): A path representing where streaming responses might be served.
*   `non_streaming_api_endpoint` (conceptual): A path for non-streaming (full) responses.

**Example Snippet (from `config.example.json`):**
```json
{
  ""chat_services"": [
    {
      ""service_id"": ""general_support_chat"",
      ""service_id"": ""general_support_chat"",
      ""llm_service_name"": ""openai_gemini_pro"",
      ""streaming_api_endpoint"": ""/chat/v1/streams/general_support_chat"",
      ""non_streaming_api_endpoint"": ""/chat/v1/completions/general_support_chat""
    }
  ]
}
```

##### **Dual-Prompt System**

The `chat_services` configuration now supports a dual-prompt system to provide more context and control over the agent's behavior. This is achieved through two optional fields: `system_prompt` and `role_prompt`.

*   **`system_prompt`**: This prompt is injected as the first message with the `system` role. It's used to give the AI model high-level instructions, context, or constraints that should apply to the entire conversation. For example, you can define the agent's persona, its core function, and its operational boundaries.

*   **`role_prompt`**: This prompt is injected as a `user` message immediately after the system prompt (if one is provided) and before the actual user's message. It's used to guide the AI on how it should behave or what specific role it should adopt for the upcoming turn in the conversation. This can be useful for setting a specific tone or directing the agent's focus for the immediate task.

When a chat request is processed, the final list of messages sent to the AI model will be in the following order:
1.  System Prompt (if provided)
2.  Role Prompt (if provided)
3.  User's Message(s)

**Example with Prompts in `config.json`:**
```json
{
  ""chat_services"": [
    {
      ""service_id"": ""general_support_chat_gemini"",
      ""llm_service_name"": ""google_gemini_default"",
      ""system_prompt"": ""You are a helpful and friendly customer support assistant for the AgenticMaid project. Your goal is to provide clear, accurate, and concise answers."",
      ""role_prompt"": ""Please answer the user's question based on the project's documentation and capabilities. Be polite and professional."",
      ""streaming_api_endpoint"": ""/chat/v1/streams/general_support_chat_gemini"",
      ""non_streaming_api_endpoint"": ""/chat/v1/completions/general_support_chat_gemini""
    }
  ]
}
```

## Usage

### 1. Initialization

First, import and initialize the `AgenticMaid`. You need to call `await client.async_initialize()` after creating an instance to complete the asynchronous setup (like fetching MCP tools).

```python
import asyncio
from pkg_AgenticMaid.client import ClientAgenticMaid # Placeholder: Actual class name from client.py

async def main():
    # Option 1: Load config from JSON file
    # client = ClientAgenticMaid(config_path_or_dict='AgenticMaid/config.example.json')

    # Option 2: Load config from a Python dictionary (Direct Python Invocation)
    # This method is ideal for embedding AgenticMaid within other Python applications,
    # allowing for dynamic configuration without relying on external JSON files.
    # The .env file for API keys and defaults is still loaded if present.
    config_dict = {
        ""ai_services"": {
            ""my_gemini_service"": { # Custom name for your service
                ""provider"": ""Google"",
                ""model"": ""gemini-2.5-pro""
                # API key can be provided here directly: ""api_key"": ""AIza..."",
                # or if omitted, it will attempt to load from .env (e.g., GOOGLE_API_KEY)
            }
        },
        ""mcp_servers"": {
            ""example_mcp_server"": { # Custom name for your MCP server connection
                ""adapter_type"": ""fastapi"", # Or other supported adapter types
                ""base_url"": ""http://localhost:8001/mcp/v1"", # URL of the target MCP server
                ""name"": ""My Example MCP Server""
            }
        },
        ""default_llm_service_name"": ""my_gemini_service"", # Default LLM for agents if not specified
        # Other sections like ""scheduled_tasks"", ""chat_services"", ""agents"" can be added as needed.
        # For a comprehensive, runnable example of direct dictionary invocation,
        # please refer to the script:
        # [`AgenticMaid/examples/direct_invocation_example.py`](./examples/direct_invocation_example.py)
    }
    client = ClientAgenticMaid(config_path_or_dict=config_dict)

    # Perform asynchronous initialization
    await client.async_initialize()

    if client.config and client.mcp_client:
        print(""ClientAgenticMaid initialized successfully."")
        print(f""Fetched {len(client.mcp_tools)} MCP tools: {[tool.name for tool in client.mcp_tools]}"")
    else:
        print(""ClientAgenticMaid initialization failed or no MCP tools found."")
        print(""AgenticMaidClient initialization failed or no MCP tools found."")
        return

    # ... use the client ...

if __name__ == ""__main__"":
    asyncio.run(main())
```

### 1.1. Detailed Example of Direct Dictionary Invocation

For a runnable script demonstrating how to instantiate and use `ClientAgenticMaid` with a direct dictionary configuration, including basic operations like chat, please refer to the example file:

*   [`AgenticMaid/examples/direct_invocation_example.py`](./examples/direct_invocation_example.py)

This example showcases how to set up the configuration dictionary and perform common client actions.

### 2. Running an MCP Interaction (Agent Invocation)

Use the `run_mcp_interaction` method to interact with an agent. The agent will be created (or retrieved if it already exists) using the specified LLM service and all fetched MCP tools.
```python
```python
# Assuming 'client' is an initialized ClientAgenticMaid instance from the example above

    # Example: Run an interaction
    messages_for_agent = [{""role"": ""user"", ""content"": ""What is the weather in London using available tools?""}]
    llm_service_to_use = ""my_gemini_service"" # Must be a key from your ai_services config
    agent_identifier = ""weather_agent_01"" # A custom key for this agent instance

    response = await client.run_mcp_interaction(
        messages=messages_for_agent,
        llm_service_name=llm_service_to_use,
        agent_key=agent_identifier
    )

    if response and ""error"" not in response:
        print(f""Agent Response: {response}"")
    else:
        print(f""Agent Interaction Error: {response.get('error') if response else 'Unknown error'}"")
```

### 3. Running Scheduled Tasks

To run scheduled tasks, first ensure they are defined in your configuration. Then, start the scheduler. The scheduler runs in a background thread.

```python
# Assuming 'client' is an initialized ClientAgenticMaid instance

    # To start the scheduler (it runs in a background thread):
    if client.scheduler.jobs: # Check if there are any jobs scheduled
        print(""Starting scheduler..."")
        client.start_scheduler()
        # The scheduler will now run tasks in the background.
        # Keep the main thread alive if you want tasks to continue running.
        # For example, in a long-running application:
        # try:
        #     while True:
        #         await asyncio.sleep(1)
        # except KeyboardInterrupt:
        #     print(""Application shutting down."")
        #     client.stop_scheduler() # Conceptual stop
    else:
        print(""No tasks scheduled."")
```
**Note:** The `start_scheduler` method runs an infinite loop in a daemon thread. Ensure your main application manages its lifecycle appropriately. The `stop_scheduler` method is currently a placeholder; a more robust stop mechanism (e.g., using `threading.Event`) might be needed for graceful shutdown in complex applications.

### 4. Interacting with Chat Services

To handle messages for a defined chat service, use the `handle_chat_message` method.

```python
# Assuming 'client' is an initialized ClientAgenticMaid instance

    # Example: Interact with a chat service
    chat_service_id_to_use = ""general_support_chat"" # Must be a service_id from your chat_services config
    chat_messages = [{""role"": ""user"", ""content"": ""Hello, I need help with my account.""}]

    chat_response = await client.handle_chat_message(
        service_id=chat_service_id_to_use,
        messages=chat_messages,
        stream=False # Set to True for streaming (currently placeholder)
    )

    if chat_response and ""error"" not in chat_response:
        print(f""Chat Service Response: {chat_response}"")
    else:
        print(f""Chat Service Error: {chat_response.get('error') if chat_response else 'Unknown error'}"")

```

<![CDATA[
### 5. Running the FastAPI Service

The `AgenticMaid` project includes a FastAPI application that exposes its functionalities over an HTTP API. This allows `AgenticMaid` to be integrated into other systems or accessed remotely.

**To run the FastAPI service:**

Ensure you have `uvicorn` and `fastapi` installed (they are included in the `pip install` command in the [Installation](#installation) section if you included `AgenticMaid/requirements.txt` dependencies).

From the root directory of the `Agent` project (where `AgenticMaid` is a subdirectory), run:

```bash
python -m uvicorn AgenticMaid.api:app --reload
```

Or, if your `PYTHONPATH` is set up correctly to find the `AgenticMaid` module:

```bash
uvicorn AgenticMaid.api:app --reload
```

The API will typically be available at `http://127.0.0.1:8000`.

#### API Endpoints

The following are the main endpoints provided by the FastAPI service:

*   **`POST /client/init`**:
    *   **Purpose**: Initializes or reconfigures the global `ClientAgenticMaid` instance within the API service.
    *   **Request Body**: A JSON object containing a `config` key. The value can be either a path (string) to a JSON configuration file accessible by the server, or a full configuration dictionary.
        ```json
        {
          ""config"": ""./AgenticMaid/config.example.json""
        }
        ```
        or
        ```json
        {
          ""config"": {
            ""ai_services"": { ""..."": ""..."" },
            ""mcp_servers"": { ""..."": ""..."" }
            // ... other config sections
          }
        }
        ```
    *   **Response**: A JSON object indicating success or failure.
        ```json
        {
          ""status"": ""success"",
          ""message"": ""ClientAgenticMaid (re)configured successfully.""
        }
        ```

*   **`POST /client/chat`**:
    *   **Purpose**: Processes a chat message using a configured chat service within `ClientAgenticMaid`.
    *   **Request Body**:
        ```json
        {
          ""service_id"": ""your_chat_service_id_from_config"",
          ""messages"": [
            { ""role"": ""user"", ""content"": ""Hello, how can you help me?"" }
          ]
        }
        ```
    *   **Response**: The response from the chat agent, which can vary in structure. Typically includes the agent's reply.
        ```json
        {
          ""data"": {
            ""messages"": [
              { ""role"": ""user"", ""content"": ""Hello, how can you help me?"" },
              { ""role"": ""assistant"", ""content"": ""I am an AI assistant..."" }
            ]
          },
          ""error"": null
        }
        ```

*   **`POST /client/run_task/{task_name}`**:
    *   **Purpose**: Triggers a specific scheduled task by its name, as defined in the `ClientAgenticMaid` configuration.
    *   **Path Parameter**: `task_name` (string) - The name of the task.
    *   **Response**: A JSON object indicating the status of the task execution.
        ```json
        {
          ""status"": ""success"",
          ""message"": ""Task 'YourTaskName' executed."",
          ""task_name"": ""YourTaskName"",
          ""details"": { /* ... task execution result ... */ }
        }
        ```

*   **`POST /client/run_all_scheduled_tasks`**:
    *   **Purpose**: Triggers all *enabled* scheduled tasks defined in the `ClientAgenticMaid` configuration.
    *   **Response**: A JSON object containing a list of results for each task attempted.
        ```json
        {
          ""results"": [
            {
              ""status"": ""success"",
              ""message"": ""Task 'Task1' executed."",
              ""task_name"": ""Task1"",
              ""details"": { /* ... */ }
            },
            {
              ""status"": ""skipped"",
              ""message"": ""Task 'Task2' is disabled."",
              ""task_name"": ""Task2"",
              ""details"": null
            }
          ]
        }
        ```

*   **`GET /`** and **`GET /health`**:
    *   **Purpose**: Health check endpoints to verify if the API service is running.
    *   **Response**:
        ```json
        {
          ""status"": ""ok"",
          ""status"": ""ok"",
          ""message"": ""AgenticMaid API is running.""
        }
        ```

### 6. Using the CLI Tool

`AgenticMaid` also provides a command-line interface (CLI) tool to execute all enabled scheduled tasks based on a provided configuration file. This is useful for batch processing or running tasks from a terminal or script.

**To use the CLI tool:**

Ensure the `AgenticMaid` package and its dependencies are accessible in your `PYTHONPATH`.

From the root directory of the `Agent` project, you can run the CLI module as follows:

```bash
python -m AgenticMaid.cli --config-file path/to/your/config.json
```

**Arguments:**

*   `--config-file` (required): Path to the JSON configuration file for `AgenticMaid`. This file should define `ai_services`, `mcp_servers`, and `scheduled_tasks` as needed. Refer to [`AgenticMaid/config.example.json`](./config.example.json) for the structure.

**Behavior:**

1.  The CLI tool will load the specified configuration file.
2.  It will initialize an `ClientAgenticMaid` instance with this configuration.
3.  It will then attempt to execute all tasks listed in the `scheduled_tasks` section of the configuration that have `""enabled"": true`.
4.  Output, including task execution status and any results or errors, will be logged to the console.

**Example Command:**

```bash
python -m AgenticMaid.cli --config-file ./AgenticMaid/config.example.json
```

This command will run all enabled scheduled tasks defined in [`AgenticMaid/config.example.json`](./config.example.json). Check the console output for details on each task's execution.

The original ""Examples"" section is now renumbered.

]]>
### 7. Multi-Agent Dispatch

The `AgenticMaid` supports a multi-agent dispatch feature, allowing one agent to invoke another. This enables the creation of sophisticated, hierarchical agent structures where a primary agent can delegate specific tasks to specialized agents.

#### a. Configuration

To enable this feature, you must add the `multi_agent_dispatch` section to your `config.json` file.

**Configuration Fields:**

*   `enabled` (boolean): Set to `true` to enable the feature.
*   `default_mode` (string): Determines the default invocation mode.
    *   `synchronous` or `sync`: The calling agent waits for the target agent to complete its task and return a result.
    *   `concurrent`: The calling agent invokes the target agent and immediately continues its own execution without waiting for a result.
*   `allowed_invocations` (object): A dictionary defining which agents are permitted to call others.
    *   The keys are the `agent_id` of the *calling* agent (from the `agents` section of your config).
    *   The values are an array of strings, where each string is the `agent_id` of a *target* agent that can be called.
    *   A wildcard `""*""` can be used in the array to allow an agent to call *any* other configured agent.

**Example `config.json` Snippet:**

```json
{
  ""multi_agent_dispatch"": {
    ""enabled"": true,
    ""default_mode"": ""concurrent"",
    ""allowed_invocations"": {
      ""orchestrator_agent"": [
        ""*""
      ],
      ""summary_agent_config_ref"": [
        ""report_agent_v2""
      ],
      ""report_agent_v2"": []
    }
  },
  ""agents"": {
    ""orchestrator_agent"": {
      ""model_config_name"": ""google_gemini_default""
    },
    ""summary_agent_config_ref"": {
      ""model_config_name"": ""google_gemini_default""
    },
    ""report_agent_v2"": {
      ""model_config_name"": ""anthropic_claude4_opus""
    }
  }
}
```

In this example:
*   `orchestrator_agent` can call any other agent.
*   `summary_agent_config_ref` can only call `report_agent_v2`.
*   `report_agent_v2` cannot call any other agents.

#### b. Usage in Prompts

When the feature is enabled, a `dispatch` tool is automatically made available to the agents that are permitted to call others. To use it, instruct the agent in your prompt to call the `dispatch` tool with the required parameters.

**Dispatch Tool Parameters:**

*   `agent_id` (string): The ID of the target agent to invoke.
*   `prompt` (string): The prompt or instruction to pass to the target agent.
*   `mode` (string, optional): The invocation mode (`sync` or `concurrent`). If omitted, the `default_mode` from the configuration is used.

**Example Prompt:**

```
""Please use the dispatch tool to ask the 'report_agent_v2' to generate a detailed analysis of the latest user feedback. Run this in sync mode.""
```

The agent will then parse this instruction and execute the following tool call: `dispatch(agent_id='report_agent_v2', prompt='Generate a detailed analysis of the latest user feedback.', mode='sync')`.
## Examples

### Full Example Script (`example_usage.py`)

```python
import asyncio
import time
from pkg_AgenticMaid.client import ClientAgenticMaid # Placeholder: Actual class name from client.py

async def run_client_operations():
    config = {
        ""ai_services"": {
            ""default_llm"": {
                ""provider"": ""Google"", # Ensure GOOGLE_API_KEY is in .env
                ""model"": ""gemini-2.5-pro""
            },
            ""claude_opus_llm"": {
                 ""provider"": ""Anthropic"", # Ensure ANTHROPIC_API_KEY is in .env
                 ""model"": ""claude-4-opus""
            }
        },
        ""mcp_servers"": {
            # Define at least one MCP server for tools to be fetched.
            # This example assumes an MCP server is running at http://localhost:8001/mcp/v1
            # If not, mcp_tools will be empty.
            ""my_mcp_server"": {
                ""adapter_type"": ""fastapi"",
                ""base_url"": ""http://localhost:8001/mcp/v1"", # Replace with your actual MCP server URL
                ""name"": ""Example MCP Server""
            }
        },
        ""scheduled_tasks"": [
            {
                ""name"": ""Test Scheduled Task"",
                ""cron_expression"": ""daily at 00:00"", # Will run once if current time is past 00:00 and scheduler is kept running
                ""prompt"": ""This is a test scheduled prompt. What time is it using Gemini?"",
                ""model_config_name"": ""default_llm"",
                ""enabled"": True # Set to False if you don't want it to run
            }
        ],
        ""chat_services"": [
            {
                ""service_id"": ""test_chat_gemini"",
                ""llm_service_name"": ""default_llm""
            },
            {
                ""service_id"": ""test_chat_claude"",
                ""llm_service_name"": ""claude_opus_llm""
            }
        ],
        ""default_llm_service_name"": ""default_llm""
    }

    client = ClientAgenticMaid(config_path_or_dict=config)
    await client.async_initialize()

    if not client.config:
        print(""Client configuration failed. Exiting."")
        return

    print(f""ClientAgenticMaid Initialized. Config Source: {client.config_source}"")
    print(f""Available MCP Tools: {[tool.name for tool in client.mcp_tools] if client.mcp_tools else 'No tools fetched (check MCP server config and availability)'}"")

    # 1. Agent Interaction with Gemini
    print(""\n--- Testing Agent Interaction (Gemini) ---"")
    interaction_messages_gemini = [{""role"": ""user"", ""content"": ""Tell me a fun fact using Gemini.""}]
    interaction_response_gemini = await client.run_mcp_interaction(
        messages=interaction_messages_gemini,
        llm_service_name=""default_llm"", # Uses gemini-2.5-pro
        agent_key=""fun_fact_agent_gemini""
    )
    print(f""Agent Interaction Response (Gemini): {interaction_response_gemini}"")

    # 1b. Agent Interaction with Claude
    print(""\n--- Testing Agent Interaction (Claude) ---"")
    interaction_messages_claude = [{""role"": ""user"", ""content"": ""Tell me a different fun fact using Claude.""}]
    interaction_response_claude = await client.run_mcp_interaction(
        messages=interaction_messages_claude,
        llm_service_name=""claude_opus_llm"", # Uses claude-4-opus
        agent_key=""fun_fact_agent_claude""
    )
    print(f""Agent Interaction Response (Claude): {interaction_response_claude}"")


    # 2. Chat Service with Gemini
    print(""\n--- Testing Chat Service (Gemini) ---"")
    chat_messages_gemini = [{""role"": ""user"", ""content"": ""Hi there, how are you? (Gemini)""}]
    chat_response_gemini = await client.handle_chat_message(
        service_id=""test_chat_gemini"",
        messages=chat_messages_gemini
    )
    print(f""Chat Service Response (Gemini): {chat_response_gemini}"")

    # 2b. Chat Service with Claude
    print(""\n--- Testing Chat Service (Claude) ---"")
    chat_messages_claude = [{""role"": ""user"", ""content"": ""Hi there, how are you? (Claude)""}]
    chat_response_claude = await client.handle_chat_message(
        service_id=""test_chat_claude"",
        messages=chat_messages_claude
    )
    print(f""Chat Service Response (Claude): {chat_response_claude}"")

    # 3. Scheduled Tasks
    print(""\n--- Testing Scheduled Tasks ---"")
    if client.scheduler.jobs:
        print(f""Scheduled jobs: {client.scheduler.jobs}"")
        print(""Starting scheduler for a short period (e.g., 5 seconds for demo)..."")
        client.start_scheduler() # Starts a daemon thread

        # Keep the main script running for a bit to allow scheduler to work
        # In a real app, this would be part of the main application loop.
        # For this demo, we'll just sleep.
        # Note: 'daily at HH:MM' tasks might not run in this short window unless HH:MM is very soon.
        # Consider a more frequent cron_expression for immediate testing, e.g., using a custom parser for 'every X seconds'.
        await asyncio.sleep(5)
        print(""Scheduler demo period finished."")
        # client.stop_scheduler() # Conceptual
    else:
        print(""No tasks scheduled."")

if __name__ == ""__main__"":
    # Note: If your MCP server or .env setup is not complete, parts of this example might show warnings or errors.
    # Ensure an MCP server is running if you expect tools, and .env has API keys for LLM calls.
    print(""Make sure your .env file (in AgenticMaid directory) has GOOGLE_API_KEY and ANTHROPIC_API_KEY set for this example to fully work."")
    print(""Also, ensure an MCP server is running at the configured URL if you expect MCP tools."")
    asyncio.run(run_client_operations())

```

This README provides a comprehensive guide to installing, configuring, and using the `ClientAgenticMaid`. Remember to adapt paths and configurations to your specific project setup.",1
https://mcp.so/server/Automata-Labs-team_MCP-Server-Playwright/MCP-Mirror,https://github.com/MCP-Mirror/Automata-Labs-team_MCP-Server-Playwright,MCP Server Playwright,Mirror of,English,developer-tools,browser-automation; playwright; llm-integration,"What is MCP Server Playwright? MCP Server Playwright is a Model Context Protocol server that provides browser automation capabilities using Playwright, enabling large language models (LLMs) to interact with web pages, take screenshots, and execute JavaScript in a real browser environment. How to use MCP Server Playwright? To use MCP Server Playwright, install it via npm or Smithery, and configure it to integrate with your LLM environment. You can then utilize various commands to navigate, capture screenshots, and interact with web elements. Key features of MCP Server Playwright? Full browser automation capabilities Screenshot capture of entire pages or specific elements Comprehensive web interaction (navigation, clicking, form filling) Console log monitoring JavaScript execution in browser context Use cases of MCP Server Playwright? Automating web testing for applications. Capturing screenshots for documentation or reporting. Interacting with web applications for data extraction. FAQ from MCP Server Playwright? Can MCP Server Playwright be used with any web application? Yes! It can automate interactions with any web application that is accessible via a browser. Is MCP Server Playwright free to use? Yes! It is open-source and free to use under the MIT License. What platforms does MCP Server Playwright support? It supports both Windows and macOS.","<h1 align=""center"">MCP Server Playwright</h1>
<p align=""center"">
  <a href=""https://www.automatalabs.io""><img alt=""MCP Playwright"" src=""https://automatalabs.io/icon.svg"" height=""250""/></a>
</p>
<p align=""center"">
  <b>A Model Context Protocol server that provides browser automation capabilities using Playwright</b></br>
  <sub>Enable LLMs to interact with web pages, take screenshots, and execute JavaScript in a real browser environment</sub>
</p>

<p align=""center"">
  <a href=""https://www.npmjs.com/package/@automatalabs/mcp-server-playwright""><img alt=""NPM Version"" src=""https://img.shields.io/npm/v/@automatalabs/mcp-server-playwright.svg"" height=""20""/></a>
  <a href=""https://npmcharts.com/compare/@automatalabs/mcp-server-playwright?minimal=true""><img alt=""Downloads per month"" src=""https://img.shields.io/npm/dm/@automatalabs/mcp-server-playwright.svg"" height=""20""/></a>
  <a href=""https://github.com/Automata-Labs-team/MCP-Server-Playwright/blob/main/LICENSE""><img alt=""License"" src=""https://img.shields.io/github/license/Automata-Labs-team/MCP-Server-Playwright.svg"" height=""20""/></a>
  <a href=""https://smithery.ai/protocol/@automatalabs/mcp-server-playwright""><img alt=""Smithery Installs"" src=""https://smithery.ai/badge/@automatalabs/mcp-server-playwright"" height=""20""/></a>
</p>

<a href=""https://glama.ai/mcp/servers/9q4zck8po5""><img width=""380"" height=""200"" src=""https://glama.ai/mcp/servers/9q4zck8po5/badge"" alt=""MCP-Server-Playwright MCP server"" /></a>

## Table of Contents

- [Features](#features)
- [Installation](#installation)
- [Configuration](#configuration)
- [Components](#components)
  - [Tools](#tools)
  - [Resources](#resources)
- [License](#license)

## Features

- ğŸŒ Full browser automation capabilities
- ğŸ“¸ Screenshot capture of entire pages or specific elements
- ğŸ–±ï¸ Comprehensive web interaction (navigation, clicking, form filling)
- ğŸ“Š Console log monitoring
- ğŸ”§ JavaScript execution in browser context

## Installation

### Installing via Smithery

To install MCP Server Playwright for Claude Desktop automatically via [Smithery](https://smithery.ai/protocol/@automatalabs/mcp-server-playwright):

```bash
npx @smithery/cli install @automatalabs/mcp-server-playwright --client claude
```

You can install the package using either npx or mcp-get:

Using npx:
```bash
npx @automatalabs/mcp-server-playwright install
```
This command will:
1. Check your operating system compatibility (Windows/macOS)
2. Create or update the Claude configuration file
3. Configure the Playwright server integration

The configuration file will be automatically created/updated at:
- Windows: `%APPDATA%\Claude\claude_desktop_config.json`
- macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`

Using mcp-get:
```bash
npx @michaellatman/mcp-get@latest install @automatalabs/mcp-server-playwright
```

## Configuration

The installation process will automatically add the following configuration to your Claude config file:

```json
{
  ""mcpServers"": {
    ""playwright"": {
      ""command"": ""npx"",
      ""args"": [""-y"", ""@automatalabs/mcp-server-playwright""]
    }
  }
}
```

## Components

### Tools

#### `playwright_navigate`
Navigate to any URL in the browser
```javascript
{
  ""url"": ""https://stealthbrowser.cloud""
}
```

#### `playwright_screenshot`
Capture screenshots of the entire page or specific elements
```javascript
{
  ""name"": ""screenshot-name"",     // required
  ""selector"": ""#element-id"",     // optional
  ""fullPage"": true              // optional, default: false
}
```

#### `playwright_click`
Click elements on the page using CSS selector
```javascript
{
  ""selector"": ""#button-id""
}
```

#### `playwright_click_text`
Click elements on the page by their text content
```javascript
{
  ""text"": ""Click me""
}
```

#### `playwright_hover`
Hover over elements on the page using CSS selector
```javascript
{
  ""selector"": ""#menu-item""
}
```

#### `playwright_hover_text`
Hover over elements on the page by their text content
```javascript
{
  ""text"": ""Hover me""
}
```

#### `playwright_fill`
Fill out input fields
```javascript
{
  ""selector"": ""#input-field"",
  ""value"": ""Hello World""
}
```

#### `playwright_select`
Select an option in a SELECT element using CSS selector
```javascript
{
  ""selector"": ""#dropdown"",
  ""value"": ""option-value""
}
```

#### `playwright_select_text`
Select an option in a SELECT element by its text content
```javascript
{
  ""text"": ""Choose me"",
  ""value"": ""option-value""
}
```

#### `playwright_evaluate`
Execute JavaScript in the browser console
```javascript
{
  ""script"": ""document.title""
}
```

### Resources

1. **Console Logs** (`console://logs`)
   - Access browser console output in text format
   - Includes all console messages from the browser

2. **Screenshots** (`screenshot://<n>`)
   - Access PNG images of captured screenshots
   - Referenced by the name specified during capture

## License

This project is licensed under the MIT License - see the [LICENSE](https://github.com/Automata-Labs-team/MCP-Server-Playwright/blob/main/LICENSE) file for details.
",1
https://mcp.so/server/Automated-Webflow/ItsMeHarambe,https://github.com/ItsMeHarambe/Automated-Webflow,Automated-Webflow,Connecting MCP Servers to a Webflow project,English,developer-tools,webflow; automation; mcp-servers,"what is Automated-Webflow? Automated-Webflow is a project designed to connect MCP (Multi-Channel Publishing) servers to a Webflow project, enabling seamless integration and automation of web content management. how to use Automated-Webflow? To use Automated-Webflow, clone the repository from GitHub, configure your MCP server settings, and follow the setup instructions provided in the documentation to connect it to your Webflow project. key features of Automated-Webflow? Integration of MCP servers with Webflow for automated content updates. Simplified management of web content through automation. Open-source project with community support. use cases of Automated-Webflow? Automating the publishing of content from MCP servers to Webflow. Streamlining the workflow for web developers managing multiple content sources. Enhancing the efficiency of web content updates and management. FAQ from Automated-Webflow? What is the purpose of Automated-Webflow? Automated-Webflow connects MCP servers to Webflow, allowing for automated content management and updates. Is Automated-Webflow free to use? Yes! Automated-Webflow is an open-source project and is free to use. How can I contribute to Automated-Webflow? You can contribute by submitting issues, feature requests, or pull requests on the GitHub repository.",,
https://mcp.so/server/AvailMCP/dhananjaypai08,https://github.com/dhananjaypai08/AvailMCP,MCP Server on AVAIL DA,An MCP server to interact with Avail DA,English,developer-tools,,"What is AvailMCP? AvailMCP is a server designed to interact with Avail DA using natural language prompts through the Claude Desktop application. How to use AvailMCP? To use AvailMCP, set up the local avail-client handler backend by configuring the .env file and running the server locally. Then, configure the MCP server in the Claude desktop settings. Key features of AvailMCP? Supports sending data to Avail DA. Future capabilities include querying transactions and integrating with Avail Nexus. Enables seamless cross-chain transfers. Use cases of AvailMCP? Interacting with Avail DA for data transactions. Facilitating natural language queries for data retrieval. Supporting cross-chain operations in blockchain applications. FAQ from AvailMCP? What is the current status of AvailMCP? It is currently a work in progress with basic functionality for sending data. How do I run the server locally? You can run the server by following the setup instructions provided in the documentation. Will there be more features in the future? Yes, future updates will include additional functionalities like transaction queries and cross-chain transfers.","# MCP Server on AVAIL DA
Interact with the avail DA via Claude Desktop using natural language prompts 

## Work In Progress
- Currently supports sending data
- Query transactions on Avail DA
### Future Work 
- Node deployment
- Integrate Avail Nexus 
- Seamless cross-chain transfers 
 
## Setup 
### Running the local avail-client handler backend 
```sh
cd avail-client
cp .env.example .env # configure your .env file
go mod tidy
go run main.go
```
Starts the server locally on 8080 port

#### Configuring the MCP Server to Claude desktop
Add this to `claude_desktop_config.json` present somewhere in `Application Support`

- for mac-os it is in :` /Library/Application\ Support/Claude`
```sh
""AvailDA"": {
    ""command"": ""python"",
    ""args"": [
        ""FULL/PATH/TO/REPO/AvailMCP/avail-mcp/server.py""
    ]
}
```
",0
https://mcp.so/server/AverbePorto-MCP/GHSix,https://github.com/GHSix/AverbePorto-MCP,AverbePorto-MCP,AverbePorto MCP Server,English,developer-tools,,"What is AverbePorto-MCP? AverbePorto-MCP is a Model Context Protocol (MCP) server that facilitates integration with the AverbePorto platform, enabling access to authentication services and document submission through AI tools. How to use AverbePorto-MCP? To use AverbePorto-MCP, access the AverbePorto web system, log in with your user credentials, and utilize the features such as API credential generation, XML document submission, and protocol consultation. Key features of AverbePorto-MCP? API credential generation for user registration XML document upload capabilities Protocol consultation for ANTT Management of cargo insurance averments Use cases of AverbePorto-MCP? Automating document submissions for cargo insurance. Integrating with AI tools for enhanced document management. Streamlining user authentication processes. FAQ from AverbePorto-MCP? Can I use AverbePorto-MCP with any AI tool? Yes! AverbePorto-MCP can be integrated with various AI tools like Claude Desktop and Github Copilot. Is there a cost associated with using AverbePorto-MCP? No, AverbePorto-MCP is free to use for all users. How do I ensure the security of my credentials? Always use secure connections and keep your API credentials confidential.","# AverbePorto-MCP

[![smithery badge](https://smithery.ai/badge/@GHSix/averbeporto-mcp)](https://smithery.ai/server/@GHSix/averbeporto-mcp)

## ğŸŒŸ Sobre
O AverbePorto-MCP Ã© um servidor MCP (Model Context Protocol) que permite a integraÃ§Ã£o com a plataforma [AverbePorto](https://www.averbeporto.com.br), facilitando o acesso aos serviÃ§os de autenticaÃ§Ã£o e envio de documentos atravÃ©s de ferramentas de IA (InteligÃªncia Artificial).

## ğŸŒ Acessando o Sistema Web

1. Acesse [https://www.averbeporto.com.br](https://www.averbeporto.com.br)
2. FaÃ§a login com suas credenciais de usuÃ¡rio
3. Na plataforma, vocÃª poderÃ¡:
   - Gerar Credenciais de API em Cadastro do UsuÃ¡rio
   - Realizar envio de documentos XML
   - Consultar protocolos ANTT
   - Acompanhar e gerenciar suas averbaÃ§Ãµes de seguros de carga

## ğŸ¤– Utilizando o MCP Server com Ferramentas de IA

### InstalaÃ§Ã£o pelo Smithery

Para instalar averbeporto-mcp para Claude Desktop automaticamente via [Smithery](https://smithery.ai/server/@GHSix/averbeporto-mcp):

```bash
npx -y @smithery/cli install @GHSix/averbeporto-mcp --client claude
```

### [Claude Desktop](https://claude.ai/download)
1. Edite o arquivo `%APPDATA%\Claude\claude_desktop_config.json` (Windows) ou `~/Library/Application Support/Claude/claude_desktop_config.json` (MacOS) e adicione a seguinte configuraÃ§Ã£o:
   ```json
    {
      ""mcpServers"": {
        ""AverbePorto-MCP"": {
          ""command"": ""node"",
          ""args"": [""/caminho/para/AverbePorto-MCP/build/index.js""]
        }
      }
    }
   ```
2. Ao iniciar a conversa, o servidor MCP serÃ¡ automaticamente iniciado com base na configuraÃ§Ã£o.

### [Cursor](https://www.cursor.com/), [Roo Code](https://roocode.com/) e outros
1. Crie um arquivo como `.cursor/mcp.json` ou `.roo/mcp.json` em seu projeto com a seguinte configuraÃ§Ã£o:
   ```json
    {
      ""mcpServers"": {
        ""AverbePorto-MCP"": {
          ""command"": ""node"",
          ""args"": [""/caminho/para/AverbePorto-MCP/build/index.js""],
          ""disabled"": false,
          ""alwaysAllow"": [
            ""login"",
            ""consultProtocol"",
            ""upload"",
            ""retrieveDocument"",
            ""decomposeKey""
          ]
        }
      }
    }
   ```
2. Ao iniciar a conversa, o servidor MCP serÃ¡ automaticamente iniciado com base na configuraÃ§Ã£o.

### [Github Copilot](https://github.com/features/copilot)
1. Com o Github Copilot ativo em seu editor, crie o arquivo `.vscode/mcp.json`:
   ```json
   {
     ""inputs"": [
       {
         ""type"": ""promptString"",
         ""id"": ""averbeporto-user"",
         ""description"": ""AverbePorto API Username""
       },
       {
         ""type"": ""promptString"",
         ""id"": ""averbeporto-pass"",
         ""description"": ""AverbePorto API Password"",
         ""password"": true
       }
     ],
     ""servers"": {
       ""AverbePorto-MCP"": {
         ""command"": ""node"",
         ""args"": [""/caminho/para/AverbePorto-MCP/build/index.js""],
         ""env"": {
           ""AVERBEPORTO_USER"": ""${input:averbeporto-user}"",
           ""AVERBEPORTO_PASS"": ""${input:averbeporto-pass}""
         }
       }
     }
   }
   ```
2. O VS Code solicitarÃ¡ suas credenciais na primeira execuÃ§Ã£o e as armazenarÃ¡ de forma segura.
3. O Copilot reconhecerÃ¡ os comandos MCP e oferecerÃ¡ sugestÃµes contextualizadas para:
   - AutenticaÃ§Ã£o na API
   - Upload de documentos XML
   - Consulta de protocolos ANTT
4. As credenciais serÃ£o automaticamente injetadas nas chamadas da API.

## ğŸ“š Ferramentas DisponÃ­veis para a IA

O AverbePorto-MCP oferece as seguintes ferramentas:

- `login`: AutenticaÃ§Ã£o na plataforma
  - ParÃ¢metros: `user`, `pass`
  - Retorna: `sessionId`

- `upload`: Envio de documentos
  - ParÃ¢metros: `sessionId`, `filePath`, `recipient` (opcional), `version` (opcional)
  - Retorna: `uploadId`

- `consultProtocol`: Consulta de protocolos por chave ou vice-versa
  - ParÃ¢metros: `sessionId`, `keys`, `protocols`, `outputFormat`, `download`, `delimiter`
  - Formatos de saÃ­da: json, xml, csv

- `retrieveDocument`: Consulta de documentos enviados
  - ParÃ¢metros:
    - `sessionId`: ID da sessÃ£o obtido no login.
    - `modDoc`: Tipo de documento (e.g., DI, MDF-e, CT-e, NF-e, Minuta CT-e).
    - `dtStart` e `dtLimit`: Datas de inÃ­cio e fim no formato `YYYY-MM-DD`.
    - `dtType`: Tipo de data (Update, Emission, Send), padrÃ£o Ã© `Send`.
    - Filtros opcionais: `numDoc`, `emit`, `rem`, `exped`, `receb`, `dest`, `toma`, `importador`, `representante`, `prot`, `taxId`.
    - PaginaÃ§Ã£o: `page`, `start`, `limit`.
    - Outros: `relation`, `modal`, `valid`.

- `decomposeKey`: DecomposiÃ§Ã£o de chaves para anÃ¡lise
  - ParÃ¢metros:
    - `key`: Chave de acesso de 44 dÃ­gitos para NF-e, CT-e ou MDF-e.

## ğŸ”’ SeguranÃ§a
- Utilize as credenciais de API geradas no mÃ³dulo Cadastro do UsuÃ¡rio
- Mantenha suas credenciais em seguranÃ§a
- NÃ£o compartilhe seu `sessionId`
- Utilize sempre conexÃµes seguras
- Mantenha o servidor MCP atualizado
",0
https://mcp.so/server/Awesome-Claude-MCP-Servers-List/boyso,https://github.com/boyso/Awesome-Claude-MCP-Servers-List,Awesome-Claude-MCP-Servers-List,"We're creating a directory site for discovering MCP servers, along with sharing 100 ways to master Claude Code, MCP, and creative uses of computers.",English,research-and-data,mcp; directory; servers; coding; education,"What is Awesome-Claude-MCP-Servers-List? Awesome-Claude-MCP-Servers-List is a directory site designed for discovering MCP servers and sharing innovative ways to master Claude Code, MCP, and creative computer uses. How to use Awesome-Claude-MCP-Servers-List? Users can visit the directory site to explore various MCP servers and access resources on mastering Claude Code. The site provides a comprehensive list of servers along with tips and tricks. Key features of Awesome-Claude-MCP-Servers-List? A curated directory of MCP servers for easy discovery. 100 ways to master Claude Code and MCP. Resources for creative uses of computers. Use cases of Awesome-Claude-MCP-Servers-List? Finding and joining MCP servers for collaborative projects. Learning and mastering Claude Code through shared resources. Exploring creative computer applications and techniques. FAQ from Awesome-Claude-MCP-Servers-List? What is MCP? MCP stands for Master Control Program, a framework for managing and controlling computer resources. How can I contribute to the directory? Contributions can be made by submitting new server listings or sharing additional resources related to Claude Code and MCP. Is there a community around Awesome-Claude-MCP-Servers-List? Yes! Users are encouraged to engage with each other through the directory and share their experiences.","# Awesome-Claude-MCP-Servers-List
We're creating a directory site for discovering MCP servers, along with sharing 100 ways to master Claude Code, MCP, and creative uses of computers.
",0
